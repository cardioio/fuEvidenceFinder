#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆPubMedè®¿é—®æ¨¡å—
è§£å†³403 Forbiddené”™è¯¯é—®é¢˜ï¼Œæä¾›å¤šç§å…è´¹å†…å®¹åˆ¤å®šæ–¹æ³•
"""

import requests
import time
import random
import json
from urllib.parse import urljoin, urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from typing import Dict, List, Optional, Tuple


class RateLimiter:
    """è¯·æ±‚é¢‘ç‡æ§åˆ¶å™¨"""
    
    def __init__(self, min_delay: float = 1.0, max_delay: float = 3.0):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.last_request_time = 0
    
    def wait(self) -> None:
        """ç­‰å¾…éšæœºå»¶è¿Ÿæ—¶é—´"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        # éšæœºå»¶è¿Ÿ1-3ç§’
        delay = random.uniform(self.min_delay, self.max_delay)
        
        if time_since_last < delay:
            sleep_time = delay - time_since_last
            print(f"â³ ç­‰å¾… {sleep_time:.1f} ç§’åç»§ç»­...")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()


class PubMedSession:
    """PubMedä¼šè¯ç®¡ç†å™¨"""
    
    @staticmethod
    def get_enhanced_headers() -> Dict[str, str]:
        """è·å–å¢å¼ºçš„HTTPè¯·æ±‚å¤´"""
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Referer': 'https://pubmed.ncbi.nlm.nih.gov/',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        }
    
    def __init__(self):
        self.session = requests.Session()
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®é»˜è®¤headers
        self.session.headers.update(self.get_enhanced_headers())
        
        # åˆå§‹åŒ–ä¼šè¯
        self._initialize_session()
    
    def _initialize_session(self) -> None:
        """åˆå§‹åŒ–PubMedä¼šè¯"""
        # è·³è¿‡ä¼šè¯åˆå§‹åŒ–ï¼Œç›´æ¥ä½¿ç”¨å¢å¼ºheaders
        print("ğŸ”„ è·³è¿‡ä¼šè¯åˆå§‹åŒ–ï¼Œç›´æ¥ä½¿ç”¨å¢å¼ºheaders")
        self.session.headers.update(self.get_enhanced_headers())
    
    def get_with_retry(self, url: str, **kwargs) -> Optional[requests.Response]:
        """å¸¦é‡è¯•çš„GETè¯·æ±‚"""
        rate_limiter = RateLimiter()
        rate_limiter.wait()
        
        try:
            response = self.session.get(url, **kwargs)
            if response.status_code == 403:
                print(f"âŒ 403é”™è¯¯: {url}")
                print("ğŸ’¡ å¯èƒ½éœ€è¦: 1) æ›´æ¢IP 2) ç­‰å¾…ä¸€æ®µæ—¶é—´ 3) ä½¿ç”¨ä»£ç†")
                return None
            return response
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return None


class EnhancedPubMedScraper:
    """å¢å¼ºç‰ˆPubMedå†…å®¹æŠ“å–å™¨"""
    
    def __init__(self):
        self.session = PubMedSession()
        self.rate_limiter = RateLimiter()
    
    def check_fulltext_via_web_scraping(self, pmid: str) -> Dict[str, any]:
        """é€šè¿‡å¢å¼ºçš„ç½‘é¡µæŠ“å–æ£€æŸ¥å…è´¹å…¨æ–‡"""
        try:
            pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {pubmed_url}")
            
            response = self.session.get_with_retry(pubmed_url, timeout=15)
            if not response:
                return {
                    'is_free': False,
                    'pmid': pmid,
                    'source': 'web_scraping',
                    'error': 'æ— æ³•è·å–é¡µé¢å†…å®¹',
                    'confidence': 'low'
                }
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¼˜å…ˆçº§1ï¼šç›´æ¥æŸ¥æ‰¾PMCå…è´¹æ ‡è¯†
            pmc_free_link = soup.find('a', title="Free full text at PubMed Central")
            if pmc_free_link:
                href = pmc_free_link.get('href', '')
                if href:
                    full_url = href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}"
                    return {
                        "is_free": True,
                        "pmid": pmid,
                        "source": "web_scraping",
                        "confidence": "high",
                        "links": [{
                            "url": full_url,
                            "title": "Free full text at PubMed Central",
                            "type": "pmc_direct"
                        }],
                        "message": "æ‰¾åˆ°PMCå…è´¹å…¨æ–‡æ ‡è¯†"
                    }
            
            # ä¼˜å…ˆçº§2ï¼šæŸ¥æ‰¾å…¨æ–‡é“¾æ¥åŒºåŸŸ
            full_text_section = soup.find('div', {'data-content-id': 'full-text-links'})
            if not full_text_section:
                full_text_section = soup.find('div', class_='full-text-links')
            
            free_links = []
            if full_text_section:
                link_elements = full_text_section.find_all('a', href=True)
                
                for link in link_elements:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    title_attr = link.get('title', '')
                    
                    is_free = False
                    free_indicators = []
                    
                    # æ£€æŸ¥å…è´¹æŒ‡æ ‡
                    if 'pmc' in href.lower():
                        is_free = True
                        free_indicators.append('PMC URL')
                    if 'europepmc' in href.lower():
                        is_free = True
                        free_indicators.append('EuropePMC URL')
                    if title_attr and 'free' in title_attr.lower():
                        is_free = True
                        free_indicators.append('Free title')
                    if text and 'free' in text.lower():
                        is_free = True
                        free_indicators.append('Free text')
                    
                    if is_free:
                        link_info = {
                            "url": href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}",
                            "title": text,
                            "indicators": free_indicators
                        }
                        free_links.append(link_info)
            
            if free_links:
                return {
                    'is_free': True,
                    'pmid': pmid,
                    'source': 'web_scraping',
                    'confidence': 'medium',
                    'links': free_links,
                    'message': f"é€šè¿‡ç½‘é¡µåˆ†ææ‰¾åˆ° {len(free_links)} ä¸ªå…è´¹é“¾æ¥"
                }
            else:
                return {
                    'is_free': False,
                    'pmid': pmid,
                    'source': 'web_scraping',
                    'confidence': 'medium',
                    'message': 'æœªæ‰¾åˆ°å…è´¹å…¨æ–‡é“¾æ¥'
                }
        
        except Exception as e:
            return {
                'is_free': False,
                'pmid': pmid,
                'source': 'web_scraping',
                'error': str(e),
                'message': 'ç½‘é¡µæŠ“å–å¤±è´¥'
            }
    
    def check_fulltext_via_europepmc(self, pmid: str) -> Dict[str, any]:
        """é€šè¿‡EuropePMC APIæ£€æŸ¥å…è´¹å…¨æ–‡"""
        try:
            print(f"ğŸŒ é€šè¿‡EuropePMCæ£€æŸ¥PMID: {pmid}")
            
            # EuropePMC API endpoint
            api_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
            params = {
                'query': f'EXT_ID:{pmid}',
                'resultType': 'core',
                'format': 'json'
            }
            
            self.rate_limiter.wait()
            response = requests.get(api_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('resultList', {}).get('result'):
                result = data['resultList']['result'][0]
                # ä¿®å¤æ£€æµ‹é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦æœ‰fullTextIdList
                is_free = bool(result.get('fullTextIdList', {}).get('fullTextId'))
                
                return {
                    'is_free': is_free,
                    'pmid': pmid,
                    'source': 'europepmc_api',
                    'confidence': 'high' if is_free else 'medium',
                    'pmcid': result.get('pmcid'),
                    'doi': result.get('doi'),
                    'title': result.get('title'),
                    'fullTextIdList': result.get('fullTextIdList'),
                    'message': 'EuropePMCæ£€æŸ¥å®Œæˆ'
                }
            else:
                return {
                    'is_free': False,
                    'pmid': pmid,
                    'source': 'europepmc_api',
                    'confidence': 'low',
                    'message': 'EuropePMCæœªæ‰¾åˆ°ç›¸å…³è®°å½•'
                }
        
        except Exception as e:
            return {
                'is_free': False,
                'pmid': pmid,
                'source': 'europepmc_api',
                'error': str(e),
                'message': 'EuropePMC APIè®¿é—®å¤±è´¥'
            }
    
    def check_fulltext_via_ncbi_api(self, pmid: str) -> Dict[str, any]:
        """é€šè¿‡NCBI E-utilities APIæ£€æŸ¥å…è´¹å…¨æ–‡"""
        try:
            print(f"ğŸ§¬ é€šè¿‡NCBI APIæ£€æŸ¥PMID: {pmid}")
            
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
            
            # ç›´æ¥ä½¿ç”¨PMIDä½œä¸ºIDæŸ¥è¯¢
            summary_url = f"{base_url}esummary.fcgi"
            summary_params = {
                'db': 'pubmed',
                'id': pmid,
                'retmode': 'json'
            }
            
            self.rate_limiter.wait()
            summary_response = requests.get(summary_url, params=summary_params, timeout=10)
            summary_data = summary_response.json()
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°è®°å½•
            if not summary_data.get('result', {}).get(pmid):
                return {
                    'is_free': False,
                    'pmid': pmid,
                    'source': 'ncbi_api',
                    'confidence': 'low',
                    'message': 'NCBIæœªæ‰¾åˆ°ç›¸å…³è®°å½•'
                }
            
            article_data = summary_data['result'][pmid]
            
            # æ£€æŸ¥PMCçŠ¶æ€ - ä»articleidsæ•°ç»„ä¸­æŸ¥æ‰¾
            pmcid_found = None
            for article_id in article_data.get('articleids', []):
                if article_id.get('idtype') == 'pmc':
                    pmcid_found = article_id.get('value')
                    break
            
            has_free = bool(pmcid_found)
            
            return {
                'is_free': has_free,
                'pmid': pmid,
                'source': 'ncbi_api',
                'confidence': 'high' if has_free else 'medium',
                'pmcid': pmcid_found,
                'message': 'NCBI APIæ£€æŸ¥å®Œæˆ'
            }
        
        except Exception as e:
            return {
                'is_free': False,
                'pmid': pmid,
                'source': 'ncbi_api',
                'error': str(e),
                'message': 'NCBI APIè®¿é—®å¤±è´¥'
            }
    
    def check_fulltext_comprehensive(self, pmid: str) -> Dict[str, any]:
        """ç»¼åˆå¤šç§æ–¹æ³•æ£€æŸ¥å…è´¹å…¨æ–‡"""
        print(f"\nğŸ” å¼€å§‹ç»¼åˆæ£€æŸ¥PMID: {pmid}")
        print("=" * 50)
        
        results = []
        
        # æ–¹æ³•1: EuropePMC API (æœ€å¿«ï¼Œæœ€å¯é )
        print("\nğŸ“¡ æ–¹æ³•1: EuropePMC API")
        result1 = self.check_fulltext_via_europepmc(pmid)
        results.append(result1)
        print(f"ç»“æœ: {result1['message']} - å…è´¹: {result1['is_free']}")
        
        # æ–¹æ³•2: NCBI API
        print("\nğŸ§¬ æ–¹æ³•2: NCBI E-utilities API")
        result2 = self.check_fulltext_via_ncbi_api(pmid)
        results.append(result2)
        print(f"ç»“æœ: {result2['message']} - å…è´¹: {result2['is_free']}")
        
        # æ–¹æ³•3: ç½‘é¡µæŠ“å– (ä½œä¸ºæœ€åæ‰‹æ®µ)
        print("\nğŸ•·ï¸ æ–¹æ³•3: ç½‘é¡µæŠ“å–")
        result3 = self.check_fulltext_via_web_scraping(pmid)
        results.append(result3)
        print(f"ç»“æœ: {result3.get('message', 'æœªçŸ¥é”™è¯¯')} - å…è´¹: {result3.get('is_free', False)}")
        
        # ç»¼åˆå†³ç­–
        print(f"\nğŸ“Š ç»¼åˆåˆ†æç»“æœ")
        print("=" * 50)
        
        # ç»Ÿè®¡å„æ–¹æ³•ç»“æœ
        free_count = sum(1 for r in results if r.get('is_free', False))
        high_confidence_count = sum(1 for r in results if r.get('confidence') == 'high')
        
        # å†³ç­–é€»è¾‘
        if free_count >= 2:
            final_result = {
                'is_free': True,
                'confidence': 'high',
                'consensus': f'{free_count}/3æ–¹æ³•ç¡®è®¤å…è´¹',
                'source': 'consensus_multi_method'
            }
        elif free_count == 1:
            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜ç½®ä¿¡åº¦ç»“æœ
            high_conf_result = next((r for r in results if r.get('is_free') and r.get('confidence') == 'high'), None)
            if high_conf_result:
                final_result = {
                    'is_free': True,
                    'confidence': 'high',
                    'consensus': 'é«˜ç½®ä¿¡åº¦æ–¹æ³•ç¡®è®¤å…è´¹',
                    'source': 'single_high_confidence'
                }
            else:
                final_result = {
                    'is_free': False,
                    'confidence': 'medium',
                    'consensus': 'ä»…1/3æ–¹æ³•æ˜¾ç¤ºå…è´¹ï¼Œç½®ä¿¡åº¦ä¸è¶³',
                    'source': 'consensus_low_confidence'
                }
        else:
            final_result = {
                'is_free': False,
                'confidence': 'high',
                'consensus': '0/3æ–¹æ³•ç¡®è®¤å…è´¹',
                'source': 'consensus_no_free'
            }
        
        # åˆå¹¶ç»“æœ
        final_result.update({
            'pmid': pmid,
            'method_results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
        
        print(f"ğŸ¯ æœ€ç»ˆå†³ç­–: å…è´¹={final_result['is_free']}, ç½®ä¿¡åº¦={final_result['confidence']}")
        print(f"ğŸ“ˆ å…±è¯†: {final_result['consensus']}")
        
        return final_result


def test_enhanced_scraper():
    """æµ‹è¯•å¢å¼ºç‰ˆæŠ“å–å™¨"""
    scraper = EnhancedPubMedScraper()
    
    test_pmids = [
        "30049270",  # æœ‰PMC IDçš„å·²çŸ¥PMID
        "23430950",  # é—®é¢˜PMID
    ]
    
    for pmid in test_pmids:
        print(f"\nğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆæŠ“å–å™¨ - PMID: {pmid}")
        print("=" * 60)
        
        result = scraper.check_fulltext_comprehensive(pmid)
        
        print(f"\nğŸ“‹ æœ€ç»ˆç»“æœ:")
        print(f"PMID: {result['pmid']}")
        print(f"å…è´¹å…¨æ–‡: {result['is_free']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']}")
        print(f"å†³ç­–ä¾æ®: {result['consensus']}")
        print(f"æ•°æ®æº: {result['source']}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        filename = f"enhanced_result_{pmid}_{int(time.time())}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        print("\n" + "=" * 60)


if __name__ == "__main__":
    test_enhanced_scraper()