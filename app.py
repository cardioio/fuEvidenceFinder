#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Flask Web åº”ç”¨ - æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ
åŸºäº Flask æ¡†æ¶çš„ç°ä»£åŒ– Web ç•Œé¢
é›†æˆ PubMed æœç´¢åŠŸèƒ½
"""

from flask import Flask, render_template, Response, request, jsonify
import json
import time
import threading
import logging
import sys
import os
from datetime import datetime
from queue import Queue

# å¯¼å…¥ PubMed æœç´¢ç›¸å…³å‡½æ•°
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from pubmed import search_pubmed, fetch_details, parse_record, ENABLE_FULLTEXT_EXTRACTION

# é…ç½®æ—¥å¿— - å¯ç”¨è°ƒè¯•æ¨¡å¼
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

# åˆ›å»ºè‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—å‘é€åˆ°å‰ç«¯
class FrontendLogHandler(logging.Handler):
    def __init__(self, queue):
        super().__init__()
        self.queue = queue
        self.encoding = 'utf-8'
    
    def emit(self, record):
        try:
            # æ ¼å¼åŒ–æ—¥å¿—è®°å½•
            message = self.format(record)
            timestamp = datetime.now().strftime('%H:%M:%S')
            
            # æ ¹æ®æ—¥å¿—çº§åˆ«æ˜ å°„åˆ°å‰ç«¯ä½¿ç”¨çš„çº§åˆ«
            level_map = {
                logging.DEBUG: 'debug',
                logging.INFO: 'info',
                logging.WARNING: 'warning',
                logging.ERROR: 'error',
                logging.CRITICAL: 'error'
            }
            level = level_map.get(record.levelno, 'info')
            
            # åˆ›å»ºæ—¥å¿—æ•°æ®
            log_data = {
                'type': 'log',
                'content': {
                    'timestamp': timestamp,
                    'level': level,
                    'message': message,
                    'module': record.name,
                    'line': record.lineno,
                    'function': record.funcName
                }
            }
            
            # å°†æ—¥å¿—æ·»åŠ åˆ°é˜Ÿåˆ—
            self.queue.put(log_data)
            
        except Exception as e:
            print(f"æ—¥å¿—å¤„ç†å™¨å‡ºé”™: {e}")

# åˆ›å»ºå…¨å±€çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—
log_queue = Queue()

# åˆ›å»ºå‰ç«¯æ—¥å¿—å¤„ç†å™¨å®ä¾‹
frontend_handler = FrontendLogHandler(log_queue)
frontend_handler.setLevel(logging.DEBUG)

# è®¾ç½®æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s')
frontend_handler.setFormatter(formatter)

# ä¸ºæ ¹æ—¥å¿—è®°å½•å™¨æ·»åŠ å‰ç«¯å¤„ç†å™¨
root_logger = logging.getLogger()
root_logger.addHandler(frontend_handler)

# è®¾ç½®Flaskåº”ç”¨çš„æ—¥å¿—çº§åˆ«
logging.getLogger('flask').setLevel(logging.INFO)  # Flaskè‡ªèº«æ—¥å¿—è®¾ä¸ºINFOé¿å…è¿‡å¤šå™ªéŸ³

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)
app.secret_key = 'your-secret-key-here'  # ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨æ›´å¼ºçš„å¯†é’¥

# å…¨å±€å˜é‡å­˜å‚¨æœç´¢çŠ¶æ€
search_status = {
    'is_running': False,
    'current_keyword': '',
    'logs': [],
    'results': []
}

def add_log(message, level='info'):
    """æ·»åŠ æ—¥å¿—åˆ°å…¨å±€çŠ¶æ€"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    log_entry = {
        'timestamp': timestamp,
        'level': level,
        'message': message
    }
    search_status['logs'].append(log_entry)
    logger.info(f"[{timestamp}] {message}")

def process_search(keyword, max_results=20, enable_fulltext=True, data_queue=None):
    """
    å®é™…çš„ PubMed æœç´¢è¿‡ç¨‹ï¼Œå°†ç»“æœå’Œæ—¥å¿—æ”¾å…¥é˜Ÿåˆ—
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡ (1-100)
        enable_fulltext: æ˜¯å¦å¯ç”¨å…¨æ–‡æœç´¢
        data_queue: ç”¨äºä¼ é€’æœç´¢ç»“æœå’Œæ—¥å¿—çš„é˜Ÿåˆ—
    """
    def add_log(message, level='info'):
        """å°†æ—¥å¿—æ·»åŠ åˆ°é˜Ÿåˆ—"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_data = {
            'type': 'log',
            'content': {
                'timestamp': timestamp,
                'level': level,
                'message': message
            }
        }
        data_queue.put(log_data)
    
    try:
        # è®°å½•æœç´¢é…ç½®
        add_log(f"ğŸ” æœç´¢é…ç½® - å…³é”®è¯: {keyword}")
        add_log(f"ğŸ“Š æœç´¢é…ç½® - æœ€å¤§ç»“æœæ•°: {max_results}ç¯‡")
        add_log(f"ğŸ“„ æœç´¢é…ç½® - åŸæ–‡æœç´¢: {'å¼€å¯' if enable_fulltext else 'å…³é—­'}")
        
        # å¼€å§‹æœç´¢æµç¨‹
        add_log(f"ğŸš€ å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        # 1. æœç´¢ PubMed
        add_log(f"ğŸ” æ­£åœ¨æœç´¢: {keyword.strip()}...")
        ids = search_pubmed(keyword, max_results)  # ä½¿ç”¨å‚æ•°åŒ–çš„æœ€å¤§ç»“æœæ•°
        
        if not ids:
            add_log("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥æœç´¢è¯æ˜¯å¦æ­£ç¡®", 'warning')
            return
        
        add_log(f"âœ… æ‰¾åˆ° {len(ids)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¼€å§‹è·å–è¯¦ç»†ä¿¡æ¯...")
        
        # 2. è·å–è¯¦æƒ…
        add_log(f"ğŸ“¥ æ­£åœ¨è·å– {len(ids)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯...")
        articles = fetch_details(ids)
        
        if not articles:
            add_log("âŒ è·å–æ–‡çŒ®è¯¦æƒ…å¤±è´¥", 'error')
            return
        
        add_log(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡çŒ®è¯¦æƒ…ï¼Œå¼€å§‹è§£ææ•°æ®...")
        
        # 3. è§£ææ•°æ® - é€æ¡å¤„ç†å¹¶æ”¾å…¥é˜Ÿåˆ—
        results_count = 0
        fulltext_success_count = 0
        paid_count = 0
        failed_count = 0
        ai_success_count = 0
        
        for i, article in enumerate(articles):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢æœç´¢ï¼ˆåœ¨å¤„ç†è¿‡ç¨‹ä¸­æ£€æŸ¥ï¼‰
            if not search_status['is_running']:
                add_log("â¹ï¸ æœç´¢è¢«ç”¨æˆ·ä¸­æ–­", 'warning')
                add_log(f"ğŸ“Š å·²å¤„ç† {results_count} ç¯‡æ–‡çŒ®", 'info')
                break
            
            add_log(f"âš™ï¸ æ­£åœ¨å¤„ç†æ–‡çŒ® {i+1}/{len(articles)}...")
            
            try:
                # è§£æå•ç¯‡æ–‡çŒ®
                data = parse_record(article, enable_fulltext)
                results_count += 1
                
                # å®æ—¶æ˜¾ç¤ºå…¨æ–‡å¤„ç†çŠ¶æ€
                if enable_fulltext:
                    free_status = data.get('å…è´¹å…¨æ–‡çŠ¶æ€', 'æœªæ£€æŸ¥')
                    if free_status == 'å…è´¹':
                        fulltext_success_count += 1
                        add_log(f"  ğŸ“¤ æ–‡çŒ® {i+1} æ£€æµ‹åˆ°å…è´¹å…¨æ–‡ï¼Œå¼€å§‹å†…å®¹æå–...", 'info')
                    elif free_status == 'ä»˜è´¹':
                        paid_count += 1
                        add_log(f"  ğŸ’° æ–‡çŒ® {i+1} ä»…ä»˜è´¹å…¨æ–‡ï¼Œè·³è¿‡å…è´¹å†…å®¹", 'warning')
                    else:
                        failed_count += 1
                        add_log(f"  âš ï¸ æ–‡çŒ® {i+1} å…è´¹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {free_status}", 'warning')
                
                # å®æ—¶æ˜¾ç¤ºAIå¤„ç†çŠ¶æ€
                if enable_fulltext:  # åªæœ‰å¯ç”¨äº†å…¨æ–‡æå–æ‰ä¼šè¿›è¡ŒAIæå–
                    if data.get('AIæå–çŠ¶æ€') == 'æˆåŠŸ':
                        ai_success_count += 1
                        add_log(f"  ğŸ¤– æ–‡çŒ® {i+1} AIæå–å®Œæˆ", 'success')
                    elif data.get('AIæå–çŠ¶æ€') == 'å¤±è´¥':
                        add_log(f"  âŒ æ–‡çŒ® {i+1} AIæå–å¤±è´¥", 'error')
                
                # ç«‹å³å°†æ•°æ®è¡Œæ”¾å…¥é˜Ÿåˆ—
                row_data = {
                    'type': 'row',
                    'content': data
                }
                data_queue.put(row_data)
                
                # æ˜¾ç¤ºå¤„ç†è¿›åº¦æ±‡æ€»
                add_log(f"âœ… æ–‡çŒ® {i+1} å¤„ç†å®Œæˆ: {data.get('æ ‡é¢˜', 'N/A')[:50]}...")
                
                if enable_fulltext:
                    add_log(f"  ğŸ“Š å¤„ç†è¿›åº¦ - å…¨æ–‡: {fulltext_success_count}/{results_count}, AI: {ai_success_count}/{results_count}", 'info')
                
                # çŸ­æš‚åœé¡¿ä»¥å…è®¸ç”¨æˆ·ä¸­æ–­
                time.sleep(0.1)
                
                # ç«‹å³æ£€æŸ¥åœæ­¢çŠ¶æ€ï¼ˆå¿«é€Ÿå“åº”ï¼‰
                if not search_status['is_running']:
                    add_log("â¹ï¸ æœç´¢è¢«ç”¨æˆ·ä¸­æ–­", 'warning')
                    add_log(f"ğŸ“Š å·²å¤„ç† {results_count} ç¯‡æ–‡çŒ®", 'info')
                    break
                
            except Exception as e:
                error_msg = f"å¤„ç†ç¬¬ {i+1} ç¯‡æ–‡çŒ®æ—¶å‡ºé”™: {str(e)}"
                add_log(error_msg, 'error')
                logger.error(error_msg)
                continue
        
        # æœç´¢å®Œæˆ - æ˜¾ç¤ºè¯¦ç»†æ±‡æ€»
        add_log(f"ğŸ‰ æœç´¢å®Œæˆï¼å…±å¤„ç† {results_count} ç¯‡æ–‡çŒ®", 'success')
        
        if enable_fulltext:
            # ç¡®ä¿ä»˜è´¹å…¨æ–‡å’Œå¤±è´¥æ•°çš„ç»Ÿè®¡æ­£ç¡®
            # å¦‚æœæ²¡æœ‰æ˜ç¡®æ ‡è®°ä¸ºå…è´¹æˆ–ä»˜è´¹ï¼Œå°±è§†ä¸ºå¤±è´¥
            total_processed = fulltext_success_count + paid_count + failed_count
            if total_processed < results_count:
                failed_count += (results_count - total_processed)
            
            add_log(f"ğŸ“Š å…¨æ–‡å¤„ç†ç»Ÿè®¡:", 'info')
            add_log(f"  âœ… å…è´¹å…¨æ–‡: {fulltext_success_count} ç¯‡", 'success')
            add_log(f"  ğŸ’° ä»˜è´¹å…¨æ–‡: {paid_count} ç¯‡", 'warning')
            add_log(f"  âŒ æ£€æŸ¥å¤±è´¥: {failed_count} ç¯‡", 'error')
            
            add_log(f"ğŸ” æœç´¢ä»»åŠ¡å®Œæˆï¼Œç”¨æˆ·å¯æŸ¥çœ‹ç»“æœè¡¨æ ¼", 'success')
        
    except Exception as e:
        error_msg = f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        add_log(error_msg, 'error')
        logger.error(error_msg)
    finally:
        search_status['is_running'] = False
        # å‘é€ç»“æŸä¿¡å·
        end_data = {'type': 'end'}
        data_queue.put(end_data)

@app.route('/')
def index():
    """ä¸»é¡µé¢è·¯ç”±"""
    return render_template('index.html')

@app.route('/stream_search')
def stream_search():
    """æµå¼æœç´¢å“åº”è·¯ç”± - ä½¿ç”¨å®é™…çš„ PubMed æœç´¢"""
    keyword = request.args.get('keyword', '')
    max_results = request.args.get('max_results', default=20, type=int)
    enable_fulltext = request.args.get('enable_fulltext', default='true').lower() == 'true'
    
    # å‚æ•°éªŒè¯
    if not keyword:
        return jsonify({'error': 'ç¼ºå°‘å…³é”®è¯å‚æ•°'}), 400
    
    if max_results < 1 or max_results > 100:
        return jsonify({'error': 'æœ€å¤§ç»“æœæ•°é‡å¿…é¡»åœ¨1-100ä¹‹é—´'}), 400
    
    # é‡ç½®æœç´¢çŠ¶æ€
    global search_status
    search_status = {
        'is_running': True,
        'current_keyword': keyword,
        'max_results': max_results,
        'enable_fulltext': enable_fulltext,
        'logs': [],
        'results': []
    }
    
    def generate():
        """ç”Ÿæˆæµå¼å“åº”"""
        # åˆ›å»ºç”¨äºæœç´¢æ•°æ®çš„é˜Ÿåˆ—
        data_queue = Queue()
        
        # åˆ›å»ºå¹¶å¯åŠ¨æœç´¢çº¿ç¨‹
        search_thread = threading.Thread(
            target=process_search,
            args=(keyword, max_results, enable_fulltext, data_queue),
            daemon=True
        )
        search_thread.start()
        
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢æœç´¢
                if not search_status['is_running']:
                    # å‘é€ç”¨æˆ·åœæ­¢ä¿¡å·
                    stop_data = {
                        'type': 'stopped',
                        'content': {'message': 'æœç´¢å·²åœæ­¢'}
                    }
                    yield f"data: {json.dumps(stop_data, ensure_ascii=False)}\n\n"
                    break
                
                # ä»æ•°æ®é˜Ÿåˆ—è·å–æ•°æ®
                if not data_queue.empty():
                    data = data_queue.get()
                    
                    # æ£€æŸ¥æ˜¯å¦æœç´¢ç»“æŸ
                    if data.get('type') == 'end':
                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                        # å‘é€å‰©ä½™çš„Pythonæ—¥å¿—
                        while not log_queue.empty():
                            log = log_queue.get()
                            yield f"data: {json.dumps(log, ensure_ascii=False)}\n\n"
                        break
                    
                    # å‘é€æ•°æ®
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
                # ä»æ—¥å¿—é˜Ÿåˆ—è·å–å¹¶å‘é€æ—¥å¿—
                while not log_queue.empty():
                    log = log_queue.get()
                    yield f"data: {json.dumps(log, ensure_ascii=False)}\n\n"
                
                # çŸ­æš‚ç¡çœ é¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.05)
                
        except Exception as e:
            logger.error(f"æµå¼æœç´¢å‡ºé”™: {e}")
            error_data = {
                'type': 'error',
                'content': {'message': f'æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'}
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return Response(
        generate(),
        mimetype='text/event-stream',  # ä½¿ç”¨ SSE æ ¼å¼
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control'
        }
    )

@app.route('/search', methods=['POST'])
def search():
    """æœç´¢æ¥å£ï¼ˆå¯é€‰çš„åŒæ­¥æ¥å£ï¼‰"""
    data = request.get_json()
    keyword = data.get('keyword', '') if data else ''
    
    if not keyword:
        return jsonify({'error': 'ç¼ºå°‘å…³é”®è¯'}), 400
    
    # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„ pubmed.py æœç´¢é€»è¾‘
    # ç›®å‰è¿”å›æ¨¡æ‹Ÿç»“æœ
    return jsonify({
        'success': True,
        'message': f'å¼€å§‹æœç´¢: {keyword}',
        'keyword': keyword
    })

@app.route('/status')
def status():
    """è·å–å½“å‰æœç´¢çŠ¶æ€"""
    return jsonify(search_status)



@app.route('/stop_search', methods=['POST'])
def stop_search():
    """åœæ­¢æœç´¢"""
    search_status['is_running'] = False
    add_log("æœç´¢å·²æ‰‹åŠ¨åœæ­¢", 'warning')
    return jsonify({'success': True, 'message': 'æœç´¢å·²åœæ­¢'})

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ Flask åº”ç”¨...")
    print("ğŸ“± è®¿é—®åœ°å€: http://localhost:5001")
    print("ğŸ” æœç´¢æ¥å£: POST /search")
    print("ğŸ“¡ æµå¼æ¥å£: GET /stream_search?keyword=å…³é”®è¯")
    
    app.run(debug=True, host='0.0.0.0', port=5001, threaded=True)