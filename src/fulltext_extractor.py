"""
å…¨æ–‡æå–æ¨¡å— - è´Ÿè´£ä»PubMedå’Œå…è´¹å…¨æ–‡é“¾æ¥ä¸­æå–æ–‡ç« å†…å®¹
"""
import requests
import logging
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup
from src.config import ConfigManager

logger = logging.getLogger(__name__)


class FullTextExtractor:
    """å…¨æ–‡å†…å®¹æå–å™¨"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        """åˆå§‹åŒ–å…¨æ–‡æå–å™¨"""
        self.config = config_manager or ConfigManager()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.timeout = 20
    
    def check_full_text_availability(self, pmid: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥PMIDå¯¹åº”çš„æ–‡ç« æ˜¯å¦æä¾›å…è´¹å…¨æ–‡
        
        Args:
            pmid: PubMed ID
        
        Returns:
            åŒ…å«å…è´¹çŠ¶æ€å’Œé“¾æ¥ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ„å»ºPubMedé¡µé¢URL
            pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {pubmed_url}")
            
            # è·å–é¡µé¢å†…å®¹
            response = requests.get(pubmed_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¼˜å…ˆçº§1ï¼šç›´æ¥æŸ¥æ‰¾PMCå…è´¹å…¨æ–‡é“¾æ¥ - æ”¹è¿›ç‰ˆæœ¬
            pmc_free_link = None
            
            # æ–¹æ³•1ï¼šç›´æ¥æŸ¥æ‰¾title="Free full text at PubMed Central"çš„aå…ƒç´ 
            pmc_free_link = soup.find('a', title="Free full text at PubMed Central")
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾åŒ…å«PMCå’Œfreeç›¸å…³çš„aå…ƒç´ 
            if not pmc_free_link:
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href', '')
                    title_attr = link.get('title', '')
                    class_attr = link.get('class', [])
                    text = link.get_text(strip=True)
                    
                    # å°†classå±æ€§è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if isinstance(class_attr, list):
                        class_str = ' '.join(class_attr).lower()
                    else:
                        class_str = str(class_attr).lower()
                    
                    # æ£€æŸ¥å„ç§å…è´¹PMCæ ‡è¯†
                    is_pmc_free = False
                    reason = ""
                    
                    # æ£€æŸ¥hrefä¸­çš„PMCæ ‡è¯†
                    if 'pmc' in href.lower() and 'pmc' in class_str:
                        is_pmc_free = True
                        reason = "PMC URL + PMC class"
                    elif 'pmc' in href.lower() and ('free' in title_attr.lower() or 'free' in text.lower()):
                        is_pmc_free = True
                        reason = "PMC URL + Free indicator"
                    elif 'pmc' in class_str and ('free' in title_attr.lower() or 'free' in text.lower()):
                        is_pmc_free = True
                        reason = "PMC class + Free indicator"
                    elif 'pmc' in href.lower() and any(keyword in text.lower() for keyword in ['free', 'pmc article']):
                        is_pmc_free = True
                        reason = "PMC URL + PMC text"
                    
                    if is_pmc_free:
                        pmc_free_link = link
                        print(f"âœ… æ‰¾åˆ°PMCå…è´¹å…¨æ–‡é“¾æ¥: {reason}")
                        break
            
            if pmc_free_link:
                href = pmc_free_link.get('href', '')
                if href:
                    full_url = href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}"
                    print(f"âœ… æ‰¾åˆ°PMCå…è´¹å…¨æ–‡é“¾æ¥: {full_url}")
                    
                    # è·å–æ›´å¤šæ ‡è¯†ä¿¡æ¯
                    title_attr = pmc_free_link.get('title', '')
                    class_attr = pmc_free_link.get('class', [])
                    text = pmc_free_link.get_text(strip=True)
                    
                    return {
                        "is_free": True,
                        "pmid": pmid,
                        "pubmed_url": pubmed_url,
                        "links": [{
                            "url": full_url,
                            "title": text or title_attr or "Free PMC article",
                            "is_free": True,
                            "element_found": "improved detection"
                        }],
                        "message": "æ‰¾åˆ°PMCå…è´¹å…¨æ–‡",
                        "source": "enhanced_pmc_detection"
                    }
            
            # ä¼˜å…ˆçº§2ï¼šæŸ¥æ‰¾Full text linkséƒ¨åˆ†
            full_text_section = soup.find('div', {'data-content-id': 'full-text-links'})
            if not full_text_section:
                full_text_section = soup.find('div', class_='full-text-links')
            
            # ä¼˜å…ˆçº§3ï¼šåœ¨å…¨æ–‡é“¾æ¥å®¹å™¨ä¸­æŸ¥æ‰¾å…è´¹é“¾æ¥
            free_links = []
            all_links = []
            
            if full_text_section:
                link_elements = full_text_section.find_all('a', href=True)
                print(f"ğŸ“„ åœ¨å…¨æ–‡é“¾æ¥éƒ¨åˆ†æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")
            else:
                # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„å…¨æ–‡é“¾æ¥éƒ¨åˆ†ï¼ŒæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                link_elements = soup.find_all('a', href=True)
                # ç­›é€‰å¯èƒ½ç›¸å…³çš„é“¾æ¥
                link_elements = [link for link in link_elements if any(keyword in link.get('href', '').lower() 
                                   for keyword in ['pmc', 'europepmc', 'full', 'text', 'article'])]
                print(f"ğŸ”— æ‰¾åˆ° {len(link_elements)} ä¸ªç›¸å…³é“¾æ¥")
            
            # åˆ†ææ¯ä¸ªé“¾æ¥
            for link in link_elements:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                title_attr = link.get('title', '')
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå…è´¹é“¾æ¥
                is_free = False
                free_indicators = []
                
                # æ£€æŸ¥å„ç§å…è´¹æŒ‡æ ‡ - æ”¹è¿›ç‰ˆæœ¬
                class_attr = link.get('class', [])
                if isinstance(class_attr, list):
                    class_str = ' '.join(class_attr).lower()
                else:
                    class_str = str(class_attr).lower()
                
                # æ£€æŸ¥hrefä¸­çš„å…è´¹æ ‡è¯†
                if 'pmc' in href.lower() or 'pmc.ncbi.nlm.nih.gov' in href.lower():
                    is_free = True
                    free_indicators.append('PMC URL')
                if 'europepmc' in href.lower():
                    is_free = True
                    free_indicators.append('EuropePMC URL')
                if 'pubmedcentral' in href.lower():
                    is_free = True
                    free_indicators.append('PubMed Central')
                
                # æ£€æŸ¥classå±æ€§ä¸­çš„å…è´¹æ ‡è¯†
                if 'pmc' in class_str:
                    is_free = True
                    free_indicators.append('PMC class')
                if 'free' in class_str:
                    is_free = True
                    free_indicators.append('Free class')
                
                # æ£€æŸ¥titleå±æ€§
                if title_attr and 'free' in title_attr.lower():
                    is_free = True
                    free_indicators.append('Free title')
                
                # æ£€æŸ¥æ–‡æœ¬å†…å®¹ - æ”¹è¿›ç‰ˆæœ¬
                if text:
                    text_lower = text.lower()
                    if any(keyword in text_lower for keyword in ['free pmc', 'pmc article', 'free full text', 'free article']):
                        is_free = True
                        free_indicators.append('Free PMC text')
                    elif 'free' in text_lower:
                        is_free = True
                        free_indicators.append('Free text')
                    elif 'pmc' in text_lower and 'article' in text_lower:
                        is_free = True
                        free_indicators.append('PMC article text')
                
                link_info = {
                    "url": href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}",
                    "title": text,
                    "title_attr": title_attr,
                    "is_free": is_free,
                    "indicators": free_indicators
                }
                
                all_links.append(link_info)
                if is_free:
                    free_links.append(link_info)
                    print(f"âœ… å‘ç°å…è´¹é“¾æ¥: {text} - {link_info['url']} ({', '.join(free_indicators)})")
            
            # ç¡®å®šæœ€ç»ˆç»“æœ
            if free_links:
                return {
                    "is_free": True,
                    "pmid": pmid,
                    "pubmed_url": pubmed_url,
                    "links": free_links,
                    "all_links": all_links,
                    "message": f"æ‰¾åˆ° {len(free_links)} ä¸ªå…è´¹å…¨æ–‡é“¾æ¥",
                    "source": "link_analysis"
                }
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…è´¹é“¾æ¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»˜è´¹é“¾æ¥
                has_paid_links = len(all_links) > 0
                return {
                    "is_free": False,
                    "pmid": pmid,
                    "pubmed_url": pubmed_url,
                    "links": all_links,
                    "message": "æœªæ‰¾åˆ°å…è´¹å…¨æ–‡" + ("ï¼Œä½†æœ‰ä»˜è´¹é“¾æ¥" if has_paid_links else ""),
                    "source": "no_free_links" if not has_paid_links else "paid_only"
                }
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            return {
                "is_free": False,
                "pmid": pmid,
                "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                "message": "æ— æ³•è·å–é¡µé¢ä¿¡æ¯"
            }
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {str(e)}")
            return {
                "is_free": False,
                "pmid": pmid,
                "error": f"è§£æå¤±è´¥: {str(e)}",
                "message": "é¡µé¢è§£æå‡ºé”™"
            }
    
    def extract_full_text_content(self, pmid: str, link_url: str = None) -> Dict[str, Any]:
        """
        ä»å…è´¹å…¨æ–‡é“¾æ¥æå–æ–‡ç« å†…å®¹
        
        Args:
            pmid: PubMed ID
            link_url: å…¨æ–‡é“¾æ¥URLï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
        
        Returns:
            åŒ…å«æå–å†…å®¹çš„å­—å…¸
        """
        try:
            # å¦‚æœæ²¡æœ‰æä¾›é“¾æ¥URLï¼Œå…ˆæ£€æŸ¥å¯ç”¨æ€§
            if not link_url:
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹PMID {pmid}çš„å…è´¹å…¨æ–‡é“¾æ¥...")
                availability = self.check_full_text_availability(pmid)
                if not availability['is_free']:
                    return {
                        "success": False,
                        "pmid": pmid,
                        "message": availability['message']
                    }
                
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå…è´¹é“¾æ¥
                free_links = [link for link in availability['links'] if link['is_free']]
                if not free_links:
                    return {
                        "success": False,
                        "pmid": pmid,
                        "message": "æœªæ‰¾åˆ°å¯ç”¨çš„å…è´¹å…¨æ–‡é“¾æ¥"
                    }
                
                link_url = free_links[0]['url']
                print(f"ğŸ“„ é€‰æ‹©å…è´¹å…¨æ–‡é“¾æ¥: {link_url}")
            
            print(f"ğŸ“– æ­£åœ¨æå–PMID {pmid}çš„å…¨æ–‡å†…å®¹...")
            
            # è·å–å…¨æ–‡é¡µé¢
            response = requests.get(link_url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åˆå§‹åŒ–æå–ç»“æœ
            content = {
                "pmid": pmid,
                "source_url": link_url,
                "extraction_success": False,
                "content": {},
                "debug_info": {
                    "page_title": "",
                    "total_sections": 0,
                    "extracted_elements": []
                }
            }
            
            # æå–é¡µé¢æ ‡é¢˜ç”¨äºè°ƒè¯•
            title_tag = soup.find('title')
            if title_tag:
                content['debug_info']['page_title'] = title_tag.get_text(strip=True)
                print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title_tag.get_text(strip=True)[:100]}...")
            
            # æå–æ ‡é¢˜ - å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1.article-title',
                'h1.title',
                'h1',
                '.article-title',
                '.title',
                'title'
            ]
            
            title_text = ""
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text and len(title_text) > 10:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                        content['content']['title'] = title_text
                        content['debug_info']['extracted_elements'].append(f"æ ‡é¢˜: {selector}")
                        print(f"âœ… æå–æ ‡é¢˜: {title_text[:100]}...")
                        break
            
            # æå–æ‘˜è¦ - å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            abstract_selectors = [
                'div.abstract',
                'section.abstract',
                'div[data-section="abstract"]',
                'div#abstract',
                '.abstract-content',
                '.article-abstract',
                'div[class*="abstract"]'
            ]
            
            abstract_text = ""
            for selector in abstract_selectors:
                abstract_elem = soup.select_one(selector)
                if abstract_elem:
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for unwanted in abstract_elem.select('script, style, .reference, .citation'):
                        unwanted.decompose()
                    
                    abstract_text = abstract_elem.get_text(strip=True)
                    if abstract_text and len(abstract_text) > 50:  # ç¡®ä¿æ‘˜è¦æœ‰æ„ä¹‰
                        content['content']['abstract'] = abstract_text
                        content['debug_info']['extracted_elements'].append(f"æ‘˜è¦: {selector}")
                        print(f"âœ… æå–æ‘˜è¦: {len(abstract_text)} å­—ç¬¦")
                        break
            
            # æå–å…³é”®è¯ - å¤šç§ä½ç½®
            keywords_selectors = [
                'div.keywords',
                'div#keywords',
                '.keyword-list',
                '[data-section="keywords"]',
                'p:contains("Keywords")',
                'div:contains("Keywords")'
            ]
            
            keywords_text = ""
            for selector in keywords_selectors:
                keywords_elem = soup.select_one(selector)
                if keywords_elem:
                    keywords_text = keywords_elem.get_text(strip=True)
                    if keywords_text and len(keywords_text) > 5:
                        content['content']['keywords'] = keywords_text
                        content['debug_info']['extracted_elements'].append(f"å…³é”®è¯: {selector}")
                        print(f"âœ… æå–å…³é”®è¯: {keywords_text[:100]}...")
                        break
            
            # æå–ä½œè€…ä¿¡æ¯
            authors_selectors = [
                'div.authors',
                'ul.author-list',
                '.author-list',
                'div.author-info',
                '[data-section="authors"]'
            ]
            
            authors_text = ""
            for selector in authors_selectors:
                authors_elem = soup.select_one(selector)
                if authors_elem:
                    authors_text = authors_elem.get_text(strip=True)
                    if authors_text and len(authors_text) > 10:
                        content['content']['authors'] = authors_text
                        content['debug_info']['extracted_elements'].append(f"ä½œè€…: {selector}")
                        print(f"âœ… æå–ä½œè€…ä¿¡æ¯: {authors_text[:100]}...")
                        break
            
            # æå–æ­£æ–‡å†…å®¹ - æ›´æ™ºèƒ½çš„ç­–ç•¥
            body_selectors = [
                'div.article-body',
                'article',
                'div.body-content',
                'div.main-content',
                'div.content',
                'div#content'
            ]
            
            body_text = ""
            for selector in body_selectors:
                body_elem = soup.select_one(selector)
                if body_elem:
                    # ç§»é™¤å¯¼èˆªã€å¹¿å‘Šã€å¼•ç”¨ç­‰ä¸éœ€è¦çš„å†…å®¹
                    unwanted_selectors = [
                        'script', 'style', 'nav', 'header', 'footer', 
                        '.advertisement', '.sidebar', '.related-articles',
                        '.reference', '.citation', '.author-notes'
                    ]
                    
                    for unwanted_selector in unwanted_selectors:
                        for unwanted in body_elem.select(unwanted_selector):
                            unwanted.decompose()
                    
                    # æå–æ–‡æœ¬
                    body_text = body_elem.get_text(strip=True)
                    if body_text and len(body_text) > 200:  # ç¡®ä¿æ­£æ–‡å†…å®¹æœ‰æ„ä¹‰
                        content['content']['body_text'] = body_text
                        content['debug_info']['extracted_elements'].append(f"æ­£æ–‡: {selector}")
                        print(f"âœ… æå–æ­£æ–‡: {len(body_text)} å­—ç¬¦")
                        break
            
            # æå–å‚è€ƒæ–‡çŒ®
            refs_selectors = [
                'div.references',
                'ol.references',
                'ul.references',
                '.reference-list',
                '[data-section="references"]'
            ]
            
            refs_text = ""
            for selector in refs_selectors:
                refs_elem = soup.select_one(selector)
                if refs_elem:
                    refs_text = refs_elem.get_text(strip=True)
                    if refs_text and len(refs_text) > 50:
                        content['content']['references'] = refs_text
                        content['debug_info']['extracted_elements'].append(f"å‚è€ƒæ–‡çŒ®: {selector}")
                        print(f"âœ… æå–å‚è€ƒæ–‡çŒ®: {len(refs_text)} å­—ç¬¦")
                        break
            
            # ç»Ÿè®¡æå–çš„å…ƒç´ 
            content['debug_info']['total_sections'] = len(content['content'])
            
            # åˆ¤æ–­æå–æ˜¯å¦æˆåŠŸ
            if len(content['content']) >= 2:  # è‡³å°‘æå–åˆ°æ ‡é¢˜å’Œæ‘˜è¦
                content['extraction_success'] = True
                content['message'] = f"æˆåŠŸæå–{len(content['content'])}ä¸ªéƒ¨åˆ†çš„å†…å®¹"
                print(f"âœ… å…¨æ–‡æå–å®Œæˆï¼Œå…±æå–{len(content['content'])}ä¸ªéƒ¨åˆ†")
            else:
                content['extraction_success'] = False
                content['message'] = f"æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†"
                print(f"âš ï¸ æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†")
            
            # å¦‚æœå®Œå…¨æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œæä¾›è°ƒè¯•ä¿¡æ¯
            if not content['content']:
                content['extraction_success'] = False
                content['message'] = "æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹"
                content['debug_info']['no_content_reason'] = "é¡µé¢å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–é“¾æ¥æ— æ•ˆ"
                print(f"âŒ æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
            
            return content
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "pmid": pmid,
                "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                "message": "æ— æ³•è·å–å…¨æ–‡é¡µé¢",
                "debug_info": {"error_type": "network_error"}
            }
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "pmid": pmid,
                "error": f"æå–å¤±è´¥: {str(e)}",
                "message": "å†…å®¹æå–å‡ºé”™",
                "debug_info": {"error_type": "extraction_error", "error_detail": str(e)}
            }
    
    def analyze_pmid_with_full_text(self, pmid: str) -> Dict[str, Any]:
        """
        ç»¼åˆåˆ†æPMIDï¼šæ£€æŸ¥å…è´¹çŠ¶æ€å¹¶æå–å…¨æ–‡å†…å®¹
        
        Args:
            pmid: PubMed ID
        
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹åˆ†æPMID: {pmid}")
        print("=" * 60)
        
        # æ­¥éª¤1ï¼šæ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§
        print("æ­¥éª¤1: æ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§...")
        availability = self.check_full_text_availability(pmid)
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            "pmid": pmid,
            "timestamp": self.config.get_current_time(),
            "is_free": availability.get('is_free', False),
            "links": availability.get('links', []),
            "message": availability.get('message', ''),
            "extraction_success": False,
            "extracted_content": {},
            "debug_info": {
                "availability_source": availability.get('source', 'unknown'),
                "total_links_found": len(availability.get('links', [])),
                "extraction_attempted": False,
                "extraction_details": {}
            }
        }
        
        if not availability.get('is_free', False):
            print(f"âŒ PMID {pmid} æ— å…è´¹å…¨æ–‡: {availability.get('message', 'æœªçŸ¥åŸå› ')}")
            result['debug_info']['no_free_reason'] = availability.get('message', 'æœªçŸ¥åŸå› ')
            result['debug_info']['availability_source'] = availability.get('source', 'unknown')
            return result
        
        print(f"âœ… PMID {pmid} æä¾›å…è´¹å…¨æ–‡ (æ¥æº: {availability.get('source', 'unknown')})")
        result['debug_info']['extraction_attempted'] = True
        
        # æ­¥éª¤2ï¼šæå–å…¨æ–‡å†…å®¹
        print("\næ­¥éª¤2: æå–å…¨æ–‡å†…å®¹...")
        try:
            full_text = self.extract_full_text_content(pmid)
            result['full_text_extraction'] = full_text
            
            # æ›´æ–°è°ƒè¯•ä¿¡æ¯
            if 'debug_info' in full_text:
                result['debug_info']['extraction_details'] = full_text['debug_info']
            
            if full_text.get('extraction_success', False):
                print(f"âœ… æˆåŠŸæå–PMID {pmid}çš„å…¨æ–‡å†…å®¹")
                content_info = full_text.get('content', {})
                result['extraction_success'] = True
                result['extracted_content'] = content_info
                
                # è¯¦ç»†è¾“å‡ºæå–çš„å†…å®¹ä¿¡æ¯
                print(f"   ğŸ“„ æ ‡é¢˜: {content_info.get('title', 'N/A')[:100]}...")
                if 'abstract' in content_info:
                    print(f"   ğŸ“ æ‘˜è¦: {len(content_info['abstract'])} å­—ç¬¦")
                if 'body_text' in content_info:
                    print(f"   ğŸ“– æ­£æ–‡: {len(content_info['body_text'])} å­—ç¬¦")
                if 'keywords' in content_info:
                    print(f"   ğŸ”‘ å…³é”®è¯: {len(content_info['keywords'])} å­—ç¬¦")
                if 'authors' in content_info:
                    print(f"   ğŸ‘¥ ä½œè€…ä¿¡æ¯: {len(content_info['authors'])} å­—ç¬¦")
                if 'references' in content_info:
                    print(f"   ğŸ“š å‚è€ƒæ–‡çŒ®: {len(content_info['references'])} å­—ç¬¦")
                
                # ç»Ÿè®¡æå–çš„å†…å®¹éƒ¨åˆ†æ•°
                content_parts = len([k for k, v in content_info.items() if v])
                print(f"   ğŸ“Š æ€»è®¡æå–äº† {content_parts} ä¸ªå†…å®¹éƒ¨åˆ†")
                
            else:
                print(f"âŒ PMID {pmid} å…¨æ–‡å†…å®¹æå–å¤±è´¥: {full_text.get('message', 'æœªçŸ¥é”™è¯¯')}")
                result['message'] = full_text.get('message', 'æå–å¤±è´¥')
                
                # æ·»åŠ é”™è¯¯è°ƒè¯•ä¿¡æ¯
                if 'error' in full_text:
                    result['debug_info']['extraction_error'] = full_text['error']
                    print(f"   ğŸ” é”™è¯¯è¯¦æƒ…: {full_text['error']}")
                
                if 'debug_info' in full_text and 'no_content_reason' in full_text['debug_info']:
                    result['debug_info']['no_content_reason'] = full_text['debug_info']['no_content_reason']
                    print(f"   ğŸ” å¤±è´¥åŸå› : {full_text['debug_info']['no_content_reason']}")
        
        except Exception as e:
            print(f"âŒ å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
            result['message'] = f"å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}"
            result['debug_info']['extraction_error'] = str(e)
        
        print(f"\nğŸ“Š PMID {pmid} åˆ†æå®Œæˆ")
        print(f"   - å…è´¹å…¨æ–‡: {'æ˜¯' if result['is_free'] else 'å¦'}")
        print(f"   - æå–æˆåŠŸ: {'æ˜¯' if result['extraction_success'] else 'å¦'}")
        print("=" * 60)
        
        return result


# åˆ›å»ºå…¨å±€å…¨æ–‡æå–å™¨å®ä¾‹
full_text_extractor = FullTextExtractor()


# å‘åå…¼å®¹çš„ä¾¿æ·å‡½æ•°
def check_full_text_availability(pmid: str) -> Dict[str, Any]:
    """æ£€æŸ¥å…è´¹å…¨æ–‡å¯ç”¨æ€§ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return full_text_extractor.check_full_text_availability(pmid)


def extract_full_text_content(pmid: str, link_url: str = None) -> Dict[str, Any]:
    """æå–å…¨æ–‡å†…å®¹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return full_text_extractor.extract_full_text_content(pmid, link_url)


def analyze_pmid_with_full_text(pmid: str) -> Dict[str, Any]:
    """ç»¼åˆåˆ†æPMIDï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return full_text_extractor.analyze_pmid_with_full_text(pmid)