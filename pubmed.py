import pandas as pd
import re
from Bio import Entrez
from datetime import datetime
import requests
import json
import time
import logging
from typing import Dict, Optional
from bs4 import BeautifulSoup

# ================= é…ç½®åŒºåŸŸ =================
# è¯·æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„é‚®ç®±ï¼Œè¿™æ˜¯PubMed APIçš„è¦æ±‚ï¼Œç”¨äºè¿½è¸ªé«˜é¢‘è®¿é—®
Entrez.email = "varian69@gmail.com" 

# æ£€ç´¢å…³é”®è¯ (ä»¥MCTä¸ºä¾‹ï¼Œå¤ç”¨ä¹‹å‰çš„é€»è¾‘)
SEARCH_TERM = """
("Medium-chain triglycerides" OR "MCT" OR "Caprylic acid") AND ("Weight loss" OR "Body composition" OR "Fat mass") AND ("Adults"[Mesh] OR "Adult") AND ("Obesity"[Mesh] OR "Overweight"[Mesh] OR "Obesity" OR "Overweight") NOT ("Diabetes Mellitus"[Mesh] OR "Diabetes" OR "Hypertension"[Mesh] OR "High blood pressure" OR "Cardiovascular Diseases"[Mesh] OR "Metabolic Syndrome"[Mesh] OR "Neoplasms"[Mesh] OR "Cancer" OR "Pregnancy" OR "Pregnant" OR "Child" OR "Adolescent")
"""

# æƒ³è¦è·å–çš„æ–‡çŒ®æ•°é‡
MAX_RESULTS = 100 

# ================= AI APIé…ç½® =================
# å¤šç§APIç«¯ç‚¹å°è¯•
API_ENDPOINTS = [
    "https://api.gptgod.online/v1/chat/completions",
    "https://api.minimax.chat/v1/text/chatcompletion_v2",
    "https://api.deepseek.com/v1/chat/completions"
]
API_KEY = "sk-1wLZqqkXDT9shZzgTqNRc0wNB6K4Kmu1t0kov0KA5I3auqVf"
ENABLE_WEB_SEARCH = True  # æ˜¯å¦å¯ç”¨web searchåŠŸèƒ½
REQUEST_DELAY = 2.0  # APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…429é”™è¯¯

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ================= ä¸»ç¨‹åºé…ç½® =================
# å…¨å±€é…ç½®
ENABLE_FULLTEXT_EXTRACTION = False  # æ˜¯å¦å¯ç”¨å…¨æ–‡æå–åŠŸèƒ½
# ===========================================

def search_pubmed(query, max_results=20):
    """åœ¨PubMedä¸­æœç´¢å¹¶è¿”å›IDåˆ—è¡¨"""
    print(f"æ­£åœ¨æœç´¢: {query.strip()}...")
    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results, sort="relevance")
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")
        return []

def fetch_details(id_list):
    """æ ¹æ®IDè·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
    print(f"æ­£åœ¨è·å– {len(id_list)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯...")
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        return records['PubmedArticle']
    except Exception as e:
        print(f"è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return []

def extract_sample_size(abstract_text):
    """
    å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ‘˜è¦ä¸­æå–æ ·æœ¬é‡ (n=xxx)
    è¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„å¯å‘å¼ç®—æ³•ï¼Œä¸ä¸€å®š100%å‡†ç¡®
    """
    if not abstract_text:
        return "N/A"
    
    # åŒ¹é…å¸¸è§çš„æ ·æœ¬é‡è¡¨è¾¾ï¼Œå¦‚ n=100, 100 participants, 100 subjects
    patterns = [
        r"n\s*=\s*(\d+)",
        r"(\d+)\s*participants",
        r"(\d+)\s*subjects",
        r"(\d+)\s*patients"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            return match.group(1)
    return "éœ€äººå·¥ç¡®è®¤"

def parse_record(article):
    """è§£æå•ç¯‡æ–‡çŒ®ï¼Œæ˜ å°„åˆ°ç›®æ ‡è¡¨æ ¼åˆ—"""
    data = {}
    medline = article['MedlineCitation']
    article_data = medline['Article']
    
    # 1. å‘è¡¨å¹´ä»½
    try:
        pub_date = article_data['Journal']['JournalIssue']['PubDate']
        year = pub_date.get('Year', '')
        if not year and 'MedlineDate' in pub_date:
            year = pub_date['MedlineDate'].split(' ')[0]
        data['å‘è¡¨å¹´ä»½'] = year
    except:
        data['å‘è¡¨å¹´ä»½'] = "N/A"

    # 2. æ•°æ®æ”¶é›†å¹´ä»½ (é€šè¿‡AIæå–)
    data['æ•°æ®æ”¶é›†å¹´ä»½'] = ai_extracted.get('æ•°æ®æ”¶é›†å¹´ä»½', "éœ€äººå·¥ç¡®è®¤")

    # 3. å›½å®¶ (å°è¯•ä»ä½œè€…æœºæ„æå–ï¼Œé€šå¸¸å–ç¬¬ä¸€ä½œè€…)
    data['å›½å®¶'] = extract_country_from_affiliation(article_data)

    # 4. ç ”ç©¶ç±»å‹ (ä»PublicationTypeListæå–)
    try:
        pt_list = [pt.title() for pt in article_data.get('PublicationTypeList', [])]
        if "Meta-Analysis" in pt_list:
            r_type = "Meta-Analysis"
        elif "Randomized Controlled Trial" in pt_list:
            r_type = "RCT"
        elif "Review" in pt_list:
            r_type = "Review"
        else:
            r_type = ", ".join(pt_list)
        data['ç ”ç©¶ç±»å‹'] = r_type
    except:
        data['ç ”ç©¶ç±»å‹'] = "N/A"

    # 5. ç ”ç©¶å¯¹è±¡ & 6. æ ·æœ¬é‡
    abstract_text = ""
    if 'Abstract' in article_data and 'AbstractText' in article_data['Abstract']:
        # AbstractText æœ‰æ—¶æ˜¯åˆ—è¡¨ï¼ˆåˆ†æ®µæ‘˜è¦ï¼‰ï¼Œæœ‰æ—¶æ˜¯å­—ç¬¦ä¸²
        abs_content = article_data['Abstract']['AbstractText']
        if isinstance(abs_content, list):
            abstract_text = " ".join([str(item) for item in abs_content])
        else:
            abstract_text = str(abs_content)
    
    # ä½¿ç”¨AIç»Ÿä¸€æå–ä¿¡æ¯ï¼ˆä¸å†ä¾èµ–æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    logger.info("å¼€å§‹ä½¿ç”¨AIæå–ç ”ç©¶ä¿¡æ¯...")
    
    # ç›´æ¥ä½¿ç”¨AIæå–æ‰€æœ‰ä¿¡æ¯
    ai_extracted = extract_info_with_ai(abstract_text)
    logger.info(f"AIæå–ç»“æœï¼š{ai_extracted}")
    
    # æ›´æ–°æ•°æ®å­—æ®µ
    data['ç ”ç©¶å¯¹è±¡'] = ai_extracted.get('ç ”ç©¶å¯¹è±¡', "éœ€äººå·¥ç¡®è®¤")
    data['æ ·æœ¬é‡'] = ai_extracted.get('æ ·æœ¬é‡', "éœ€äººå·¥ç¡®è®¤")
    data['æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•'] = ai_extracted.get('æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•', "éœ€äººå·¥ç¡®è®¤")
    data['ä½œç”¨æœºç†'] = ai_extracted.get('ä½œç”¨æœºç†', "éœ€äººå·¥ç¡®è®¤")
    data['æ‘˜è¦ä¸»è¦å†…å®¹'] = ai_extracted.get('æ‘˜è¦ä¸»è¦å†…å®¹', "éœ€äººå·¥ç¡®è®¤")
    data['ç»“è®ºæ‘˜è¦'] = abstract_text # ä¿ç•™åŸæ–‡æ‘˜è¦ä¾›å‚è€ƒ
    
    # 9. è¯æ®ç­‰çº§ (åŸºäºç ”ç©¶ç±»å‹é¢„åˆ¤)
    if "Meta-Analysis" in data['ç ”ç©¶ç±»å‹']:
        data['è¯æ®ç­‰çº§'] = "Level 1"
    elif "RCT" in data['ç ”ç©¶ç±»å‹']:
        data['è¯æ®ç­‰çº§'] = "Level 2"
    else:
        data['è¯æ®ç­‰çº§'] = "å¾…å®š"

    # é¢å¤–ä¿¡æ¯æ–¹ä¾¿æ ¸å¯¹
    data['æ ‡é¢˜'] = article_data.get('ArticleTitle', '')
    data['PMID'] = medline.get('PMID', '')

    # å¦‚æœå¯ç”¨å…¨æ–‡æå–åŠŸèƒ½ï¼Œè·å–PMIDå¹¶è¿›è¡Œå…¨æ–‡åˆ†æ
    if ENABLE_FULLTEXT_EXTRACTION and data['PMID']:
        try:
            print(f"  ğŸ” æ­£åœ¨æ£€æŸ¥PMID {data['PMID']} çš„å…¨æ–‡å¯ç”¨æ€§...")
            
            # ä½¿ç”¨å…¨æ–‡åˆ†æåŠŸèƒ½
            fulltext_analysis = analyze_pmid_with_full_text(data['PMID'])
            
            # å°†å…¨æ–‡åˆ†æç»“æœæ·»åŠ åˆ°æ•°æ®ä¸­
            data['å…è´¹å…¨æ–‡çŠ¶æ€'] = fulltext_analysis.get('is_free', False)
            data['å…è´¹å…¨æ–‡é“¾æ¥æ•°'] = len(fulltext_analysis.get('links', []))
            data['å…¨æ–‡æå–çŠ¶æ€'] = fulltext_analysis.get('extraction_success', False)
            data['å…¨æ–‡å†…å®¹æ‘˜è¦'] = fulltext_analysis.get('extracted_content', {}).get('abstract', 'æœªæå–')
            
            if fulltext_analysis.get('is_free'):
                print(f"  âœ… å‘ç°å…è´¹å…¨æ–‡: {data['å…è´¹å…¨æ–‡é“¾æ¥æ•°']} ä¸ªé“¾æ¥")
            else:
                print(f"  âŒ æ— å…è´¹å…¨æ–‡")
                
        except Exception as e:
            logger.error(f"å¤„ç†PMID {data['PMID']} å…¨æ–‡åˆ†ææ—¶å‡ºé”™: {e}")
            data['å…è´¹å…¨æ–‡çŠ¶æ€'] = False
            data['å…è´¹å…¨æ–‡é“¾æ¥æ•°'] = 0
            data['å…¨æ–‡æå–çŠ¶æ€'] = False
            data['å…¨æ–‡å†…å®¹æ‘˜è¦'] = "åˆ†æå¤±è´¥"

    return data

def extract_country_from_affiliation(article_data: Dict) -> str:
    """
    ä»ä½œè€…æœºæ„ä¿¡æ¯ä¸­æå–å›½å®¶åç§°
    
    Args:
        article_data: ä»PubMedè·å–çš„æ–‡ç« æ•°æ®
        
    Returns:
        å›½å®¶åç§°å­—ç¬¦ä¸²
    """
    # é¢„å®šä¹‰çš„å›½å®¶åˆ—è¡¨ï¼ˆåŒ…å«å¸¸è§å›½å®¶åç§°å’Œå˜ä½“ï¼‰
    country_mappings = {
        # åŒ—ç¾æ´²
        "United States": "United States", "USA": "United States", "US": "United States", "America": "United States", "American": "United States",
        "Canada": "Canada", "Canadian": "Canada", 
        "Mexico": "Mexico", "Mexican": "Mexico",
        
        # æ¬§æ´²
        "United Kingdom": "United Kingdom", "UK": "United Kingdom", "Britain": "United Kingdom", "British": "United Kingdom", "England": "United Kingdom", "English": "United Kingdom", "Scotland": "United Kingdom", "Scottish": "United Kingdom", "Wales": "United Kingdom", "Welsh": "United Kingdom",
        "Germany": "Germany", "German": "Germany", "Deutschland": "Germany",
        "France": "France", "French": "France", 
        "Italy": "Italy", "Italian": "Italy",
        "Spain": "Spain", "Spanish": "Spain",
        "Netherlands": "Netherlands", "Dutch": "Netherlands",
        "Sweden": "Sweden", "Swedish": "Sweden",
        "Norway": "Norway", "Norwegian": "Norway",
        "Denmark": "Denmark", "Danish": "Denmark",
        "Finland": "Finland", "Finnish": "Finland",
        "Switzerland": "Switzerland", "Swiss": "Switzerland",
        "Austria": "Austria", "Austrian": "Austria",
        "Belgium": "Belgium", "Belgian": "Belgium",
        "Poland": "Poland", "Polish": "Poland",
        "Czech": "Czech Republic", "Czechia": "Czech Republic",
        "Portugal": "Portugal", "Portuguese": "Portugal",
        "Greece": "Greece", "Greek": "Greece",
        "Russia": "Russia", "Russian": "Russia",
        
        # äºšæ´²
        "China": "China", "Chinese": "China", "Beijing": "China", "Shanghai": "China", "Guangzhou": "China", "Shenzhen": "China",
        "Japan": "Japan", "Japanese": "Japan", "Tokyo": "Japan", "Osaka": "Japan",
        "Korea": "South Korea", "Korean": "South Korea", "Seoul": "South Korea",
        "South Korea": "South Korea", 
        "India": "India", "Indian": "India", "Mumbai": "India", "Delhi": "India",
        "Singapore": "Singapore", "Singaporean": "Singapore",
        "Thailand": "Thailand", "Thai": "Thailand",
        "Malaysia": "Malaysia", "Malaysian": "Malaysia",
        "Indonesia": "Indonesia", "Indonesian": "Indonesia",
        "Philippines": "Philippines", "Philippine": "Philippines",
        "Vietnam": "Vietnam", "Vietnamese": "Vietnam",
        "Taiwan": "Taiwan", "Taiwanese": "Taiwan",
        "Hong Kong": "Hong Kong",
        
        # å¤§æ´‹æ´²
        "Australia": "Australia", "Australian": "Australia", "Sydney": "Australia", "Melbourne": "Australia",
        "New Zealand": "New Zealand", "NZ": "New Zealand", "Auckland": "New Zealand",
        
        # éæ´²
        "South Africa": "South Africa", "Egypt": "Egypt", "Egyptian": "Egypt",
        "Nigeria": "Nigeria", "Ghana": "Ghana", "Kenya": "Kenya",
        
        # å—ç¾æ´²
        "Brazil": "Brazil", "Brazilian": "Brazil", "Argentina": "Argentina", "Chile": "Chile", "Colombia": "Colombia"
    }
    
    # éœ€è¦è¿‡æ»¤çš„éå›½å®¶è¯æ±‡
    invalid_country_indicators = [
        # åŸå¸‚å’Œåœ°åŒº
        "street", "st.", "avenue", "ave.", "road", "rd.", "boulevard", "blvd.",
        "hospital", "university", "college", "institute", "school", "department",
        "center", "centre", "laboratory", "lab", "building", "floor", "room",
        "zip", "postal", "postcode", "code", "district", "province", "state",
        # é‚®æ”¿ç¼–ç æ ¼å¼
        r'\d{5}(-\d{4})?', r'[A-Z]\d[A-Z] \d[A-Z]\d', r'\d{4}-\d{3}', r'\d{3}\s?\d{3}',
        # ç‰¹æ®Šæ ¼å¼
        "H9X", "M5V", "V1M", "SW3P", "WC1N", "1A1", "2B2"
    ]
    
    try:
        # å°è¯•ä»ç¬¬ä¸€ä½œè€…æå–æœºæ„ä¿¡æ¯
        if 'AuthorList' not in article_data or not article_data['AuthorList']:
            return "éœ€äººå·¥ç¡®è®¤"
            
        first_author = article_data['AuthorList'][0]
        
        # è·å–æœºæ„ä¿¡æ¯
        affiliation = ""
        if 'AffiliationInfo' in first_author and first_author['AffiliationInfo']:
            affiliation = first_author['AffiliationInfo'][0].get('Affiliation', '')
        elif 'Affiliation' in first_author:
            affiliation = first_author['Affiliation']
        
        if not affiliation:
            return "éœ€äººå·¥ç¡®è®¤"
        
        logger.info(f"æå–åˆ°çš„æœºæ„ä¿¡æ¯: {affiliation[:200]}...")
        
        # æ¸…ç†æœºæ„ä¿¡æ¯
        affiliation = affiliation.replace('\n', ' ').replace('\r', ' ')
        affiliation_parts = [part.strip() for part in affiliation.split(',')]
        
        # æå–å›½å®¶å…³é”®è¯
        country_candidates = []
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦åŒ…å«å›½å®¶ä¿¡æ¯
        logger.info(f"æœºæ„ä¿¡æ¯åˆ†å‰²å: {affiliation_parts}")
        
        for part in affiliation_parts:
            part_upper = part.upper().strip()
            part_lower = part.lower().strip()
            
            logger.info(f"æ£€æŸ¥éƒ¨åˆ†: '{part}' -> ä¸Š: '{part_upper}' -> ä¸‹: '{part_lower}'")
            
            # å…ˆæ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥å›½å®¶
            matched_country = None
            for country_key, country_name in country_mappings.items():
                if (country_key.upper() in part_upper or 
                    country_key.lower() in part_lower):
                    logger.info(f"åŒ¹é…åˆ°å›½å®¶å…³é”®è¯: '{country_key}' -> '{country_name}'")
                    matched_country = country_name
                    break
            
            if matched_country:
                country_candidates.append(matched_country)
                logger.info(f"æ·»åŠ åˆ°å€™é€‰å›½å®¶åˆ—è¡¨: {matched_country}")
                continue
                
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å·²çŸ¥å›½å®¶ï¼Œå†æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆæŒ‡æ ‡
            is_invalid = False
            for invalid in invalid_country_indicators:
                if isinstance(invalid, str):
                    if invalid.lower() in part_lower:
                        is_invalid = True
                        break
                else:  # æ­£åˆ™è¡¨è¾¾å¼
                    if invalid.search(part):
                        is_invalid = True
                        break
            
            if is_invalid:
                continue
        
        # å¦‚æœæ‰¾åˆ°å›½å®¶å€™é€‰ï¼Œè¿”å›æœ€å¯èƒ½çš„
        if country_candidates:
            # ä¼˜å…ˆè¿”å›United Statesæˆ–Chinaï¼ˆæœ€å¸¸è§ï¼‰ï¼Œå¦åˆ™è¿”å›ç¬¬ä¸€ä¸ª
            for priority_country in ["United States", "China", "United Kingdom", "Germany", "Japan", "Australia"]:
                if priority_country in country_candidates:
                    return priority_country
            return country_candidates[0]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢„å®šä¹‰å›½å®¶ï¼Œå°è¯•æ™ºèƒ½æå–
        # æå–æœ€åä¸€ä¸ªé€—å·åˆ†éš”çš„éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯å›½å®¶ï¼‰
        potential_country = affiliation_parts[-1].strip()
        
        # éªŒè¯æå–çš„å›½å®¶æ˜¯å¦æœ‰æ•ˆ
        if is_likely_country(potential_country):
            return potential_country
        
        return "éœ€äººå·¥ç¡®è®¤"
        
    except Exception as e:
        logger.error(f"æå–å›½å®¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return "éœ€äººå·¥ç¡®è®¤"

def is_likely_country(text: str) -> bool:
    """
    éªŒè¯æå–çš„æ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯å›½å®¶åç§°
    
    Args:
        text: å¾…éªŒè¯çš„æ–‡æœ¬
        
    Returns:
        å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦æ˜¯å¯èƒ½çš„å›½å®¶åç§°
    """
    if not text or len(text.strip()) < 2:
        return False
    
    text = text.strip()
    
    # é•¿åº¦é™åˆ¶ï¼ˆå›½å®¶åç§°é€šå¸¸2-30ä¸ªå­—ç¬¦ï¼‰
    if len(text) < 2 or len(text) > 30:
        return False
    
    # ä¸èƒ½åŒ…å«æ•°å­—ï¼ˆé‚®æ”¿ç¼–ç ï¼‰
    if any(char.isdigit() for char in text):
        return False
    
    # ä¸èƒ½åŒ…å«å¸¸è§çš„éå›½å®¶è¯æ±‡
    invalid_patterns = [
        r'\d+',  # åŒ…å«æ•°å­—
        r'^\d',  # ä»¥æ•°å­—å¼€å¤´
        r'\d$',  # ä»¥æ•°å­—ç»“å°¾
        r'[A-Z]\d[A-Z]',  # é‚®æ”¿ç¼–ç æ ¼å¼
        r'\d[A-Z]\d',  # é‚®æ”¿ç¼–ç æ ¼å¼
        r'(street|st\.?|avenue|ave\.?|road|rd\.?)',  # è¡—é“
        r'(hospital|university|college|institute)',  # æœºæ„
        r'(zip|postal|postcode)',  # é‚®æ”¿ç¼–ç 
        r'(building|floor|room)',  # å»ºç­‘ä¿¡æ¯
    ]
    
    import re
    for pattern in invalid_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¸¸è§çš„å›½å®¶åç§°æ ¼å¼
    country_patterns = [
        r'^[A-Z][a-z]+$',  # é¦–å­—æ¯å¤§å†™ï¼Œå¦‚"Germany"
        r'^[A-Z][a-z]+ [A-Z][a-z]+$',  # ä¸¤ä¸ªè¯ï¼Œå¦‚"New Zealand"
        r'^[A-Z]+$',  # å…¨å¤§å†™ï¼Œå¦‚"USA"
        r'^[A-Z][a-z]+$',  # æ ‡å‡†å›½å®¶åæ ¼å¼
    ]
    
    for pattern in country_patterns:
        if re.match(pattern, text):
            return True
    
    return False

def extract_info_with_regex(abstract_text: str) -> Dict[str, str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ‘˜è¦ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
    """
    result = {
        "ç ”ç©¶å¯¹è±¡": "æœªæ˜ç¡®è¯´æ˜",
        "æ ·æœ¬é‡": "æœªæ˜ç¡®è¯´æ˜", 
        "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "æœªæ˜ç¡®è¯´æ˜",
        "ä½œç”¨æœºç†": "æœªæ˜ç¡®è¯´æ˜"
    }
    
    if not abstract_text:
        return result
    
    # æå–æ ·æœ¬é‡
    sample_patterns = [
        r"(\d+)\s*participants?",
        r"(\d+)\s*subjects?",
        r"(\d+)\s*patients?",
        r"n\s*=\s*(\d+)",
        r"sample\s+size\s+(?:of\s+)?(\d+)",
        r"involving\s+(\d+)\s+(?:participants|subjects|patients)",
        r"(\d+)\s+(?:healthy\s+)?volunteers?"
    ]
    
    for pattern in sample_patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            result["æ ·æœ¬é‡"] = match.group(1)
            break
    
    # æå–ç ”ç©¶å¯¹è±¡ç‰¹å¾
    subject_patterns = [
        r"(\d+)-?(\d+)?\s*years?\s+old",
        r"aged\s+(\d+)-?(\d+)?\s*years?",
        r"(overweight|obese|healthy)\s+(?:adults?|participants?)",
        r"(men|women|male|female)",
        r"(diabetes|hypertension|metabolic\s+syndrome)",
        r"(BMI\s+\d+-\d+)"
    ]
    
    subjects = []
    for pattern in subject_patterns:
        matches = re.findall(pattern, abstract_text, re.IGNORECASE)
        if matches:
            if isinstance(matches[0], tuple):
                subjects.extend([m for m in matches[0] if m])
            else:
                subjects.extend(matches)
    
    if subjects:
        result["ç ”ç©¶å¯¹è±¡"] = "ã€".join(subjects)
    
    # æå–å‰‚é‡ä¿¡æ¯
    dose_patterns = [
        r"(\d+)\s*ml\s+(?:MCT\s+)?oil\s+daily",
        r"(\d+)\s*g\s+(?:MCT\s+)?daily", 
        r"(\d+)\s*ml\s+per\s+day",
        r"(\d+)\s*g\s+per\s+day",
        r"(\d+)\s*ml\s+twice\s+daily",
        r"(\d+)\s*g\s+(\d+)\s*times\s+per\s+day"
    ]
    
    for pattern in dose_patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            if len(match.groups()) == 2:
                result["æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•"] = f"{match.group(1)}g {match.group(2)}æ¬¡/å¤©"
            else:
                result["æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•"] = match.group(0)
            break
    
    # æå–ä½œç”¨æœºç†
    mechanism_patterns = [
        r"(increase\s+thermogenesis)",
        r"(promote\s+fat\s+oxidation)",
        r"(enhance\s+ketone\s+production)",
        r"(increase\s+energy\s+expenditure)",
        r"(enhance\s+satiety)",
        r"(promote\s+ketogenesis)",
        r"(stimulate\s+uncoupling\s+proteins)",
        r"(rapid\s+oxidation)"
    ]
    
    mechanisms = []
    for pattern in mechanism_patterns:
        matches = re.findall(pattern, abstract_text, re.IGNORECASE)
        if matches:
            mechanisms.extend(matches)
    
    if mechanisms:
        result["ä½œç”¨æœºç†"] = "ï¼›".join(mechanisms)
    
    return result

def extract_info_with_ai(abstract_text: str) -> Dict[str, str]:
    """
    ä½¿ç”¨GPT-4.1 APIä»æ‘˜è¦ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
    
    Args:
        abstract_text: æ–‡çŒ®æ‘˜è¦æ–‡æœ¬
        
    Returns:
        åŒ…å«æå–ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - ç ”ç©¶å¯¹è±¡
        - æ ·æœ¬é‡  
        - æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•
        - ä½œç”¨æœºç†
        - æ‘˜è¦ä¸»è¦å†…å®¹
        - æ•°æ®æ”¶é›†å¹´ä»½
    """
    if not abstract_text or abstract_text.strip() == "":
        logger.warning("æ‘˜è¦æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é»˜è®¤ç©ºå€¼")
        return {
            "ç ”ç©¶å¯¹è±¡": "éœ€äººå·¥ç¡®è®¤",
            "æ ·æœ¬é‡": "éœ€äººå·¥ç¡®è®¤", 
            "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "éœ€äººå·¥ç¡®è®¤",
            "ä½œç”¨æœºç†": "éœ€äººå·¥ç¡®è®¤",
            "æ‘˜è¦ä¸»è¦å†…å®¹": "éœ€äººå·¥ç¡®è®¤",
            "å›½å®¶": "éœ€äººå·¥ç¡®è®¤",
            "æ•°æ®æ”¶é›†å¹´ä»½": "éœ€äººå·¥ç¡®è®¤"
        }
    
    # æ„å»ºå…¨ä¸­æ–‡æç¤ºè¯ï¼Œè¦æ±‚AIä»æ‘˜è¦ä¸­æå–ç‰¹å®šä¿¡æ¯
    prompt = f"""
è¯·åˆ†æä»¥ä¸‹è‹±æ–‡å­¦æœ¯æ–‡çŒ®æ‘˜è¦ï¼Œå¹¶æå–ä»¥ä¸‹ä¸ƒä¸ªæ–¹é¢çš„ä¸­æ–‡ä¿¡æ¯ï¼š

**æ‘˜è¦åŸæ–‡ï¼š**
{abstract_text}

**è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæ‘˜è¦ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ ‡æ³¨"æœªæ˜ç¡®è¯´æ˜"ï¼‰ï¼š**

1. **ç ”ç©¶å¯¹è±¡**ï¼šç ”ç©¶æ¶‰åŠçš„äººç¾¤ç‰¹å¾ï¼ˆå¹´é¾„èŒƒå›´ã€æ€§åˆ«ã€å¥åº·çŠ¶å†µã€BMIèŒƒå›´ç­‰ï¼‰
   - ä¾‹å¦‚ï¼š18-65å²å¥åº·æˆå¹´äººï¼Œè‚¥èƒ–å¥³æ€§ï¼Œä»£è°¢ç»¼åˆå¾æ‚£è€…ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œä¸èƒ½å‡ºç°è‹±æ–‡å•è¯å¦‚"men"ã€"women"ç­‰

2. **æ ·æœ¬é‡**ï¼šç ”ç©¶ä¸­çš„å‚ä¸è€…æ•°é‡å’Œç±»å‹
   - ä¾‹å¦‚ï¼š120åå‚ä¸è€…ï¼Œ60ä¾‹æ‚£è€…ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡

3. **æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•**ï¼šç ”ç©¶ä¸­çš„MCTæˆ–ç›¸å…³è¥å…»ç´ è¡¥å……æ–¹æ¡ˆ
   - ä¾‹å¦‚ï¼šæ¯æ—¥30æ¯«å‡MCTæ²¹ï¼Œåˆ†2æ¬¡æœç”¨ï¼›æ¯é¤å‰10å…‹MCTç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œæ•°å­—å’Œå•ä½è¦æ¸…æ™°

4. **ä½œç”¨æœºç†**ï¼šMCTå‘æŒ¥æ•ˆåº”çš„ç”Ÿç‰©å­¦æœºåˆ¶
   - ä¾‹å¦‚ï¼šé€šè¿‡ç”Ÿé…®ä½œç”¨ä¿ƒè¿›è„‚è‚ªç‡ƒçƒ§ï¼›æé«˜ä»£è°¢ç‡ï¼›æŠ‘åˆ¶é£Ÿæ¬²ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œç”¨ç§‘å­¦æœ¯è¯­æè¿°

5. **æ‘˜è¦ä¸»è¦å†…å®¹**ï¼šç”¨1-2å¥è¯æ¦‚æ‹¬è¯¥ç ”ç©¶çš„é‡ç‚¹å‘ç°å’Œç»“è®º
   - ä¾‹å¦‚ï¼šç ”ç©¶å‘ç°æ¯æ—¥è¡¥å……30æ¯«å‡MCTæ²¹å¯ä»¥æ˜¾è‘—å‡å°‘è¶…é‡æˆå¹´äººçš„ä½“è„‚å«é‡
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œç®€æ´æ˜äº†

6. **å›½å®¶**ï¼šç ”ç©¶è¿›è¡Œæ‰€åœ¨çš„å›½å®¶
   - ä¾‹å¦‚ï¼šç¾å›½ã€ä¸­å›½ã€è‹±å›½ã€å¾·å›½ã€æ—¥æœ¬ã€æ¾³å¤§åˆ©äºšç­‰
   - åªè¿”å›æ ‡å‡†å›½å®¶åç§°ï¼Œå¦‚"USA"å¯¹åº”"ç¾å›½"ï¼Œ"China"å¯¹åº”"ä¸­å›½"
   - ç»ä¸èƒ½åŒ…å«åŸå¸‚åï¼ˆå¦‚Beijingã€Shanghaiã€New Yorkã€Londonç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«é‚®æ”¿ç¼–ç ï¼ˆå¦‚H9X 3V9ã€M5Vã€V1Mç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«æœºæ„åç§°ï¼ˆå¦‚Universityã€Hospitalã€Instituteç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«è¡—é“åœ°å€ï¼ˆå¦‚Streetã€Roadã€Avenueç­‰ï¼‰
   - å¦‚æœæ— æ³•ç¡®å®šå‡†ç¡®çš„å›½å®¶ï¼Œæ ‡æ³¨"éœ€äººå·¥ç¡®è®¤"

7. **æ•°æ®æ”¶é›†å¹´ä»½**ï¼šç ”ç©¶å®é™…æ•°æ®æ”¶é›†çš„æ—¶é—´æœŸé—´
   - ä¾‹å¦‚ï¼š2018å¹´1æœˆè‡³12æœˆï¼Œ2019å¹´6æœˆ-2020å¹´5æœˆï¼Œ2020å¹´ç­‰
   - åªè¿”å›å…·ä½“å¹´ä»½æˆ–å¹´ä»½èŒƒå›´ï¼Œä¸è¦åŒ…å«å‘è¡¨å¹´ä»½
   - å¦‚æœæ‘˜è¦ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°æ•°æ®æ”¶é›†æ—¶é—´ï¼Œæ ‡æ³¨"æœªæ˜ç¡®è¯´æ˜"

**è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š**
```json
{{
  "ç ”ç©¶å¯¹è±¡": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ ·æœ¬é‡": "æå–çš„ä¸­æ–‡å†…å®¹", 
  "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "æå–çš„ä¸­æ–‡å†…å®¹",
  "ä½œç”¨æœºç†": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ‘˜è¦ä¸»è¦å†…å®¹": "æå–çš„ä¸­æ–‡å†…å®¹",
  "å›½å®¶": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ•°æ®æ”¶é›†å¹´ä»½": "æå–çš„ä¸­æ–‡å†…å®¹"
}}
```

**é‡è¦è¦æ±‚ï¼š**
- æ‰€æœ‰ç­”æ¡ˆå¿…é¡»æ˜¯çº¯ä¸­æ–‡ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡å•è¯
- **å›½å®¶å­—æ®µç‰¹åˆ«è¦æ±‚**ï¼šç»å¯¹ä¸èƒ½è¿”å›åŸå¸‚ã€é‚®æ”¿ç¼–ç ã€æœºæ„åç§°æˆ–åœ°å€ä¿¡æ¯
- **æ•°æ®æ”¶é›†å¹´ä»½å­—æ®µç‰¹åˆ«è¦æ±‚**ï¼šå¿…é¡»åŒºåˆ†å‘è¡¨å¹´ä»½å’Œæ•°æ®æ”¶é›†å¹´ä»½ï¼Œå‘è¡¨å¹´ä»½ä¸æ˜¯æ•°æ®æ”¶é›†å¹´ä»½
- åªæå–æ‘˜è¦ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸è¦æ¨æ–­
- å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œä½¿ç”¨"æœªæ˜ç¡®è¯´æ˜"æˆ–"éœ€äººå·¥ç¡®è®¤"
- è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSON
"""
  
    # å°è¯•ä¸åŒçš„APIç«¯ç‚¹å’Œæ¨¡å‹ç»„åˆ
    model_configs = [
        # ç«¯ç‚¹, æ¨¡å‹åç§°
        ("gpt-3.5-turbo", API_ENDPOINTS[0]),  # GPTGod + gpt-3.5
        ("gpt-4", API_ENDPOINTS[0]),  # GPTGod + gpt-4
        ("deepseek-chat", API_ENDPOINTS[2])  # DeepSeek + deepseek-chat
    ]
    
    for model_name, api_base_url in model_configs:
        headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.1
        }
        
        try:
            logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹ {model_name} åœ¨ç«¯ç‚¹ {api_base_url}ï¼Œæ‘˜è¦é•¿åº¦ï¼š{len(abstract_text)}å­—ç¬¦")
            
            # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…429é”™è¯¯
            time.sleep(REQUEST_DELAY)
            
            # å‘é€APIè¯·æ±‚
            response = requests.post(api_base_url, headers=headers, json=payload, timeout=30)
            
            # å¤„ç†APIå“åº”
            if response.status_code == 200:
                result = response.json()
                ai_content = result['choices'][0]['message']['content']
                logger.info(f"AI APIè°ƒç”¨æˆåŠŸï¼Œæ¨¡å‹ï¼š{model_name}")
                
                # æå–JSONéƒ¨åˆ†
                try:
                    # å°è¯•ä»AIå“åº”ä¸­æå–JSON
                    json_start = ai_content.find('{')
                    json_end = ai_content.rfind('}') + 1
                    if json_start != -1 and json_end != 0:
                        json_str = ai_content[json_start:json_end]
                        extracted_data = json.loads(json_str)
                        
                        # éªŒè¯æå–çš„æ•°æ®
                        validated_data = validate_extracted_data(extracted_data)
                        logger.info(f"æˆåŠŸæå–ä¿¡æ¯ï¼š{validated_data}")
                        return validated_data
                    else:
                        raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                        
                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"JSONè§£æå¤±è´¥ï¼š{e}ï¼ŒAIå“åº”ï¼š{ai_content}")
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                    
            elif response.status_code == 429:
                logger.warning(f"APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œæ¨¡å‹ï¼š{model_name}")
                time.sleep(REQUEST_DELAY * 5)  # ç­‰å¾…æ›´é•¿æ—¶é—´
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼Œæ¨¡å‹ï¼š{model_name}ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                
        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯ï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œé”™è¯¯ï¼š{e}")
            continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        except Exception as e:
            logger.error(f"AIä¿¡æ¯æå–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œé”™è¯¯ï¼š{e}")
            continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
    
    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    logger.warning("æ‰€æœ‰AIæ¨¡å‹éƒ½è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
    return get_fallback_data()

def validate_extracted_data(data: Dict[str, str]) -> Dict[str, str]:
    """
    éªŒè¯å’Œæ¸…ç†æå–çš„æ•°æ®
    """
    validated = {}
    for key in ["ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•", "ä½œç”¨æœºç†", "æ‘˜è¦ä¸»è¦å†…å®¹", "å›½å®¶", "æ•°æ®æ”¶é›†å¹´ä»½"]:
        value = data.get(key, "N/A")
        # æ¸…ç†å’ŒéªŒè¯å€¼
        if isinstance(value, str):
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            value = value.strip()
            # å¦‚æœä¸ºç©ºæˆ–åŒ…å«æ— æ•ˆå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not value or value.lower() in ["null", "none", "", "undefined"]:
                value = "æœªæ˜ç¡®è¯´æ˜"
        else:
            value = "æœªæ˜ç¡®è¯´æ˜"
        validated[key] = value
    
    return validated

def get_fallback_data() -> Dict[str, str]:
    """
    å½“AIæå–å¤±è´¥æ—¶è¿”å›çš„å¤‡ç”¨æ•°æ®
    """
    return {
        "ç ”ç©¶å¯¹è±¡": "éœ€äººå·¥ç¡®è®¤",
        "æ ·æœ¬é‡": "éœ€äººå·¥ç¡®è®¤", 
        "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "éœ€äººå·¥ç¡®è®¤",
        "ä½œç”¨æœºç†": "éœ€äººå·¥ç¡®è®¤",
        "æ‘˜è¦ä¸»è¦å†…å®¹": "éœ€äººå·¥ç¡®è®¤",
        "å›½å®¶": "éœ€äººå·¥ç¡®è®¤",
        "æ•°æ®æ”¶é›†å¹´ä»½": "éœ€äººå·¥ç¡®è®¤"
    }

# ================= å…¨æ–‡æå–åŠŸèƒ½ =================
def check_full_text_availability(pmid: str) -> Dict[str, any]:
    """
    æ£€æŸ¥PMIDå¯¹åº”çš„æ–‡ç« æ˜¯å¦æä¾›å…è´¹å…¨æ–‡
    é‡ç‚¹æ£€æŸ¥title="Free full text at PubMed Central"çš„aå…ƒç´ 
    
    Args:
        pmid: PubMed ID
    
    Returns:
        åŒ…å«å…è´¹çŠ¶æ€å’Œé“¾æ¥ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # æ„å»ºPubMedé¡µé¢URL
        pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
        print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {pubmed_url}")
        
        # è·å–é¡µé¢å†…å®¹ï¼Œæ·»åŠ æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(pubmed_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä¼˜å…ˆçº§1ï¼šç›´æ¥æŸ¥æ‰¾title="Free full text at PubMed Central"çš„aå…ƒç´ 
        pmc_free_link = soup.find('a', title="Free full text at PubMed Central")
        if pmc_free_link:
            href = pmc_free_link.get('href', '')
            if href:
                full_url = href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}"
                print(f"âœ… æ‰¾åˆ°PMCå…è´¹å…¨æ–‡é“¾æ¥: {full_url}")
                return {
                    "is_free": True,
                    "pmid": pmid,
                    "pubmed_url": pubmed_url,
                    "links": [{
                        "url": full_url,
                        "title": "Free full text at PubMed Central",
                        "is_free": True,
                        "element_found": "title attribute"
                    }],
                    "message": "æ‰¾åˆ°PMCå…è´¹å…¨æ–‡",
                    "source": "direct_title_match"
                }
        
        # ä¼˜å…ˆçº§2ï¼šæŸ¥æ‰¾Full text linkséƒ¨åˆ†
        full_text_section = soup.find('div', {'data-content-id': 'full-text-links'})
        if not full_text_section:
            full_text_section = soup.find('div', class_='full-text-links')
        
        # ä¼˜å…ˆçº§3ï¼šåœ¨å…¨æ–‡é“¾æ¥å®¹å™¨ä¸­æŸ¥æ‰¾å…è´¹é“¾æ¥
        free_links = []
        all_links = []
        
        if full_text_section:
            link_elements = full_text_section.find_all('a', href=True)
            print(f"ğŸ“„ åœ¨å…¨æ–‡é“¾æ¥éƒ¨åˆ†æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")
        else:
            # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„å…¨æ–‡é“¾æ¥éƒ¨åˆ†ï¼ŒæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            link_elements = soup.find_all('a', href=True)
            # ç­›é€‰å¯èƒ½ç›¸å…³çš„é“¾æ¥
            link_elements = [link for link in link_elements if any(keyword in link.get('href', '').lower() 
                               for keyword in ['pmc', 'europepmc', 'full', 'text', 'article'])]
            print(f"ğŸ”— æ‰¾åˆ° {len(link_elements)} ä¸ªç›¸å…³é“¾æ¥")
        
        # åˆ†ææ¯ä¸ªé“¾æ¥
        for link in link_elements:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            title_attr = link.get('title', '')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå…è´¹é“¾æ¥
            is_free = False
            free_indicators = []
            
            # æ£€æŸ¥å„ç§å…è´¹æŒ‡æ ‡
            if 'pmc' in href.lower():
                is_free = True
                free_indicators.append('PMC URL')
            if 'europepmc' in href.lower():
                is_free = True
                free_indicators.append('EuropePMC URL')
            if title_attr and 'free' in title_attr.lower():
                is_free = True
                free_indicators.append('Free title')
            if text and 'free' in text.lower():
                is_free = True
                free_indicators.append('Free text')
            if 'pubmedcentral' in href.lower():
                is_free = True
                free_indicators.append('PubMed Central')
            
            link_info = {
                "url": href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}",
                "title": text,
                "title_attr": title_attr,
                "is_free": is_free,
                "indicators": free_indicators
            }
            
            all_links.append(link_info)
            if is_free:
                free_links.append(link_info)
                print(f"âœ… å‘ç°å…è´¹é“¾æ¥: {text} - {link_info['url']} ({', '.join(free_indicators)})")
        
        # ç¡®å®šæœ€ç»ˆç»“æœ
        if free_links:
            return {
                "is_free": True,
                "pmid": pmid,
                "pubmed_url": pubmed_url,
                "links": free_links,
                "all_links": all_links,
                "message": f"æ‰¾åˆ° {len(free_links)} ä¸ªå…è´¹å…¨æ–‡é“¾æ¥",
                "source": "link_analysis"
            }
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…è´¹é“¾æ¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»˜è´¹é“¾æ¥
            has_paid_links = len(all_links) > 0
            return {
                "is_free": False,
                "pmid": pmid,
                "pubmed_url": pubmed_url,
                "links": all_links,
                "message": "æœªæ‰¾åˆ°å…è´¹å…¨æ–‡" + ("ï¼Œä½†æœ‰ä»˜è´¹é“¾æ¥" if has_paid_links else ""),
                "source": "no_free_links" if not has_paid_links else "paid_only"
            }
        
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return {
            "is_free": False,
            "pmid": pmid,
            "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
            "message": "æ— æ³•è·å–é¡µé¢ä¿¡æ¯"
        }
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {str(e)}")
        return {
            "is_free": False,
            "pmid": pmid,
            "error": f"è§£æå¤±è´¥: {str(e)}",
            "message": "é¡µé¢è§£æå‡ºé”™"
        }

def extract_full_text_content(pmid: str, link_url: str = None) -> Dict[str, any]:
    """
    ä»å…è´¹å…¨æ–‡é“¾æ¥æå–æ–‡ç« å†…å®¹
    å¢å¼ºå…ƒç´ å®šä½å’Œå†…å®¹æå–é€»è¾‘
    
    Args:
        pmid: PubMed ID
        link_url: å…¨æ–‡é“¾æ¥URLï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
    
    Returns:
        åŒ…å«æå–å†…å®¹çš„å­—å…¸
    """
    try:
        # å¦‚æœæ²¡æœ‰æä¾›é“¾æ¥URLï¼Œå…ˆæ£€æŸ¥å¯ç”¨æ€§
        if not link_url:
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹PMID {pmid}çš„å…è´¹å…¨æ–‡é“¾æ¥...")
            availability = check_full_text_availability(pmid)
            if not availability['is_free']:
                return {
                    "success": False,
                    "pmid": pmid,
                    "message": availability['message']
                }
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå…è´¹é“¾æ¥
            free_links = [link for link in availability['links'] if link['is_free']]
            if not free_links:
                return {
                    "success": False,
                    "pmid": pmid,
                    "message": "æœªæ‰¾åˆ°å¯ç”¨çš„å…è´¹å…¨æ–‡é“¾æ¥"
                }
            
            link_url = free_links[0]['url']
            print(f"ğŸ“„ é€‰æ‹©å…è´¹å…¨æ–‡é“¾æ¥: {link_url}")
        
        print(f"ğŸ“– æ­£åœ¨æå–PMID {pmid}çš„å…¨æ–‡å†…å®¹...")
        
        # è·å–å…¨æ–‡é¡µé¢ï¼Œä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(link_url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åˆå§‹åŒ–æå–ç»“æœ
        content = {
            "pmid": pmid,
            "source_url": link_url,
            "extraction_success": False,
            "content": {},
            "debug_info": {
                "page_title": "",
                "total_sections": 0,
                "extracted_elements": []
            }
        }
        
        # æå–é¡µé¢æ ‡é¢˜ç”¨äºè°ƒè¯•
        title_tag = soup.find('title')
        if title_tag:
            content['debug_info']['page_title'] = title_tag.get_text(strip=True)
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title_tag.get_text(strip=True)[:100]}...")
        
        # æå–æ ‡é¢˜ - å¤šç§é€‰æ‹©å™¨
        title_selectors = [
            'h1.article-title',
            'h1.title',
            'h1',
            '.article-title',
            '.title',
            'title'
        ]
        
        title_text = ""
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and len(title_text) > 10:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                    content['content']['title'] = title_text
                    content['debug_info']['extracted_elements'].append(f"æ ‡é¢˜: {selector}")
                    print(f"âœ… æå–æ ‡é¢˜: {title_text[:100]}...")
                    break
        
        # æå–æ‘˜è¦ - å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        abstract_selectors = [
            'div.abstract',
            'section.abstract',
            'div[data-section="abstract"]',
            'div#abstract',
            '.abstract-content',
            '.article-abstract',
            'div[class*="abstract"]'
        ]
        
        abstract_text = ""
        for selector in abstract_selectors:
            abstract_elem = soup.select_one(selector)
            if abstract_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in abstract_elem.select('script, style, .reference, .citation'):
                    unwanted.decompose()
                
                abstract_text = abstract_elem.get_text(strip=True)
                if abstract_text and len(abstract_text) > 50:  # ç¡®ä¿æ‘˜è¦æœ‰æ„ä¹‰
                    content['content']['abstract'] = abstract_text
                    content['debug_info']['extracted_elements'].append(f"æ‘˜è¦: {selector}")
                    print(f"âœ… æå–æ‘˜è¦: {len(abstract_text)} å­—ç¬¦")
                    break
        
        # æå–å…³é”®è¯ - å¤šç§ä½ç½®
        keywords_selectors = [
            'div.keywords',
            'div#keywords',
            '.keyword-list',
            '[data-section="keywords"]',
            'p:contains("Keywords")',
            'div:contains("Keywords")'
        ]
        
        keywords_text = ""
        for selector in keywords_selectors:
            keywords_elem = soup.select_one(selector)
            if keywords_elem:
                keywords_text = keywords_elem.get_text(strip=True)
                if keywords_text and len(keywords_text) > 5:
                    content['content']['keywords'] = keywords_text
                    content['debug_info']['extracted_elements'].append(f"å…³é”®è¯: {selector}")
                    print(f"âœ… æå–å…³é”®è¯: {keywords_text[:100]}...")
                    break
        
        # æå–ä½œè€…ä¿¡æ¯
        authors_selectors = [
            'div.authors',
            'ul.author-list',
            '.author-list',
            'div.author-info',
            '[data-section="authors"]'
        ]
        
        authors_text = ""
        for selector in authors_selectors:
            authors_elem = soup.select_one(selector)
            if authors_elem:
                authors_text = authors_elem.get_text(strip=True)
                if authors_text and len(authors_text) > 10:
                    content['content']['authors'] = authors_text
                    content['debug_info']['extracted_elements'].append(f"ä½œè€…: {selector}")
                    print(f"âœ… æå–ä½œè€…ä¿¡æ¯: {authors_text[:100]}...")
                    break
        
        # æå–æ­£æ–‡å†…å®¹ - æ›´æ™ºèƒ½çš„ç­–ç•¥
        body_selectors = [
            'div.article-body',
            'article',
            'div.body-content',
            'div.main-content',
            'div.content',
            'div#content'
        ]
        
        body_text = ""
        for selector in body_selectors:
            body_elem = soup.select_one(selector)
            if body_elem:
                # ç§»é™¤å¯¼èˆªã€å¹¿å‘Šã€å¼•ç”¨ç­‰ä¸éœ€è¦çš„å†…å®¹
                unwanted_selectors = [
                    'script', 'style', 'nav', 'header', 'footer', 
                    '.advertisement', '.sidebar', '.related-articles',
                    '.reference', '.citation', '.author-notes'
                ]
                
                for unwanted_selector in unwanted_selectors:
                    for unwanted in body_elem.select(unwanted_selector):
                        unwanted.decompose()
                
                # æå–æ–‡æœ¬
                body_text = body_elem.get_text(strip=True)
                if body_text and len(body_text) > 200:  # ç¡®ä¿æ­£æ–‡å†…å®¹æœ‰æ„ä¹‰
                    content['content']['body_text'] = body_text
                    content['debug_info']['extracted_elements'].append(f"æ­£æ–‡: {selector}")
                    print(f"âœ… æå–æ­£æ–‡: {len(body_text)} å­—ç¬¦")
                    break
        
        # æå–å‚è€ƒæ–‡çŒ®
        refs_selectors = [
            'div.references',
            'ol.references',
            'ul.references',
            '.reference-list',
            '[data-section="references"]'
        ]
        
        refs_text = ""
        for selector in refs_selectors:
            refs_elem = soup.select_one(selector)
            if refs_elem:
                refs_text = refs_elem.get_text(strip=True)
                if refs_text and len(refs_text) > 50:
                    content['content']['references'] = refs_text
                    content['debug_info']['extracted_elements'].append(f"å‚è€ƒæ–‡çŒ®: {selector}")
                    print(f"âœ… æå–å‚è€ƒæ–‡çŒ®: {len(refs_text)} å­—ç¬¦")
                    break
        
        # ç»Ÿè®¡æå–çš„å…ƒç´ 
        content['debug_info']['total_sections'] = len(content['content'])
        
        # åˆ¤æ–­æå–æ˜¯å¦æˆåŠŸ
        if len(content['content']) >= 2:  # è‡³å°‘æå–åˆ°æ ‡é¢˜å’Œæ‘˜è¦
            content['extraction_success'] = True
            content['message'] = f"æˆåŠŸæå–{len(content['content'])}ä¸ªéƒ¨åˆ†çš„å†…å®¹"
            print(f"âœ… å…¨æ–‡æå–å®Œæˆï¼Œå…±æå–{len(content['content'])}ä¸ªéƒ¨åˆ†")
        else:
            content['extraction_success'] = False
            content['message'] = f"æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†"
            print(f"âš ï¸ æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†")
        
        # å¦‚æœå®Œå…¨æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œæä¾›è°ƒè¯•ä¿¡æ¯
        if not content['content']:
            content['extraction_success'] = False
            content['message'] = "æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹"
            content['debug_info']['no_content_reason'] = "é¡µé¢å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–é“¾æ¥æ— æ•ˆ"
            print(f"âŒ æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
        
        return content
        
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "pmid": pmid,
            "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
            "message": "æ— æ³•è·å–å…¨æ–‡é¡µé¢",
            "debug_info": {"error_type": "network_error"}
        }
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "pmid": pmid,
            "error": f"æå–å¤±è´¥: {str(e)}",
            "message": "å†…å®¹æå–å‡ºé”™",
            "debug_info": {"error_type": "extraction_error", "error_detail": str(e)}
        }

def analyze_pmid_with_full_text(pmid: str) -> Dict[str, any]:
    """
    ç»¼åˆåˆ†æPMIDï¼šæ£€æŸ¥å…è´¹çŠ¶æ€å¹¶æå–å…¨æ–‡å†…å®¹
    å¢å¼ºè°ƒè¯•ä¿¡æ¯å’Œé”™è¯¯å¤„ç†
    
    Args:
        pmid: PubMed ID
    
    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
    """
    print(f"\nğŸ” å¼€å§‹åˆ†æPMID: {pmid}")
    print("=" * 60)
    
    # æ­¥éª¤1ï¼šæ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§
    print("æ­¥éª¤1: æ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§...")
    availability = check_full_text_availability(pmid)
    
    # åˆå§‹åŒ–ç»“æœï¼ŒåŒ…å«parse_recordéœ€è¦çš„å­—æ®µ
    result = {
        "pmid": pmid,
        "timestamp": datetime.now().isoformat(),
        "is_free": availability.get('is_free', False),
        "links": availability.get('links', []),
        "message": availability.get('message', ''),
        "extraction_success": False,
        "extracted_content": {},
        "debug_info": {
            "availability_source": availability.get('source', 'unknown'),
            "total_links_found": len(availability.get('links', [])),
            "extraction_attempted": False,
            "extraction_details": {}
        }
    }
    
    if not availability.get('is_free', False):
        print(f"âŒ PMID {pmid} æ— å…è´¹å…¨æ–‡: {availability.get('message', 'æœªçŸ¥åŸå› ')}")
        result['debug_info']['no_free_reason'] = availability.get('message', 'æœªçŸ¥åŸå› ')
        result['debug_info']['availability_source'] = availability.get('source', 'unknown')
        return result
    
    print(f"âœ… PMID {pmid} æä¾›å…è´¹å…¨æ–‡ (æ¥æº: {availability.get('source', 'unknown')})")
    result['debug_info']['extraction_attempted'] = True
    
    # æ­¥éª¤2ï¼šæå–å…¨æ–‡å†…å®¹
    print("\næ­¥éª¤2: æå–å…¨æ–‡å†…å®¹...")
    try:
        full_text = extract_full_text_content(pmid)
        result['full_text_extraction'] = full_text
        
        # æ›´æ–°è°ƒè¯•ä¿¡æ¯
        if 'debug_info' in full_text:
            result['debug_info']['extraction_details'] = full_text['debug_info']
        
        if full_text.get('extraction_success', False):
            print(f"âœ… æˆåŠŸæå–PMID {pmid}çš„å…¨æ–‡å†…å®¹")
            content_info = full_text.get('content', {})
            result['extraction_success'] = True
            result['extracted_content'] = content_info
            
            # è¯¦ç»†è¾“å‡ºæå–çš„å†…å®¹ä¿¡æ¯
            print(f"   ğŸ“„ æ ‡é¢˜: {content_info.get('title', 'N/A')[:100]}...")
            if 'abstract' in content_info:
                print(f"   ğŸ“ æ‘˜è¦: {len(content_info['abstract'])} å­—ç¬¦")
            if 'body_text' in content_info:
                print(f"   ğŸ“– æ­£æ–‡: {len(content_info['body_text'])} å­—ç¬¦")
            if 'keywords' in content_info:
                print(f"   ğŸ”‘ å…³é”®è¯: {len(content_info['keywords'])} å­—ç¬¦")
            if 'authors' in content_info:
                print(f"   ğŸ‘¥ ä½œè€…ä¿¡æ¯: {len(content_info['authors'])} å­—ç¬¦")
            if 'references' in content_info:
                print(f"   ğŸ“š å‚è€ƒæ–‡çŒ®: {len(content_info['references'])} å­—ç¬¦")
            
            # ç»Ÿè®¡æå–çš„å†…å®¹éƒ¨åˆ†æ•°
            content_parts = len([k for k, v in content_info.items() if v])
            print(f"   ğŸ“Š æ€»è®¡æå–äº† {content_parts} ä¸ªå†…å®¹éƒ¨åˆ†")
            
        else:
            print(f"âŒ PMID {pmid} å…¨æ–‡å†…å®¹æå–å¤±è´¥: {full_text.get('message', 'æœªçŸ¥é”™è¯¯')}")
            result['message'] = full_text.get('message', 'æå–å¤±è´¥')
            
            # æ·»åŠ é”™è¯¯è°ƒè¯•ä¿¡æ¯
            if 'error' in full_text:
                result['debug_info']['extraction_error'] = full_text['error']
                print(f"   ğŸ” é”™è¯¯è¯¦æƒ…: {full_text['error']}")
            
            if 'debug_info' in full_text and 'no_content_reason' in full_text['debug_info']:
                result['debug_info']['no_content_reason'] = full_text['debug_info']['no_content_reason']
                print(f"   ğŸ” å¤±è´¥åŸå› : {full_text['debug_info']['no_content_reason']}")
    
    except Exception as e:
        print(f"âŒ å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        result['message'] = f"å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}"
        result['debug_info']['extraction_error'] = str(e)
    
    print(f"\nğŸ“Š PMID {pmid} åˆ†æå®Œæˆ")
    print(f"   - å…è´¹å…¨æ–‡: {'æ˜¯' if result['is_free'] else 'å¦'}")
    print(f"   - æå–æˆåŠŸ: {'æ˜¯' if result['extraction_success'] else 'å¦'}")
    print("=" * 60)
    
    return result

# ================= æµ‹è¯•åŠŸèƒ½ =================
def test_ai_extraction():
    """
    æµ‹è¯•æ··åˆä¿¡æ¯æå–åŠŸèƒ½çš„ç¤ºä¾‹æ‘˜è¦
    """
    test_abstracts = [
        # ç¤ºä¾‹æ‘˜è¦1 - RCTç ”ç©¶
        """
        Background: Medium-chain triglycerides (MCT) have been proposed as a dietary supplement for weight management. 
        Objective: To evaluate the effects of MCT supplementation on body composition in overweight adults.
        Methods: We conducted a randomized controlled trial with 120 overweight participants (BMI 25-30 kg/mÂ²) aged 25-55 years. 
        Participants received either 30ml MCT oil daily (n=60) or placebo (n=60) for 12 weeks. 
        Results: MCT group showed significant reductions in body fat mass (-2.3Â±0.8 kg vs -0.8Â±0.5 kg, p<0.001). 
        Mechanism: MCTs increase thermogenesis and promote fat oxidation through enhanced ketone production.
        Conclusions: Daily MCT supplementation effectively reduces body fat in overweight adults.
        """,
        
        # ç¤ºä¾‹æ‘˜è¦2 - Metaåˆ†æ
        """
        Background: The efficacy of medium-chain triglycerides (MCT) for weight loss remains controversial.
        Objective: To systematically review and meta-analyze RCTs examining MCT effects on weight loss in adults.
        Methods: We searched databases for randomized controlled trials comparing MCT vs control interventions. 
        Eight studies involving 512 participants (age 18-65 years, various BMI ranges) were included.
        Dosage: MCT interventions ranged from 15-45ml daily for 4-24 weeks.
        Results: MCT supplementation significantly reduced body weight (-1.8 kg, 95% CI: -2.5 to -1.1 kg).
        Mechanisms: MCTs increase energy expenditure, enhance satiety, and promote ketogenesis.
        Conclusions: Strong evidence supports MCT use for moderate weight loss in adults.
        """,
        
        # ç¤ºä¾‹æ‘˜è¦3 - æœºåˆ¶ç ”ç©¶
        """
        Background: Medium-chain triglycerides (MCT) may influence metabolic pathways differently than long-chain fatty acids.
        Objective: To investigate the metabolic mechanisms of MCT in human adipocytes.
        Methods: 80 healthy volunteers (40 men, 40 women, age 20-40 years) participated in this study. 
        Subjects consumed 20g MCT daily for 8 weeks. 
        Results: MCT increased resting metabolic rate by 8% and reduced appetite scores significantly.
        Mechanism: MCTs are rapidly oxidized, generating more ATP per gram than long-chain fatty acids, 
        and stimulate uncoupling proteins in brown adipose tissue.
        Conclusions: MCT enhances metabolic rate through enhanced thermogenesis and fat oxidation.
        """
    ]
    
    print("=" * 60)
    print("æµ‹è¯•æ··åˆä¿¡æ¯æå–åŠŸèƒ½")
    print("=" * 60)
    
    for i, abstract in enumerate(test_abstracts, 1):
        print(f"\næµ‹è¯•æ‘˜è¦ {i}:")
        print("-" * 40)
        
        # ä½¿ç”¨æ··åˆæ–¹æ³•æå–ä¿¡æ¯ï¼ˆä¸»è¦ï¼šæ­£åˆ™è¡¨è¾¾å¼ï¼Œå¤‡ç”¨ï¼šAIï¼‰
        print("å¼€å§‹æå–ç ”ç©¶ä¿¡æ¯...")
        
        # é¦–å…ˆå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–
        regex_extracted = extract_info_with_regex(abstract)
        print(f"æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœï¼š{regex_extracted}")
        
        # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœçš„è´¨é‡
        regex_quality = sum(1 for v in regex_extracted.values() if v != "æœªæ˜ç¡®è¯´æ˜")
        print(f"æ­£åˆ™è¡¨è¾¾å¼æå–è´¨é‡ï¼š{regex_quality}/4 å­—æ®µæœ‰æœ‰æ•ˆä¿¡æ¯")
        
        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼ç»“æœè´¨é‡è¾ƒä½ï¼Œå°è¯•AIæå–
        if regex_quality < 2:  # å¦‚æœå°‘äº2ä¸ªå­—æ®µæœ‰æœ‰æ•ˆä¿¡æ¯
            print("æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœè´¨é‡è¾ƒä½ï¼Œå°è¯•AIæå–...")
            ai_extracted = extract_info_with_ai(abstract)
            print(f"AIæå–ç»“æœï¼š{ai_extracted}")
            
            # åˆå¹¶ç»“æœï¼šä¼˜å…ˆä½¿ç”¨AIç»“æœï¼Œç¼ºå¤±æ—¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç»“æœ
            final_extracted = {}
            for key in ["ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•", "ä½œç”¨æœºç†"]:
                ai_value = ai_extracted.get(key, "")
                regex_value = regex_extracted.get(key, "")
                
                if ai_value and ai_value not in ["N/A", "éœ€äººå·¥ç¡®è®¤", ""]:
                    final_extracted[key] = ai_value
                elif regex_value and regex_value != "æœªæ˜ç¡®è¯´æ˜":
                    final_extracted[key] = regex_value
                else:
                    final_extracted[key] = "éœ€äººå·¥ç¡®è®¤"
        else:
            # æ­£åˆ™è¡¨è¾¾å¼ç»“æœè´¨é‡è¾ƒå¥½ï¼Œç›´æ¥ä½¿ç”¨
            final_extracted = regex_extracted
            print("æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœè´¨é‡è‰¯å¥½ï¼Œç›´æ¥ä½¿ç”¨")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("æœ€ç»ˆæå–ç»“æœ:")
        for key, value in final_extracted.items():
            print(f"  {key}: {value}")
        
        print(f"\næ‘˜è¦é•¿åº¦: {len(abstract)} å­—ç¬¦")
        time.sleep(2)  # æµ‹è¯•é—´éš”
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)

# ================= ä¸»ç¨‹åº =================
def get_user_search_term():
    """
    è·å–ç”¨æˆ·è¾“å…¥çš„æœç´¢è¯
    """
    global MAX_RESULTS, ENABLE_FULLTEXT_EXTRACTION
    
    print("\n" + "="*60)
    print("PubMedæ–‡çŒ®æœç´¢å·¥å…·")
    print("="*60)
    
    print("\nå½“å‰é…ç½®:")
    print(f"é»˜è®¤æœç´¢è¯: {SEARCH_TERM.strip()[:100]}...")
    print(f"æœ€å¤§ç»“æœæ•°: {MAX_RESULTS}")
    print(f"å…¨æ–‡æå–åŠŸèƒ½: {'å¼€å¯' if ENABLE_FULLTEXT_EXTRACTION else 'å…³é—­'}")
    print(f"é‚®ç®±: {Entrez.email}")
    
    print("\nè¯·é€‰æ‹©æ“ä½œ:")
    print("1. ä½¿ç”¨é»˜è®¤æœç´¢è¯å¼€å§‹æœç´¢")
    print("2. è¾“å…¥æ–°çš„æœç´¢è¯")
    print("3. æŸ¥çœ‹è¯¦ç»†æœç´¢è¯")
    print("4. è®¾ç½®æœ€å¤§ç»“æœæ•°é‡")
    print("5. å¯ç”¨/ç¦ç”¨å…¨æ–‡æå–åŠŸèƒ½")
    print("6. é€€å‡º")
    
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-6): ").strip()
        
        if choice == "1":
            return SEARCH_TERM
        elif choice == "2":
            print("\nè¯·è¾“å…¥æœç´¢è¯ (æ”¯æŒPubMedè¯­æ³•):")
            print("ç¤ºä¾‹: (diabetes OR diabetes mellitus) AND (metformin OR insulin)")
            print("æˆ–è€…ç›´æ¥æŒ‰Enterä½¿ç”¨é»˜è®¤æœç´¢è¯")
            
            custom_search = input("æœç´¢è¯: ").strip()
            if custom_search:
                return custom_search
            else:
                return SEARCH_TERM
        elif choice == "3":
            print(f"\nå½“å‰é»˜è®¤æœç´¢è¯:")
            print("-"*40)
            print(SEARCH_TERM)
            print("-"*40)
            continue
        elif choice == "4":
            print(f"\nå½“å‰æœ€å¤§ç»“æœæ•°é‡: {MAX_RESULTS}")
            print("å»ºè®®å€¼ï¼š20-500ï¼ˆæ•°å­—è¶Šå¤§æœç´¢æ—¶é—´è¶Šé•¿ï¼‰")
            
            try:
                new_max = input("è¯·è¾“å…¥æ–°çš„æœ€å¤§ç»“æœæ•°é‡ (ç›´æ¥æŒ‰Enterä¿æŒå½“å‰å€¼): ").strip()
                if new_max:
                    new_max_num = int(new_max)
                    if new_max_num > 0:
                        MAX_RESULTS = new_max_num
                        print(f"âœ… æœ€å¤§ç»“æœæ•°é‡å·²æ›´æ–°ä¸º: {MAX_RESULTS}")
                    else:
                        print("âŒ è¯·è¾“å…¥å¤§äº0çš„æ•°å­—")
                else:
                    print(f"âœ… ä¿æŒå½“å‰å€¼: {MAX_RESULTS}")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            continue
        elif choice == "5":
            ENABLE_FULLTEXT_EXTRACTION = not ENABLE_FULLTEXT_EXTRACTION
            status = "å·²å¯ç”¨" if ENABLE_FULLTEXT_EXTRACTION else "å·²ç¦ç”¨"
            print(f"\nå…¨æ–‡æå–åŠŸèƒ½: {status}")
            print(f"å½“å‰çŠ¶æ€: {'å¼€å¯' if ENABLE_FULLTEXT_EXTRACTION else 'å…³é—­'}")
            continue
        elif choice == "6":
            print("ç¨‹åºé€€å‡º")
            exit(0)
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-6")

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦‚æœæ˜¯ "test" åˆ™è¿è¡Œæµ‹è¯•
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_ai_extraction()
    else:
        # è·å–æœç´¢è¯
        search_term = get_user_search_term()
        
        print(f"\nå¼€å§‹æœç´¢: {search_term[:100]}...")
        
        # 1. æœç´¢
        ids = search_pubmed(search_term, MAX_RESULTS)
        
        if ids:
            print(f"æ‰¾åˆ° {len(ids)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¼€å§‹è·å–è¯¦ç»†ä¿¡æ¯...")
            
            # 2. è·å–è¯¦æƒ…
            articles = fetch_details(ids)
            
            # 3. è§£ææ•°æ®
            results = []
            for i, article in enumerate(articles):
                print(f"æ­£åœ¨å¤„ç†æ–‡çŒ® {i+1}/{len(articles)}...")
                results.append(parse_record(article))
            
            # 4. ç”Ÿæˆè¡¨æ ¼
            df = pd.DataFrame(results)
            
            # è°ƒæ•´åˆ—é¡ºåºä»¥ç¬¦åˆæ–‡ä»¶è¦æ±‚
            columns_order = [
                'å‘è¡¨å¹´ä»½', 'æ•°æ®æ”¶é›†å¹´ä»½', 'å›½å®¶', 'ç ”ç©¶ç±»å‹', 
                'ç ”ç©¶å¯¹è±¡', 'æ ·æœ¬é‡', 'æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•', 
                'ä½œç”¨æœºç†', 'æ‘˜è¦ä¸»è¦å†…å®¹', 'è¯æ®ç­‰çº§', 'ç»“è®ºæ‘˜è¦', 'æ ‡é¢˜', 'PMID',
                # å…¨æ–‡ç›¸å…³å­—æ®µ
                'å…è´¹å…¨æ–‡çŠ¶æ€', 'å…è´¹å…¨æ–‡é“¾æ¥æ•°', 'å…¨æ–‡æå–çŠ¶æ€', 'å…¨æ–‡å†…å®¹æ‘˜è¦'
            ]
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            for col in columns_order:
                if col not in df.columns:
                    if col == 'æ‘˜è¦ä¸»è¦å†…å®¹':
                        df[col] = "éœ€äººå·¥ç¡®è®¤"
                    elif col in ['å…è´¹å…¨æ–‡çŠ¶æ€', 'å…è´¹å…¨æ–‡é“¾æ¥æ•°', 'å…¨æ–‡æå–çŠ¶æ€']:
                        df[col] = False if col in ['å…è´¹å…¨æ–‡çŠ¶æ€', 'å…¨æ–‡æå–çŠ¶æ€'] else 0
                    elif col == 'å…¨æ–‡å†…å®¹æ‘˜è¦':
                        df[col] = "æœªå¯ç”¨å…¨æ–‡æå–"
                    else:
                        df[col] = ""
                    
            df = df[columns_order]
            
            # å¯¼å‡º
            filename = f"Literature_Search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            df.to_excel(filename, index=False)
            print(f"\nâœ… æˆåŠŸå¯¼å‡ºè¡¨æ ¼ï¼š{filename}")
            print(f"ğŸ“Š åŒ…å« {len(df)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯")
        else:
            print("\nâŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥æœç´¢è¯æ˜¯å¦æ­£ç¡®")