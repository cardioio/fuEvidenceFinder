import pandas as pd
import re
from Bio import Entrez
from datetime import datetime
import requests
import json
import time
import logging
from typing import Dict, Optional
from bs4 import BeautifulSoup

# ================= é…ç½®åŒºåŸŸ =================
# è¯·æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„é‚®ç®±ï¼Œè¿™æ˜¯PubMed APIçš„è¦æ±‚ï¼Œç”¨äºè¿½è¸ªé«˜é¢‘è®¿é—®
Entrez.email = "varian69@gmail.com" 

# æ£€ç´¢å…³é”®è¯ (ä»¥MCTä¸ºä¾‹ï¼Œå¤ç”¨ä¹‹å‰çš„é€»è¾‘)
SEARCH_TERM = """
("Medium-chain triglycerides" OR "MCT" OR "Caprylic acid") AND ("Weight loss" OR "Body composition" OR "Fat mass") AND ("Adults"[Mesh] OR "Adult") AND ("Obesity"[Mesh] OR "Overweight"[Mesh] OR "Obesity" OR "Overweight") NOT ("Diabetes Mellitus"[Mesh] OR "Diabetes" OR "Hypertension"[Mesh] OR "High blood pressure" OR "Cardiovascular Diseases"[Mesh] OR "Metabolic Syndrome"[Mesh] OR "Neoplasms"[Mesh] OR "Cancer" OR "Pregnancy" OR "Pregnant" OR "Child" OR "Adolescent")
"""

# æƒ³è¦è·å–çš„æ–‡çŒ®æ•°é‡
MAX_RESULTS = 100 

# ================= AI APIé…ç½® =================
# å¤šç§APIç«¯ç‚¹å°è¯•
API_ENDPOINTS = [
    "https://api.gptgod.online/v1/chat/completions",
    "https://api.minimax.chat/v1/text/chatcompletion_v2",
    "https://api.deepseek.com/v1/chat/completions"
]

# APIå¯†é’¥æ± é…ç½® - å¤šä¸ªå¯†é’¥ç”¨äºæé«˜è¯·æ±‚æˆåŠŸç‡
API_KEYS_POOL = [
    "sk-1wLZqqkXDT9shZzgTqNRc0wNB6K4Kmu1t0kov0KA5I3auqVf",  # ä¸»å¯†é’¥
    "sk-19GhS2EHMvZJZrm4LYdL94KrAfIb5ckAhwH7Btcorg23zh8H",  # å¤‡ç”¨å¯†é’¥1
    "sk-t0WZJnqINXX2LnRvPIvRvhMLIcfYtZ76UvOjHf82IGPcYRj1",  # å¤‡ç”¨å¯†é’¥2
]

# å‘åå…¼å®¹ - ä¿ç•™åŸæœ‰å•å¯†é’¥é…ç½®
API_KEY = API_KEYS_POOL[0]

# APIå¯†é’¥æ± ç®¡ç†é…ç½®
API_KEY_POOL_CONFIG = {
    "max_failure_count": 3,        # æœ€å¤§å¤±è´¥æ¬¡æ•°ï¼Œè¶…è¿‡åæš‚æ—¶ç¦ç”¨å¯†é’¥
    "disable_duration": 300,       # å¯†é’¥ç¦ç”¨æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ5åˆ†é’Ÿ
    "success_reset_threshold": 2,  # æˆåŠŸæ¬¡æ•°é˜ˆå€¼ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
    "enable_key_rotation": True,   # å¯ç”¨å¯†é’¥è½®æ¢
    "log_key_usage": True          # æ˜¯å¦è®°å½•å¯†é’¥ä½¿ç”¨æƒ…å†µï¼ˆä¸è®°å½•å…·ä½“å¯†é’¥å†…å®¹ï¼‰
}

# å›½å®¶è¯†åˆ«ç¼“å­˜é…ç½®
COUNTRY_CACHE = {}  # ç®€å•å†…å­˜ç¼“å­˜
COUNTRY_CACHE_MAX_SIZE = 1000
COUNTRY_CACHE_TTL = 3600  # 1å°æ—¶è¿‡æœŸ

ENABLE_WEB_SEARCH = True  # æ˜¯å¦å¯ç”¨web searchåŠŸèƒ½
REQUEST_DELAY = 2.0  # APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…429é”™è¯¯

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ================= APIå¯†é’¥æ± ç®¡ç†å™¨ =================
class APIKeyPoolManager:
    """
    APIå¯†é’¥æ± ç®¡ç†å™¨ - æä¾›å¯†é’¥çš„åŠ¨æ€ç®¡ç†ã€è‡ªåŠ¨è½®æ¢å’ŒçŠ¶æ€ç›‘æ§åŠŸèƒ½
    """
    
    def __init__(self, api_keys: list, config: dict):
        """
        åˆå§‹åŒ–APIå¯†é’¥æ± ç®¡ç†å™¨
        
        Args:
            api_keys: APIå¯†é’¥åˆ—è¡¨
            config: é…ç½®å­—å…¸
        """
        self.api_keys = api_keys
        self.config = config
        self.current_key_index = 0
        self.key_states = {}
        
        # åˆå§‹åŒ–æ¯ä¸ªå¯†é’¥çš„çŠ¶æ€
        for i, key in enumerate(api_keys):
            key_id = f"key_{i+1}"  # ä½¿ç”¨key_1, key_2ç­‰ä½œä¸ºå¯†é’¥æ ‡è¯†ç¬¦
            self.key_states[key_id] = {
                "key": key,
                "failure_count": 0,
                "success_count": 0,
                "is_disabled": False,
                "disabled_until": None,
                "last_used": None,
                "total_requests": 0,
                "total_successes": 0
            }
    
    def get_available_key(self) -> Optional[str]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„APIå¯†é’¥
        
        Returns:
            å¯ç”¨çš„APIå¯†é’¥ï¼Œå¦‚æœæ‰€æœ‰å¯†é’¥éƒ½ä¸å¯ç”¨åˆ™è¿”å›None
        """
        if not self.config.get("enable_key_rotation", True):
            return self.api_keys[0] if self.api_keys else None
            
        attempts = 0
        max_attempts = len(self.api_keys)
        
        while attempts < max_attempts:
            key_id = f"key_{self.current_key_index + 1}"
            state = self.key_states[key_id]
            
            # æ£€æŸ¥å¯†é’¥æ˜¯å¦è¢«ç¦ç”¨
            if self._is_key_disabled(state):
                # å°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥
                self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
                attempts += 1
                continue
                
            # å¯†é’¥å¯ç”¨
            return state["key"]
        
        # æ‰€æœ‰å¯†é’¥éƒ½ä¸å¯ç”¨
        logger.error("æ‰€æœ‰APIå¯†é’¥éƒ½ä¸å¯ç”¨")
        return None
    
    def _is_key_disabled(self, key_state: dict) -> bool:
        """
        æ£€æŸ¥å¯†é’¥æ˜¯å¦è¢«ç¦ç”¨
        
        Args:
            key_state: å¯†é’¥çŠ¶æ€å­—å…¸
            
        Returns:
            å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºå¯†é’¥æ˜¯å¦è¢«ç¦ç”¨
        """
        if not key_state["is_disabled"]:
            return False
            
        # æ£€æŸ¥ç¦ç”¨æ—¶é—´æ˜¯å¦å·²è¿‡
        if key_state["disabled_until"] and time.time() > key_state["disabled_until"]:
            # é‡æ–°å¯ç”¨å¯†é’¥
            key_state["is_disabled"] = False
            key_state["disabled_until"] = None
            logger.info(f"å¯†é’¥é‡æ–°å¯ç”¨")
            return False
            
        return True
    
    def report_success(self, key: str):
        """
        æŠ¥å‘ŠAPIè¯·æ±‚æˆåŠŸ
        
        Args:
            key: ä½¿ç”¨çš„APIå¯†é’¥
        """
        key_id = self._get_key_id(key)
        if key_id and key_id in self.key_states:
            state = self.key_states[key_id]
            state["success_count"] += 1
            state["total_successes"] += 1
            state["last_used"] = time.time()
            
            # å¦‚æœæœ‰å¤±è´¥è®°å½•ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
            if state["failure_count"] > 0:
                state["failure_count"] = max(0, state["failure_count"] - 1)
            
            # è®°å½•å¯†é’¥ä½¿ç”¨æƒ…å†µ
            if self.config.get("log_key_usage", True):
                logger.debug(f"å¯†é’¥ {key_id} è¯·æ±‚æˆåŠŸï¼Œç´¯è®¡æˆåŠŸ: {state['total_successes']}")
    
    def report_failure(self, key: str, error_type: str = "unknown"):
        """
        æŠ¥å‘ŠAPIè¯·æ±‚å¤±è´¥
        
        Args:
            key: ä½¿ç”¨çš„APIå¯†é’¥
            error_type: é”™è¯¯ç±»å‹
        """
        key_id = self._get_key_id(key)
        if key_id and key_id in self.key_states:
            state = self.key_states[key_id]
            state["failure_count"] += 1
            state["total_requests"] += 1
            state["last_used"] = time.time()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¦ç”¨å¯†é’¥
            max_failures = self.config.get("max_failure_count", 3)
            if state["failure_count"] >= max_failures:
                self._disable_key(key_id, error_type)
            
            # è®°å½•å¯†é’¥ä½¿ç”¨æƒ…å†µ
            if self.config.get("log_key_usage", True):
                logger.warning(f"å¯†é’¥ {key_id} è¯·æ±‚å¤±è´¥ ({error_type})ï¼Œå¤±è´¥æ¬¡æ•°: {state['failure_count']}")
    
    def _disable_key(self, key_id: str, reason: str):
        """
        ç¦ç”¨å¯†é’¥
        
        Args:
            key_id: å¯†é’¥æ ‡è¯†ç¬¦
            reason: ç¦ç”¨åŸå› 
        """
        disable_duration = self.config.get("disable_duration", 300)
        state = self.key_states[key_id]
        
        state["is_disabled"] = True
        state["disabled_until"] = time.time() + disable_duration
        
        logger.warning(f"å¯†é’¥ {key_id} å› å¤±è´¥æ¬¡æ•°è¿‡å¤šè¢«ä¸´æ—¶ç¦ç”¨ï¼ŒåŸå› : {reason}ï¼Œç¦ç”¨æ—¶é•¿: {disable_duration}ç§’")
    
    def _get_key_id(self, key: str) -> Optional[str]:
        """
        æ ¹æ®å¯†é’¥è·å–å¯†é’¥æ ‡è¯†ç¬¦
        
        Args:
            key: APIå¯†é’¥
            
        Returns:
            å¯†é’¥æ ‡è¯†ç¬¦ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›None
        """
        for key_id, state in self.key_states.items():
            if state["key"] == key:
                return key_id
        return None
    
    def get_key_statistics(self) -> dict:
        """
        è·å–æ‰€æœ‰å¯†é’¥çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        stats = {}
        for key_id, state in self.key_states.items():
            stats[key_id] = {
                "is_disabled": state["is_disabled"],
                "failure_count": state["failure_count"],
                "success_count": state["success_count"],
                "total_requests": state["total_requests"],
                "total_successes": state["total_successes"],
                "success_rate": state["total_successes"] / max(1, state["total_requests"]),
                "last_used": state["last_used"]
            }
        return stats
    
    def rotate_key(self):
        """
        è½®æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥
        """
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        logger.debug(f"å¯†é’¥è½®æ¢åˆ°ç´¢å¼•: {self.current_key_index}")

# åˆ›å»ºå…¨å±€APIå¯†é’¥æ± ç®¡ç†å™¨å®ä¾‹
api_key_pool = APIKeyPoolManager(API_KEYS_POOL, API_KEY_POOL_CONFIG)

# ================= ä¸»ç¨‹åºé…ç½® =================
# å…¨å±€é…ç½®
ENABLE_FULLTEXT_EXTRACTION = False  # æ˜¯å¦å¯ç”¨å…¨æ–‡æå–åŠŸèƒ½
# ===========================================

def search_pubmed(query, max_results=20):
    """åœ¨PubMedä¸­æœç´¢å¹¶è¿”å›IDåˆ—è¡¨"""
    print(f"æ­£åœ¨æœç´¢: {query.strip()}...")
    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results, sort="relevance")
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")
        return []

def fetch_details(id_list):
    """æ ¹æ®IDè·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
    print(f"æ­£åœ¨è·å– {len(id_list)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯...")
    ids = ",".join(id_list)
    try:
        handle = Entrez.efetch(db="pubmed", id=ids, retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        return records['PubmedArticle']
    except Exception as e:
        print(f"è·å–è¯¦æƒ…å¤±è´¥: {e}")
        return []

def extract_sample_size(abstract_text):
    """
    å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ‘˜è¦ä¸­æå–æ ·æœ¬é‡ (n=xxx)
    è¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„å¯å‘å¼ç®—æ³•ï¼Œä¸ä¸€å®š100%å‡†ç¡®
    """
    if not abstract_text:
        return "N/A"
    
    # åŒ¹é…å¸¸è§çš„æ ·æœ¬é‡è¡¨è¾¾ï¼Œå¦‚ n=100, 100 participants, 100 subjects
    patterns = [
        r"n\s*=\s*(\d+)",
        r"(\d+)\s*participants",
        r"(\d+)\s*subjects",
        r"(\d+)\s*patients"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            return match.group(1)
    return "éœ€äººå·¥ç¡®è®¤"

def parse_record(article):
    """è§£æå•ç¯‡æ–‡çŒ®ï¼Œæ˜ å°„åˆ°ç›®æ ‡è¡¨æ ¼åˆ—"""
    data = {}
    medline = article['MedlineCitation']
    article_data = medline['Article']
    
    # 1. å‘è¡¨å¹´ä»½
    try:
        pub_date = article_data['Journal']['JournalIssue']['PubDate']
        year = pub_date.get('Year', '')
        if not year and 'MedlineDate' in pub_date:
            year = pub_date['MedlineDate'].split(' ')[0]
        data['å‘è¡¨å¹´ä»½'] = year
    except:
        data['å‘è¡¨å¹´ä»½'] = "N/A"

    # 2. æ•°æ®æ”¶é›†å¹´ä»½ (é€šè¿‡AIæå–) - ç¨åä»AIæå–ç»“æœè·å–ï¼Œå…ˆè®¾ä¸ºé»˜è®¤å€¼
    data['æ•°æ®æ”¶é›†å¹´ä»½'] = "éœ€AIæå–"

    # 3. å›½å®¶ (å°è¯•ä»ä½œè€…æœºæ„æå–ï¼Œé€šå¸¸å–ç¬¬ä¸€ä½œè€…)
    data['å›½å®¶'] = extract_country_from_affiliation(article_data)

    # 4. ç ”ç©¶ç±»å‹ (ä»PublicationTypeListæå–)
    try:
        pt_list = [pt.title() for pt in article_data.get('PublicationTypeList', [])]
        if "Meta-Analysis" in pt_list:
            r_type = "Meta-Analysis"
        elif "Randomized Controlled Trial" in pt_list:
            r_type = "RCT"
        elif "Review" in pt_list:
            r_type = "Review"
        else:
            r_type = ", ".join(pt_list)
        data['ç ”ç©¶ç±»å‹'] = r_type
    except:
        data['ç ”ç©¶ç±»å‹'] = "N/A"

    # 5. ç ”ç©¶å¯¹è±¡ & 6. æ ·æœ¬é‡
    abstract_text = ""
    if 'Abstract' in article_data and 'AbstractText' in article_data['Abstract']:
        # AbstractText æœ‰æ—¶æ˜¯åˆ—è¡¨ï¼ˆåˆ†æ®µæ‘˜è¦ï¼‰ï¼Œæœ‰æ—¶æ˜¯å­—ç¬¦ä¸²
        abs_content = article_data['Abstract']['AbstractText']
        if isinstance(abs_content, list):
            abstract_text = " ".join([str(item) for item in abs_content])
        else:
            abstract_text = str(abs_content)
    
    # ä½¿ç”¨AIç»Ÿä¸€æå–ä¿¡æ¯ï¼ˆä¸å†ä¾èµ–æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    logger.info("å¼€å§‹ä½¿ç”¨AIæå–ç ”ç©¶ä¿¡æ¯...")
    
    # ç›´æ¥ä½¿ç”¨AIæå–æ‰€æœ‰ä¿¡æ¯
    print("  ğŸ“¤ æ­£åœ¨å°†æ‘˜è¦/åŸæ–‡htmlå‘ç»™AIè¯¢é—®ä¸­...")
    ai_extracted = extract_info_with_ai(abstract_text)
    print("  ğŸ“¥ AIæ•°æ®å·²è¿”å›")
    logger.info(f"AIæå–ç»“æœï¼š{ai_extracted}")
    
    # æ›´æ–°æ•°æ®å­—æ®µ
    data['ç ”ç©¶å¯¹è±¡'] = ai_extracted.get('ç ”ç©¶å¯¹è±¡', "éœ€äººå·¥ç¡®è®¤")
    data['æ ·æœ¬é‡'] = ai_extracted.get('æ ·æœ¬é‡', "éœ€äººå·¥ç¡®è®¤")
    data['æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•'] = ai_extracted.get('æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•', "éœ€äººå·¥ç¡®è®¤")
    data['ä½œç”¨æœºç†'] = ai_extracted.get('ä½œç”¨æœºç†', "éœ€äººå·¥ç¡®è®¤")
    data['æ‘˜è¦ä¸»è¦å†…å®¹'] = ai_extracted.get('æ‘˜è¦ä¸»è¦å†…å®¹', "éœ€äººå·¥ç¡®è®¤")
    data['ç»“è®ºæ‘˜è¦'] = ai_extracted.get('ç»“è®ºæ‘˜è¦', "éœ€äººå·¥ç¡®è®¤")  # ä»AIæå–ç»“æœä¸­è·å–ä¸­æ–‡ç»“è®ºæ‘˜è¦
    data['æ•°æ®æ”¶é›†å¹´ä»½'] = ai_extracted.get('æ•°æ®æ”¶é›†å¹´ä»½', "éœ€äººå·¥ç¡®è®¤")  # ä»AIæå–ç»“æœä¸­è·å–æ•°æ®æ”¶é›†å¹´ä»½
    
    # 9. è¯æ®ç­‰çº§ (åŸºäºç ”ç©¶ç±»å‹é¢„åˆ¤)
    if "Meta-Analysis" in data['ç ”ç©¶ç±»å‹']:
        data['è¯æ®ç­‰çº§'] = "Level 1"
    elif "RCT" in data['ç ”ç©¶ç±»å‹']:
        data['è¯æ®ç­‰çº§'] = "Level 2"
    else:
        data['è¯æ®ç­‰çº§'] = "å¾…å®š"

    # é¢å¤–ä¿¡æ¯æ–¹ä¾¿æ ¸å¯¹
    data['æ ‡é¢˜'] = article_data.get('ArticleTitle', '')
    data['PMID'] = medline.get('PMID', '')

    # å¦‚æœå¯ç”¨å…¨æ–‡æå–åŠŸèƒ½ï¼Œè·å–PMIDå¹¶è¿›è¡Œå…¨æ–‡åˆ†æ
    if ENABLE_FULLTEXT_EXTRACTION and data['PMID']:
        try:
            print(f"  ğŸ” æ­£åœ¨æ£€æŸ¥PMID {data['PMID']} çš„å…¨æ–‡å¯ç”¨æ€§...")
            
            # ä½¿ç”¨å…¨æ–‡åˆ†æåŠŸèƒ½
            fulltext_analysis = analyze_pmid_with_full_text(data['PMID'])
            
            # å°†å…¨æ–‡åˆ†æç»“æœæ·»åŠ åˆ°æ•°æ®ä¸­
            data['å…è´¹å…¨æ–‡çŠ¶æ€'] = fulltext_analysis.get('is_free', False)
            data['å…è´¹å…¨æ–‡é“¾æ¥æ•°'] = len(fulltext_analysis.get('links', []))
            data['å…¨æ–‡æå–çŠ¶æ€'] = fulltext_analysis.get('extraction_success', False)
            data['å…¨æ–‡å†…å®¹æ‘˜è¦'] = fulltext_analysis.get('extracted_content', {}).get('abstract', 'æœªæå–')
            
            if fulltext_analysis.get('is_free'):
                print(f"  âœ… å‘ç°å…è´¹å…¨æ–‡: {data['å…è´¹å…¨æ–‡é“¾æ¥æ•°']} ä¸ªé“¾æ¥")
            else:
                print(f"  âŒ æ— å…è´¹å…¨æ–‡")
                
        except Exception as e:
            logger.error(f"å¤„ç†PMID {data['PMID']} å…¨æ–‡åˆ†ææ—¶å‡ºé”™: {e}")
            data['å…è´¹å…¨æ–‡çŠ¶æ€'] = False
            data['å…è´¹å…¨æ–‡é“¾æ¥æ•°'] = 0
            data['å…¨æ–‡æå–çŠ¶æ€'] = False
            data['å…¨æ–‡å†…å®¹æ‘˜è¦'] = "åˆ†æå¤±è´¥"

    return data

def extract_country_from_affiliation(article_data: Dict) -> str:
    """
    ä»ä½œè€…æœºæ„ä¿¡æ¯ä¸­æå–å›½å®¶åç§° - åŸºäºGPT AIçš„ç®€åŒ–å®ç°
    
    Args:
        article_data: ä»PubMedè·å–çš„æ–‡ç« æ•°æ®
        
    Returns:
        å›½å®¶åç§°å­—ç¬¦ä¸²
    """
    try:
        # å°è¯•ä»ç¬¬ä¸€ä½œè€…æå–æœºæ„ä¿¡æ¯
        if 'AuthorList' not in article_data or not article_data['AuthorList']:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä½œè€…ä¿¡æ¯ï¼Œè¿”å›éœ€äººå·¥ç¡®è®¤")
            return "éœ€äººå·¥ç¡®è®¤"
            
        first_author = article_data['AuthorList'][0]
        
        # è·å–æœºæ„ä¿¡æ¯
        affiliation = ""
        if 'AffiliationInfo' in first_author and first_author['AffiliationInfo']:
            affiliation = first_author['AffiliationInfo'][0].get('Affiliation', '')
        elif 'Affiliation' in first_author:
            affiliation = first_author['Affiliation']
        
        if not affiliation:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœºæ„ä¿¡æ¯ï¼Œè¿”å›éœ€äººå·¥ç¡®è®¤")
            return "éœ€äººå·¥ç¡®è®¤"
        
        # æ¸…ç†æœºæ„ä¿¡æ¯ç”¨äºç¼“å­˜é”®
        clean_affiliation = affiliation.replace('\n', ' ').replace('\r', ' ').strip()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{hash(clean_affiliation)}_{len(clean_affiliation)}"
        if cache_key in COUNTRY_CACHE:
            cached_result, cache_time = COUNTRY_CACHE[cache_key]
            if time.time() - cache_time < COUNTRY_CACHE_TTL:
                logger.debug(f"ä»ç¼“å­˜è·å–å›½å®¶ä¿¡æ¯: {cached_result}")
                return cached_result
        
        # ä½¿ç”¨AIè¿›è¡Œå›½å®¶è¯†åˆ«
        ai_result = _extract_country_with_ai(clean_affiliation)
        
        if ai_result and ai_result != "éœ€äººå·¥ç¡®è®¤":
            # æ›´æ–°ç¼“å­˜
            _update_country_cache(cache_key, ai_result)
            return ai_result
        
        # å›é€€åˆ°ç®€å•çš„å…³é”®è¯åŒ¹é…
        logger.info("AIè¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨å›é€€æœºåˆ¶")
        return _fallback_country_extraction(clean_affiliation)
        
    except Exception as e:
        logger.error(f"æå–å›½å®¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return "éœ€äººå·¥ç¡®è®¤"

def _extract_country_with_ai(affiliation: str) -> str:
    """
    ä½¿ç”¨GPT AIä»æœºæ„ä¿¡æ¯ä¸­æå–å›½å®¶åç§°
    
    Args:
        affiliation: æœºæ„ä¿¡æ¯å­—ç¬¦ä¸²
        
    Returns:
        å›½å®¶åç§°å­—ç¬¦ä¸²
    """
    prompt = f"""è¯·ä»ä»¥ä¸‹ä½œè€…æœºæ„ä¿¡æ¯ä¸­æå–å›½å®¶åç§°ã€‚è¯·åªè¿”å›å›½å®¶åç§°ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›"éœ€äººå·¥ç¡®è®¤"ã€‚

æœºæ„ä¿¡æ¯ï¼š
{affiliation}

è¦æ±‚ï¼š
1. åªè¿”å›å›½å®¶åç§°ï¼Œå¦‚"United States"ã€"China"ã€"Germany"ç­‰
2. å¦‚æœä¿¡æ¯ä¸è¶³æˆ–æ— æ³•ç¡®å®šï¼Œè¿”å›"éœ€äººå·¥ç¡®è®¤"
3. ä¸è¦åŒ…å«å…¶ä»–æ–‡å­—æˆ–è§£é‡Š
4. ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†å›½å®¶åç§°ï¼ˆå¦‚"United States"è€Œé"USA"ï¼‰
"""

    try:
        # ä½¿ç”¨ç®€åŒ–çš„AIè°ƒç”¨å‡½æ•°
        result = _call_ai_api(prompt, "country_extraction")
        if result:
            result = result.strip()
            # éªŒè¯è¿”å›ç»“æœ
            if result and result != "éœ€äººå·¥ç¡®è®¤":
                logger.info(f"AIè¯†åˆ«å›½å®¶æˆåŠŸ: {result}")
                return result
        return "éœ€äººå·¥ç¡®è®¤"
    except Exception as e:
        logger.error(f"AIå›½å®¶è¯†åˆ«å¤±è´¥: {e}")
        return "éœ€äººå·¥ç¡®è®¤"

def _fallback_country_extraction(affiliation: str) -> str:
    """
    å›é€€æœºåˆ¶ï¼šç®€å•çš„å…³é”®è¯åŒ¹é…æå–å›½å®¶
    
    Args:
        affiliation: æœºæ„ä¿¡æ¯å­—ç¬¦ä¸²
        
    Returns:
        å›½å®¶åç§°å­—ç¬¦ä¸²
    """
    # ç®€åŒ–çš„å›½å®¶å…³é”®è¯æ˜ å°„
    country_keywords = {
        "United States": ["USA", "US", "America", "United States", "American"],
        "China": ["China", "Chinese", "Beijing", "Shanghai", "Guangzhou"],
        "United Kingdom": ["UK", "Britain", "England", "Scotland", "Wales"],
        "Germany": ["Germany", "German", "Deutschland"],
        "Japan": ["Japan", "Japanese", "Tokyo", "Osaka"],
        "Australia": ["Australia", "Australian", "Sydney", "Melbourne"],
        "Canada": ["Canada", "Canadian"],
        "France": ["France", "French"],
        "Italy": ["Italy", "Italian"],
        "Spain": ["Spain", "Spanish"],
        "Netherlands": ["Netherlands", "Dutch"],
        "South Korea": ["Korea", "Korean", "Seoul"],
        "India": ["India", "Indian", "Mumbai", "Delhi"],
        "Singapore": ["Singapore", "Singaporean"],
        "Taiwan": ["Taiwan", "Taiwanese"],
        "Hong Kong": ["Hong Kong"],
        "Brazil": ["Brazil", "Brazilian"],
        "Mexico": ["Mexico", "Mexican"]
    }
    
    affiliation_upper = affiliation.upper()
    
    for country, keywords in country_keywords.items():
        for keyword in keywords:
            if keyword.upper() in affiliation_upper:
                logger.info(f"å›é€€æœºåˆ¶è¯†åˆ«å›½å®¶: {country} (åŒ¹é…å…³é”®è¯: {keyword})")
                return country
    
    logger.info("å›é€€æœºåˆ¶ä¹Ÿæœªèƒ½è¯†åˆ«å›½å®¶ï¼Œè¿”å›éœ€äººå·¥ç¡®è®¤")
    return "éœ€äººå·¥ç¡®è®¤"

def _call_ai_api(prompt: str, context: str) -> str:
    """
    è°ƒç”¨AI APIçš„ç®€åŒ–æ¥å£
    
    Args:
        prompt: æç¤ºè¯
        context: ä¸Šä¸‹æ–‡æ ‡è¯†
        
    Returns:
        AIè¿”å›çš„æ–‡æœ¬
    """
    try:
        # å®šä¹‰æ¨¡å‹é…ç½®
        model_configs = [
            ("gpt-3.5-turbo", API_ENDPOINTS[0]),  # GPTGod + gpt-3.5
            ("gpt-4", API_ENDPOINTS[0]),  # GPTGod + gpt-4
            ("deepseek-chat", API_ENDPOINTS[2])  # DeepSeek + deepseek-chat
        ]
        
        # ä½¿ç”¨ç°æœ‰çš„extract_info_with_aié€»è¾‘ï¼Œä½†åªè·å–ç®€å•æ–‡æœ¬ç»“æœ
        max_retries_per_config = 2
        max_total_retries = 6
        
        total_attempts = 0
        for model, endpoint in model_configs:
            if total_attempts >= max_total_retries:
                break
                
            for retry in range(max_retries_per_config):
                total_attempts += 1
                if total_attempts >= max_total_retries:
                    break
                    
                # è·å–å¯ç”¨å¯†é’¥
                api_key = api_key_pool.get_available_key()
                if not api_key:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥")
                    return ""
                
                try:
                    headers = {
                        'Content-Type': 'application/json',
                        'Authorization': f'Bearer {api_key}'
                    }
                    
                    payload = {
                        'model': model,
                        'messages': [
                            {
                                'role': 'user',
                                'content': prompt
                            }
                        ],
                        'max_tokens': 100,
                        'temperature': 0.1
                    }
                    
                    response = requests.post(endpoint, headers=headers, json=payload, timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if 'choices' in data and len(data['choices']) > 0:
                            content = data['choices'][0]['message']['content'].strip()
                            api_key_pool.report_success(api_key)
                            logger.debug(f"AI APIè°ƒç”¨æˆåŠŸ ({context})")
                            return content
                        else:
                            api_key_pool.report_failure(api_key, "invalid_response")
                    elif response.status_code == 429:
                        # é™æµé”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                        wait_time = (2 ** retry) + random.uniform(0, 1)
                        logger.warning(f"APIé™æµï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                        time.sleep(wait_time)
                        api_key_pool.report_failure(api_key, "rate_limit")
                        continue
                    elif response.status_code in [401, 403]:
                        # è®¤è¯é”™è¯¯ï¼Œåˆ‡æ¢å¯†é’¥
                        logger.warning(f"APIå¯†é’¥è®¤è¯å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥")
                        api_key_pool.report_failure(api_key, "auth_error")
                        break
                    else:
                        logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                        api_key_pool.report_failure(api_key, f"http_{response.status_code}")
                        
                except requests.exceptions.Timeout:
                    logger.error(f"APIè°ƒç”¨è¶…æ—¶")
                    api_key_pool.report_failure(api_key, "timeout")
                    continue
                except requests.exceptions.RequestException as e:
                    logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
                    api_key_pool.report_failure(api_key, "request_error")
                    continue
                except Exception as e:
                    logger.error(f"å¤„ç†APIå“åº”æ—¶å‡ºé”™: {e}")
                    api_key_pool.report_failure(api_key, "processing_error")
                    continue
        
        logger.error("æ‰€æœ‰AI APIè°ƒç”¨å°è¯•å‡å¤±è´¥")
        return ""
        
    except Exception as e:
        logger.error(f"è°ƒç”¨AI APIæ—¶å‡ºé”™: {e}")
        return ""

def _update_country_cache(key: str, country: str):
    """
    æ›´æ–°å›½å®¶è¯†åˆ«ç¼“å­˜
    
    Args:
        key: ç¼“å­˜é”®
        country: å›½å®¶åç§°
    """
    try:
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(COUNTRY_CACHE) >= COUNTRY_CACHE_MAX_SIZE:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•å®ç°ï¼šåˆ é™¤ç¬¬ä¸€ä¸ªï¼‰
            oldest_key = next(iter(COUNTRY_CACHE))
            del COUNTRY_CACHE[oldest_key]
            logger.debug("ç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§æ¡ç›®")
        
        COUNTRY_CACHE[key] = (country, time.time())
        logger.debug(f"æ›´æ–°å›½å®¶ç¼“å­˜: {key[:20]}... -> {country}")
    except Exception as e:
        logger.error(f"æ›´æ–°ç¼“å­˜æ—¶å‡ºé”™: {e}")



def extract_info_with_regex(abstract_text: str) -> Dict[str, str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ‘˜è¦ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
    """
    result = {
        "ç ”ç©¶å¯¹è±¡": "æœªæ˜ç¡®è¯´æ˜",
        "æ ·æœ¬é‡": "æœªæ˜ç¡®è¯´æ˜", 
        "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "æœªæ˜ç¡®è¯´æ˜",
        "ä½œç”¨æœºç†": "æœªæ˜ç¡®è¯´æ˜"
    }
    
    if not abstract_text:
        return result
    
    # æå–æ ·æœ¬é‡
    sample_patterns = [
        r"(\d+)\s*participants?",
        r"(\d+)\s*subjects?",
        r"(\d+)\s*patients?",
        r"n\s*=\s*(\d+)",
        r"sample\s+size\s+(?:of\s+)?(\d+)",
        r"involving\s+(\d+)\s+(?:participants|subjects|patients)",
        r"(\d+)\s+(?:healthy\s+)?volunteers?"
    ]
    
    for pattern in sample_patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            result["æ ·æœ¬é‡"] = match.group(1)
            break
    
    # æå–ç ”ç©¶å¯¹è±¡ç‰¹å¾
    subject_patterns = [
        r"(\d+)-?(\d+)?\s*years?\s+old",
        r"aged\s+(\d+)-?(\d+)?\s*years?",
        r"(overweight|obese|healthy)\s+(?:adults?|participants?)",
        r"(men|women|male|female)",
        r"(diabetes|hypertension|metabolic\s+syndrome)",
        r"(BMI\s+\d+-\d+)"
    ]
    
    subjects = []
    for pattern in subject_patterns:
        matches = re.findall(pattern, abstract_text, re.IGNORECASE)
        if matches:
            if isinstance(matches[0], tuple):
                subjects.extend([m for m in matches[0] if m])
            else:
                subjects.extend(matches)
    
    if subjects:
        result["ç ”ç©¶å¯¹è±¡"] = "ã€".join(subjects)
    
    # æå–å‰‚é‡ä¿¡æ¯
    dose_patterns = [
        r"(\d+)\s*ml\s+(?:MCT\s+)?oil\s+daily",
        r"(\d+)\s*g\s+(?:MCT\s+)?daily", 
        r"(\d+)\s*ml\s+per\s+day",
        r"(\d+)\s*g\s+per\s+day",
        r"(\d+)\s*ml\s+twice\s+daily",
        r"(\d+)\s*g\s+(\d+)\s*times\s+per\s+day"
    ]
    
    for pattern in dose_patterns:
        match = re.search(pattern, abstract_text, re.IGNORECASE)
        if match:
            if len(match.groups()) == 2:
                result["æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•"] = f"{match.group(1)}g {match.group(2)}æ¬¡/å¤©"
            else:
                result["æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•"] = match.group(0)
            break
    
    # æå–ä½œç”¨æœºç†
    mechanism_patterns = [
        r"(increase\s+thermogenesis)",
        r"(promote\s+fat\s+oxidation)",
        r"(enhance\s+ketone\s+production)",
        r"(increase\s+energy\s+expenditure)",
        r"(enhance\s+satiety)",
        r"(promote\s+ketogenesis)",
        r"(stimulate\s+uncoupling\s+proteins)",
        r"(rapid\s+oxidation)"
    ]
    
    mechanisms = []
    for pattern in mechanism_patterns:
        matches = re.findall(pattern, abstract_text, re.IGNORECASE)
        if matches:
            mechanisms.extend(matches)
    
    if mechanisms:
        result["ä½œç”¨æœºç†"] = "ï¼›".join(mechanisms)
    
    return result

def extract_info_with_ai(abstract_text: str) -> Dict[str, str]:
    """
    ä½¿ç”¨GPT-4.1 APIä»æ‘˜è¦ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
    
    Args:
        abstract_text: æ–‡çŒ®æ‘˜è¦æ–‡æœ¬
        
    Returns:
        åŒ…å«æå–ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - ç ”ç©¶å¯¹è±¡
        - æ ·æœ¬é‡  
        - æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•
        - ä½œç”¨æœºç†
        - æ‘˜è¦ä¸»è¦å†…å®¹
        - æ•°æ®æ”¶é›†å¹´ä»½
    """
    if not abstract_text or abstract_text.strip() == "":
        logger.warning("æ‘˜è¦æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é»˜è®¤ç©ºå€¼")
        return {
            "ç ”ç©¶å¯¹è±¡": "éœ€äººå·¥ç¡®è®¤",
            "æ ·æœ¬é‡": "éœ€äººå·¥ç¡®è®¤", 
            "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "éœ€äººå·¥ç¡®è®¤",
            "ä½œç”¨æœºç†": "éœ€äººå·¥ç¡®è®¤",
            "æ‘˜è¦ä¸»è¦å†…å®¹": "éœ€äººå·¥ç¡®è®¤",
            "ç»“è®ºæ‘˜è¦": "éœ€äººå·¥ç¡®è®¤",
            "å›½å®¶": "éœ€äººå·¥ç¡®è®¤",
            "æ•°æ®æ”¶é›†å¹´ä»½": "éœ€äººå·¥ç¡®è®¤"
        }
    
    # æ„å»ºå…¨ä¸­æ–‡æç¤ºè¯ï¼Œè¦æ±‚AIä»æ‘˜è¦ä¸­æå–ç‰¹å®šä¿¡æ¯
    prompt = f"""
è¯·åˆ†æä»¥ä¸‹è‹±æ–‡å­¦æœ¯æ–‡çŒ®æ‘˜è¦ï¼Œå¹¶æå–ä»¥ä¸‹å…«ä¸ªæ–¹é¢çš„ä¸­æ–‡ä¿¡æ¯ï¼š

**æ‘˜è¦åŸæ–‡ï¼š**
{abstract_text}

**è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæ‘˜è¦ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ ‡æ³¨"æœªæ˜ç¡®è¯´æ˜"ï¼‰ï¼š**

1. **ç ”ç©¶å¯¹è±¡**ï¼šç ”ç©¶æ¶‰åŠçš„äººç¾¤ç‰¹å¾ï¼ˆå¹´é¾„èŒƒå›´ã€æ€§åˆ«ã€å¥åº·çŠ¶å†µã€BMIèŒƒå›´ç­‰ï¼‰
   - ä¾‹å¦‚ï¼š18-65å²å¥åº·æˆå¹´äººï¼Œè‚¥èƒ–å¥³æ€§ï¼Œä»£è°¢ç»¼åˆå¾æ‚£è€…ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œä¸èƒ½å‡ºç°è‹±æ–‡å•è¯å¦‚"men"ã€"women"ç­‰

2. **æ ·æœ¬é‡**ï¼šç ”ç©¶ä¸­çš„å‚ä¸è€…æ•°é‡å’Œç±»å‹
   - ä¾‹å¦‚ï¼š120åå‚ä¸è€…ï¼Œ60ä¾‹æ‚£è€…ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡

3. **æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•**ï¼šç ”ç©¶ä¸­çš„MCTæˆ–ç›¸å…³è¥å…»ç´ è¡¥å……æ–¹æ¡ˆ
   - ä¾‹å¦‚ï¼šæ¯æ—¥30æ¯«å‡MCTæ²¹ï¼Œåˆ†2æ¬¡æœç”¨ï¼›æ¯é¤å‰10å…‹MCTç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œæ•°å­—å’Œå•ä½è¦æ¸…æ™°

4. **ä½œç”¨æœºç†**ï¼šMCTå‘æŒ¥æ•ˆåº”çš„ç”Ÿç‰©å­¦æœºåˆ¶
   - ä¾‹å¦‚ï¼šé€šè¿‡ç”Ÿé…®ä½œç”¨ä¿ƒè¿›è„‚è‚ªç‡ƒçƒ§ï¼›æé«˜ä»£è°¢ç‡ï¼›æŠ‘åˆ¶é£Ÿæ¬²ç­‰
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œç”¨ç§‘å­¦æœ¯è¯­æè¿°

5. **æ‘˜è¦ä¸»è¦å†…å®¹**ï¼šç”¨1-2å¥è¯æ¦‚æ‹¬è¯¥ç ”ç©¶çš„é‡ç‚¹å‘ç°å’Œç»“è®º
   - ä¾‹å¦‚ï¼šç ”ç©¶å‘ç°æ¯æ—¥è¡¥å……30æ¯«å‡MCTæ²¹å¯ä»¥æ˜¾è‘—å‡å°‘è¶…é‡æˆå¹´äººçš„ä½“è„‚å«é‡
   - ç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œç®€æ´æ˜äº†

6. **ç»“è®ºæ‘˜è¦**ï¼šç ”ç©¶çš„æ ¸å¿ƒç»“è®ºå’Œç ”ç©¶æ„ä¹‰ï¼Œå¿…é¡»ç”¨ä¸­æ–‡è¡¨è¾¾
   - ä¾‹å¦‚ï¼šæœ¬ç ”ç©¶è¡¨æ˜MCTæ²¹è¡¥å……å‰‚èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„è‚¥èƒ–äººç¾¤çš„ä½“é‡å’Œä½“è„‚åˆ†å¸ƒï¼Œä¸ºä¸´åºŠè¥å…»å¹²é¢„æä¾›äº†æ–°çš„è¯æ®æ”¯æŒ
   - **å¼ºåˆ¶æ€§è¦æ±‚ï¼šç­”æ¡ˆå¿…é¡»æ˜¯ä¸­æ–‡ï¼Œä¸èƒ½ä½¿ç”¨è‹±æ–‡** 
   - å¦‚æœæ‘˜è¦ä¸­æ²¡æœ‰æ˜ç¡®ç»“è®ºï¼Œè¯·åŸºäºç ”ç©¶ç»“æœæ€»ç»“ä¸­æ–‡ç»“è®º

7. **å›½å®¶**ï¼šç ”ç©¶è¿›è¡Œæ‰€åœ¨çš„å›½å®¶
   - ä¾‹å¦‚ï¼šç¾å›½ã€ä¸­å›½ã€è‹±å›½ã€å¾·å›½ã€æ—¥æœ¬ã€æ¾³å¤§åˆ©äºšç­‰
   - åªè¿”å›æ ‡å‡†å›½å®¶åç§°ï¼Œå¦‚"USA"å¯¹åº”"ç¾å›½"ï¼Œ"China"å¯¹åº”"ä¸­å›½"
   - ç»ä¸èƒ½åŒ…å«åŸå¸‚åï¼ˆå¦‚Beijingã€Shanghaiã€New Yorkã€Londonç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«é‚®æ”¿ç¼–ç ï¼ˆå¦‚H9X 3V9ã€M5Vã€V1Mç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«æœºæ„åç§°ï¼ˆå¦‚Universityã€Hospitalã€Instituteç­‰ï¼‰
   - ç»ä¸èƒ½åŒ…å«è¡—é“åœ°å€ï¼ˆå¦‚Streetã€Roadã€Avenueç­‰ï¼‰
   - å¦‚æœæ— æ³•ç¡®å®šå‡†ç¡®çš„å›½å®¶ï¼Œæ ‡æ³¨"éœ€äººå·¥ç¡®è®¤"

8. **æ•°æ®æ”¶é›†å¹´ä»½**ï¼šç ”ç©¶å®é™…æ•°æ®æ”¶é›†çš„æ—¶é—´æœŸé—´
   - ä¾‹å¦‚ï¼š2018å¹´1æœˆè‡³12æœˆï¼Œ2019å¹´6æœˆ-2020å¹´5æœˆï¼Œ2020å¹´ç­‰
   - åªè¿”å›å…·ä½“å¹´ä»½æˆ–å¹´ä»½èŒƒå›´ï¼Œä¸è¦åŒ…å«å‘è¡¨å¹´ä»½
   - å¦‚æœæ‘˜è¦ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°æ•°æ®æ”¶é›†æ—¶é—´ï¼Œæ ‡æ³¨"æœªæ˜ç¡®è¯´æ˜"

**è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š**
```json
{{
  "ç ”ç©¶å¯¹è±¡": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ ·æœ¬é‡": "æå–çš„ä¸­æ–‡å†…å®¹", 
  "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "æå–çš„ä¸­æ–‡å†…å®¹",
  "ä½œç”¨æœºç†": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ‘˜è¦ä¸»è¦å†…å®¹": "æå–çš„ä¸­æ–‡å†…å®¹",
  "ç»“è®ºæ‘˜è¦": "æå–çš„ä¸­æ–‡å†…å®¹",
  "å›½å®¶": "æå–çš„ä¸­æ–‡å†…å®¹",
  "æ•°æ®æ”¶é›†å¹´ä»½": "æå–çš„ä¸­æ–‡å†…å®¹"
}}
```

**é‡è¦è¦æ±‚ï¼š**
- **ç»“è®ºæ‘˜è¦å­—æ®µå¼ºåˆ¶æ€§è¦æ±‚ï¼šå¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸èƒ½åŒ…å«ä»»ä½•è‹±æ–‡å†…å®¹**
- æ‰€æœ‰ç­”æ¡ˆå¿…é¡»æ˜¯çº¯ä¸­æ–‡ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡å•è¯
- **å›½å®¶å­—æ®µç‰¹åˆ«è¦æ±‚**ï¼šç»å¯¹ä¸èƒ½è¿”å›åŸå¸‚ã€é‚®æ”¿ç¼–ç ã€æœºæ„åç§°æˆ–åœ°å€ä¿¡æ¯
- **æ•°æ®æ”¶é›†å¹´ä»½å­—æ®µç‰¹åˆ«è¦æ±‚**ï¼šå¿…é¡»åŒºåˆ†å‘è¡¨å¹´ä»½å’Œæ•°æ®æ”¶é›†å¹´ä»½ï¼Œå‘è¡¨å¹´ä»½ä¸æ˜¯æ•°æ®æ”¶é›†å¹´ä»½
- åªæå–æ‘˜è¦ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸è¦æ¨æ–­
- å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œä½¿ç”¨"æœªæ˜ç¡®è¯´æ˜"æˆ–"éœ€äººå·¥ç¡®è®¤"
- è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSON
"""
  
    # å°è¯•ä¸åŒçš„APIç«¯ç‚¹å’Œæ¨¡å‹ç»„åˆ
    model_configs = [
        # ç«¯ç‚¹, æ¨¡å‹åç§°
        ("gpt-3.5-turbo", API_ENDPOINTS[0]),  # GPTGod + gpt-3.5
        ("gpt-4", API_ENDPOINTS[0]),  # GPTGod + gpt-4
        ("deepseek-chat", API_ENDPOINTS[2])  # DeepSeek + deepseek-chat
    ]
    
    print("  ğŸ¤– AIæ¨¡å‹å¼€å§‹åˆ†ææ‘˜è¦å†…å®¹...")
    max_retries_per_config = 3  # æ¯ä¸ªæ¨¡å‹é…ç½®çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    
    for model_name, api_base_url in model_configs:
        for attempt in range(max_retries_per_config):
            # ä»å¯†é’¥æ± è·å–å¯ç”¨å¯†é’¥
            current_api_key = api_key_pool.get_available_key()
            if not current_api_key:
                logger.error("æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹")
                break
                
            headers = {
                "Authorization": f"Bearer {current_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": model_name,
                "messages": [
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.1
            }
            
            try:
                # è®°å½•å°è¯•ä¿¡æ¯ï¼ˆä½¿ç”¨å®‰å…¨çš„æ—¥å¿—è®°å½•ï¼‰
                if API_KEY_POOL_CONFIG.get("log_key_usage", True):
                    key_id = api_key_pool._get_key_id(current_api_key)
                    logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹ {model_name} åœ¨ç«¯ç‚¹ {api_base_url}ï¼Œå°è¯• {attempt + 1}/{max_retries_per_config}ï¼Œå¯†é’¥ {key_id}")
                else:
                    logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹ {model_name} åœ¨ç«¯ç‚¹ {api_base_url}ï¼Œå°è¯• {attempt + 1}/{max_retries_per_config}")
                
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…429é”™è¯¯
                time.sleep(REQUEST_DELAY)
                
                # å‘é€APIè¯·æ±‚
                response = requests.post(api_base_url, headers=headers, json=payload, timeout=30)
                
                # å¤„ç†APIå“åº”
                if response.status_code == 200:
                    result = response.json()
                    ai_content = result['choices'][0]['message']['content']
                    
                    # è®°å½•æˆåŠŸä¿¡æ¯
                    api_key_pool.report_success(current_api_key)
                    
                    if API_KEY_POOL_CONFIG.get("log_key_usage", True):
                        key_id = api_key_pool._get_key_id(current_api_key)
                        logger.info(f"AI APIè°ƒç”¨æˆåŠŸï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œå¯†é’¥ï¼š{key_id}")
                    else:
                        logger.info(f"AI APIè°ƒç”¨æˆåŠŸï¼Œæ¨¡å‹ï¼š{model_name}")
                    
                    # æå–JSONéƒ¨åˆ†
                    try:
                        # å°è¯•ä»AIå“åº”ä¸­æå–JSON
                        json_start = ai_content.find('{')
                        json_end = ai_content.rfind('}') + 1
                        if json_start != -1 and json_end != 0:
                            json_str = ai_content[json_start:json_end]
                            extracted_data = json.loads(json_str)
                            
                            # éªŒè¯æå–çš„æ•°æ®
                            validated_data = validate_extracted_data(extracted_data)
                            logger.info(f"æˆåŠŸæå–ä¿¡æ¯")
                            return validated_data
                        else:
                            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                            
                    except (json.JSONDecodeError, ValueError) as e:
                        # JSONè§£æå¤±è´¥ä¹ŸæŠ¥å‘Šä¸ºå¤±è´¥ï¼Œä½†ä¸åˆ‡æ¢å¯†é’¥
                        api_key_pool.report_failure(current_api_key, "json_parse_error")
                        logger.error(f"JSONè§£æå¤±è´¥ï¼š{e}")
                        continue  # é‡è¯•å½“å‰æ¨¡å‹
                        
                elif response.status_code == 429:
                    # è¯·æ±‚é¢‘ç‡è¿‡é«˜
                    api_key_pool.report_failure(current_api_key, "rate_limit")
                    wait_time = REQUEST_DELAY * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    logger.warning(f"APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•")
                    time.sleep(wait_time)
                    continue  # é‡è¯•å½“å‰æ¨¡å‹
                    
                else:
                    # å…¶ä»–HTTPé”™è¯¯
                    api_key_pool.report_failure(current_api_key, f"http_{response.status_code}")
                    logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼Œæ¨¡å‹ï¼š{model_name}ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
                    
                    # å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼ˆ401/403ï¼‰ï¼Œç›´æ¥åˆ‡æ¢å¯†é’¥
                    if response.status_code in [401, 403]:
                        logger.warning(f"è®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯†é’¥")
                        api_key_pool.rotate_key()
                        continue  # å°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥
                    else:
                        continue  # é‡è¯•å½“å‰æ¨¡å‹
                    
            except requests.exceptions.RequestException as e:
                # ç½‘ç»œè¯·æ±‚é”™è¯¯
                api_key_pool.report_failure(current_api_key, "network_error")
                logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯ï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œé”™è¯¯ï¼š{e}")
                continue  # é‡è¯•å½“å‰æ¨¡å‹
                
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸
                api_key_pool.report_failure(current_api_key, "unknown_error")
                logger.error(f"AIä¿¡æ¯æå–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼Œæ¨¡å‹ï¼š{model_name}ï¼Œé”™è¯¯ï¼š{e}")
                continue  # é‡è¯•å½“å‰æ¨¡å‹
        
        # å½“å‰æ¨¡å‹é…ç½®çš„æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        logger.warning(f"æ¨¡å‹ {model_name} åœ¨æ‰€æœ‰é‡è¯•åä»ç„¶å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹")
    
    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    logger.warning("æ‰€æœ‰AIæ¨¡å‹å’Œå¯†é’¥ç»„åˆéƒ½è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
    return get_fallback_data()

def validate_extracted_data(data: Dict[str, str]) -> Dict[str, str]:
    """
    éªŒè¯å’Œæ¸…ç†æå–çš„æ•°æ®
    """
    validated = {}
    for key in ["ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•", "ä½œç”¨æœºç†", "æ‘˜è¦ä¸»è¦å†…å®¹", "ç»“è®ºæ‘˜è¦", "å›½å®¶", "æ•°æ®æ”¶é›†å¹´ä»½"]:
        value = data.get(key, "N/A")
        # æ¸…ç†å’ŒéªŒè¯å€¼
        if isinstance(value, str):
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            value = value.strip()
            # å¦‚æœä¸ºç©ºæˆ–åŒ…å«æ— æ•ˆå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not value or value.lower() in ["null", "none", "", "undefined"]:
                value = "æœªæ˜ç¡®è¯´æ˜"
        else:
            value = "æœªæ˜ç¡®è¯´æ˜"
        validated[key] = value
    
    return validated

def get_fallback_data() -> Dict[str, str]:
    """
    å½“AIæå–å¤±è´¥æ—¶è¿”å›çš„å¤‡ç”¨æ•°æ®
    """
    return {
        "ç ”ç©¶å¯¹è±¡": "éœ€äººå·¥ç¡®è®¤",
        "æ ·æœ¬é‡": "éœ€äººå·¥ç¡®è®¤", 
        "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•": "éœ€äººå·¥ç¡®è®¤",
        "ä½œç”¨æœºç†": "éœ€äººå·¥ç¡®è®¤",
        "æ‘˜è¦ä¸»è¦å†…å®¹": "éœ€äººå·¥ç¡®è®¤",
        "ç»“è®ºæ‘˜è¦": "éœ€äººå·¥ç¡®è®¤",
        "å›½å®¶": "éœ€äººå·¥ç¡®è®¤",
        "æ•°æ®æ”¶é›†å¹´ä»½": "éœ€äººå·¥ç¡®è®¤"
    }

# ================= å…¨æ–‡æå–åŠŸèƒ½ =================
def check_full_text_availability(pmid: str) -> Dict[str, any]:
    """
    æ£€æŸ¥PMIDå¯¹åº”çš„æ–‡ç« æ˜¯å¦æä¾›å…è´¹å…¨æ–‡
    é‡ç‚¹æ£€æŸ¥title="Free full text at PubMed Central"çš„aå…ƒç´ 
    
    Args:
        pmid: PubMed ID
    
    Returns:
        åŒ…å«å…è´¹çŠ¶æ€å’Œé“¾æ¥ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # æ„å»ºPubMedé¡µé¢URL
        pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
        print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {pubmed_url}")
        
        # è·å–é¡µé¢å†…å®¹ï¼Œæ·»åŠ æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(pubmed_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä¼˜å…ˆçº§1ï¼šç›´æ¥æŸ¥æ‰¾title="Free full text at PubMed Central"çš„aå…ƒç´ 
        pmc_free_link = soup.find('a', title="Free full text at PubMed Central")
        if pmc_free_link:
            href = pmc_free_link.get('href', '')
            if href:
                full_url = href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}"
                print(f"âœ… æ‰¾åˆ°PMCå…è´¹å…¨æ–‡é“¾æ¥: {full_url}")
                return {
                    "is_free": True,
                    "pmid": pmid,
                    "pubmed_url": pubmed_url,
                    "links": [{
                        "url": full_url,
                        "title": "Free full text at PubMed Central",
                        "is_free": True,
                        "element_found": "title attribute"
                    }],
                    "message": "æ‰¾åˆ°PMCå…è´¹å…¨æ–‡",
                    "source": "direct_title_match"
                }
        
        # ä¼˜å…ˆçº§2ï¼šæŸ¥æ‰¾Full text linkséƒ¨åˆ†
        full_text_section = soup.find('div', {'data-content-id': 'full-text-links'})
        if not full_text_section:
            full_text_section = soup.find('div', class_='full-text-links')
        
        # ä¼˜å…ˆçº§3ï¼šåœ¨å…¨æ–‡é“¾æ¥å®¹å™¨ä¸­æŸ¥æ‰¾å…è´¹é“¾æ¥
        free_links = []
        all_links = []
        
        if full_text_section:
            link_elements = full_text_section.find_all('a', href=True)
            print(f"ğŸ“„ åœ¨å…¨æ–‡é“¾æ¥éƒ¨åˆ†æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")
        else:
            # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„å…¨æ–‡é“¾æ¥éƒ¨åˆ†ï¼ŒæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            link_elements = soup.find_all('a', href=True)
            # ç­›é€‰å¯èƒ½ç›¸å…³çš„é“¾æ¥
            link_elements = [link for link in link_elements if any(keyword in link.get('href', '').lower() 
                               for keyword in ['pmc', 'europepmc', 'full', 'text', 'article'])]
            print(f"ğŸ”— æ‰¾åˆ° {len(link_elements)} ä¸ªç›¸å…³é“¾æ¥")
        
        # åˆ†ææ¯ä¸ªé“¾æ¥
        for link in link_elements:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            title_attr = link.get('title', '')
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå…è´¹é“¾æ¥
            is_free = False
            free_indicators = []
            
            # æ£€æŸ¥å„ç§å…è´¹æŒ‡æ ‡
            if 'pmc' in href.lower():
                is_free = True
                free_indicators.append('PMC URL')
            if 'europepmc' in href.lower():
                is_free = True
                free_indicators.append('EuropePMC URL')
            if title_attr and 'free' in title_attr.lower():
                is_free = True
                free_indicators.append('Free title')
            if text and 'free' in text.lower():
                is_free = True
                free_indicators.append('Free text')
            if 'pubmedcentral' in href.lower():
                is_free = True
                free_indicators.append('PubMed Central')
            
            link_info = {
                "url": href if href.startswith('http') else f"https://pubmed.ncbi.nlm.nih.gov{href}",
                "title": text,
                "title_attr": title_attr,
                "is_free": is_free,
                "indicators": free_indicators
            }
            
            all_links.append(link_info)
            if is_free:
                free_links.append(link_info)
                print(f"âœ… å‘ç°å…è´¹é“¾æ¥: {text} - {link_info['url']} ({', '.join(free_indicators)})")
        
        # ç¡®å®šæœ€ç»ˆç»“æœ
        if free_links:
            return {
                "is_free": True,
                "pmid": pmid,
                "pubmed_url": pubmed_url,
                "links": free_links,
                "all_links": all_links,
                "message": f"æ‰¾åˆ° {len(free_links)} ä¸ªå…è´¹å…¨æ–‡é“¾æ¥",
                "source": "link_analysis"
            }
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…è´¹é“¾æ¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»˜è´¹é“¾æ¥
            has_paid_links = len(all_links) > 0
            return {
                "is_free": False,
                "pmid": pmid,
                "pubmed_url": pubmed_url,
                "links": all_links,
                "message": "æœªæ‰¾åˆ°å…è´¹å…¨æ–‡" + ("ï¼Œä½†æœ‰ä»˜è´¹é“¾æ¥" if has_paid_links else ""),
                "source": "no_free_links" if not has_paid_links else "paid_only"
            }
        
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return {
            "is_free": False,
            "pmid": pmid,
            "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
            "message": "æ— æ³•è·å–é¡µé¢ä¿¡æ¯"
        }
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {str(e)}")
        return {
            "is_free": False,
            "pmid": pmid,
            "error": f"è§£æå¤±è´¥: {str(e)}",
            "message": "é¡µé¢è§£æå‡ºé”™"
        }

def extract_full_text_content(pmid: str, link_url: str = None) -> Dict[str, any]:
    """
    ä»å…è´¹å…¨æ–‡é“¾æ¥æå–æ–‡ç« å†…å®¹
    å¢å¼ºå…ƒç´ å®šä½å’Œå†…å®¹æå–é€»è¾‘
    
    Args:
        pmid: PubMed ID
        link_url: å…¨æ–‡é“¾æ¥URLï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
    
    Returns:
        åŒ…å«æå–å†…å®¹çš„å­—å…¸
    """
    try:
        # å¦‚æœæ²¡æœ‰æä¾›é“¾æ¥URLï¼Œå…ˆæ£€æŸ¥å¯ç”¨æ€§
        if not link_url:
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹PMID {pmid}çš„å…è´¹å…¨æ–‡é“¾æ¥...")
            availability = check_full_text_availability(pmid)
            if not availability['is_free']:
                return {
                    "success": False,
                    "pmid": pmid,
                    "message": availability['message']
                }
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå…è´¹é“¾æ¥
            free_links = [link for link in availability['links'] if link['is_free']]
            if not free_links:
                return {
                    "success": False,
                    "pmid": pmid,
                    "message": "æœªæ‰¾åˆ°å¯ç”¨çš„å…è´¹å…¨æ–‡é“¾æ¥"
                }
            
            link_url = free_links[0]['url']
            print(f"ğŸ“„ é€‰æ‹©å…è´¹å…¨æ–‡é“¾æ¥: {link_url}")
        
        print(f"ğŸ“– æ­£åœ¨æå–PMID {pmid}çš„å…¨æ–‡å†…å®¹...")
        
        # è·å–å…¨æ–‡é¡µé¢ï¼Œä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(link_url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åˆå§‹åŒ–æå–ç»“æœ
        content = {
            "pmid": pmid,
            "source_url": link_url,
            "extraction_success": False,
            "content": {},
            "debug_info": {
                "page_title": "",
                "total_sections": 0,
                "extracted_elements": []
            }
        }
        
        # æå–é¡µé¢æ ‡é¢˜ç”¨äºè°ƒè¯•
        title_tag = soup.find('title')
        if title_tag:
            content['debug_info']['page_title'] = title_tag.get_text(strip=True)
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title_tag.get_text(strip=True)[:100]}...")
        
        # æå–æ ‡é¢˜ - å¤šç§é€‰æ‹©å™¨
        title_selectors = [
            'h1.article-title',
            'h1.title',
            'h1',
            '.article-title',
            '.title',
            'title'
        ]
        
        title_text = ""
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                if title_text and len(title_text) > 10:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                    content['content']['title'] = title_text
                    content['debug_info']['extracted_elements'].append(f"æ ‡é¢˜: {selector}")
                    print(f"âœ… æå–æ ‡é¢˜: {title_text[:100]}...")
                    break
        
        # æå–æ‘˜è¦ - å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        abstract_selectors = [
            'div.abstract',
            'section.abstract',
            'div[data-section="abstract"]',
            'div#abstract',
            '.abstract-content',
            '.article-abstract',
            'div[class*="abstract"]'
        ]
        
        abstract_text = ""
        for selector in abstract_selectors:
            abstract_elem = soup.select_one(selector)
            if abstract_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in abstract_elem.select('script, style, .reference, .citation'):
                    unwanted.decompose()
                
                abstract_text = abstract_elem.get_text(strip=True)
                if abstract_text and len(abstract_text) > 50:  # ç¡®ä¿æ‘˜è¦æœ‰æ„ä¹‰
                    content['content']['abstract'] = abstract_text
                    content['debug_info']['extracted_elements'].append(f"æ‘˜è¦: {selector}")
                    print(f"âœ… æå–æ‘˜è¦: {len(abstract_text)} å­—ç¬¦")
                    break
        
        # æå–å…³é”®è¯ - å¤šç§ä½ç½®
        keywords_selectors = [
            'div.keywords',
            'div#keywords',
            '.keyword-list',
            '[data-section="keywords"]',
            'p:contains("Keywords")',
            'div:contains("Keywords")'
        ]
        
        keywords_text = ""
        for selector in keywords_selectors:
            keywords_elem = soup.select_one(selector)
            if keywords_elem:
                keywords_text = keywords_elem.get_text(strip=True)
                if keywords_text and len(keywords_text) > 5:
                    content['content']['keywords'] = keywords_text
                    content['debug_info']['extracted_elements'].append(f"å…³é”®è¯: {selector}")
                    print(f"âœ… æå–å…³é”®è¯: {keywords_text[:100]}...")
                    break
        
        # æå–ä½œè€…ä¿¡æ¯
        authors_selectors = [
            'div.authors',
            'ul.author-list',
            '.author-list',
            'div.author-info',
            '[data-section="authors"]'
        ]
        
        authors_text = ""
        for selector in authors_selectors:
            authors_elem = soup.select_one(selector)
            if authors_elem:
                authors_text = authors_elem.get_text(strip=True)
                if authors_text and len(authors_text) > 10:
                    content['content']['authors'] = authors_text
                    content['debug_info']['extracted_elements'].append(f"ä½œè€…: {selector}")
                    print(f"âœ… æå–ä½œè€…ä¿¡æ¯: {authors_text[:100]}...")
                    break
        
        # æå–æ­£æ–‡å†…å®¹ - æ›´æ™ºèƒ½çš„ç­–ç•¥
        body_selectors = [
            'div.article-body',
            'article',
            'div.body-content',
            'div.main-content',
            'div.content',
            'div#content'
        ]
        
        body_text = ""
        for selector in body_selectors:
            body_elem = soup.select_one(selector)
            if body_elem:
                # ç§»é™¤å¯¼èˆªã€å¹¿å‘Šã€å¼•ç”¨ç­‰ä¸éœ€è¦çš„å†…å®¹
                unwanted_selectors = [
                    'script', 'style', 'nav', 'header', 'footer', 
                    '.advertisement', '.sidebar', '.related-articles',
                    '.reference', '.citation', '.author-notes'
                ]
                
                for unwanted_selector in unwanted_selectors:
                    for unwanted in body_elem.select(unwanted_selector):
                        unwanted.decompose()
                
                # æå–æ–‡æœ¬
                body_text = body_elem.get_text(strip=True)
                if body_text and len(body_text) > 200:  # ç¡®ä¿æ­£æ–‡å†…å®¹æœ‰æ„ä¹‰
                    content['content']['body_text'] = body_text
                    content['debug_info']['extracted_elements'].append(f"æ­£æ–‡: {selector}")
                    print(f"âœ… æå–æ­£æ–‡: {len(body_text)} å­—ç¬¦")
                    break
        
        # æå–å‚è€ƒæ–‡çŒ®
        refs_selectors = [
            'div.references',
            'ol.references',
            'ul.references',
            '.reference-list',
            '[data-section="references"]'
        ]
        
        refs_text = ""
        for selector in refs_selectors:
            refs_elem = soup.select_one(selector)
            if refs_elem:
                refs_text = refs_elem.get_text(strip=True)
                if refs_text and len(refs_text) > 50:
                    content['content']['references'] = refs_text
                    content['debug_info']['extracted_elements'].append(f"å‚è€ƒæ–‡çŒ®: {selector}")
                    print(f"âœ… æå–å‚è€ƒæ–‡çŒ®: {len(refs_text)} å­—ç¬¦")
                    break
        
        # ç»Ÿè®¡æå–çš„å…ƒç´ 
        content['debug_info']['total_sections'] = len(content['content'])
        
        # åˆ¤æ–­æå–æ˜¯å¦æˆåŠŸ
        if len(content['content']) >= 2:  # è‡³å°‘æå–åˆ°æ ‡é¢˜å’Œæ‘˜è¦
            content['extraction_success'] = True
            content['message'] = f"æˆåŠŸæå–{len(content['content'])}ä¸ªéƒ¨åˆ†çš„å†…å®¹"
            print(f"âœ… å…¨æ–‡æå–å®Œæˆï¼Œå…±æå–{len(content['content'])}ä¸ªéƒ¨åˆ†")
        else:
            content['extraction_success'] = False
            content['message'] = f"æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†"
            print(f"âš ï¸ æå–å†…å®¹ä¸å®Œæ•´ï¼Œä»…è·å–åˆ°{len(content['content'])}ä¸ªéƒ¨åˆ†")
        
        # å¦‚æœå®Œå…¨æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œæä¾›è°ƒè¯•ä¿¡æ¯
        if not content['content']:
            content['extraction_success'] = False
            content['message'] = "æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹"
            content['debug_info']['no_content_reason'] = "é¡µé¢å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–é“¾æ¥æ— æ•ˆ"
            print(f"âŒ æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
        
        return content
        
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "pmid": pmid,
            "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
            "message": "æ— æ³•è·å–å…¨æ–‡é¡µé¢",
            "debug_info": {"error_type": "network_error"}
        }
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "pmid": pmid,
            "error": f"æå–å¤±è´¥: {str(e)}",
            "message": "å†…å®¹æå–å‡ºé”™",
            "debug_info": {"error_type": "extraction_error", "error_detail": str(e)}
        }

def analyze_pmid_with_full_text(pmid: str) -> Dict[str, any]:
    """
    ç»¼åˆåˆ†æPMIDï¼šæ£€æŸ¥å…è´¹çŠ¶æ€å¹¶æå–å…¨æ–‡å†…å®¹
    å¢å¼ºè°ƒè¯•ä¿¡æ¯å’Œé”™è¯¯å¤„ç†
    
    Args:
        pmid: PubMed ID
    
    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
    """
    print(f"\nğŸ” å¼€å§‹åˆ†æPMID: {pmid}")
    print("=" * 60)
    
    # æ­¥éª¤1ï¼šæ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§
    print("æ­¥éª¤1: æ£€æŸ¥å…¨æ–‡å¯ç”¨æ€§...")
    availability = check_full_text_availability(pmid)
    
    # åˆå§‹åŒ–ç»“æœï¼ŒåŒ…å«parse_recordéœ€è¦çš„å­—æ®µ
    result = {
        "pmid": pmid,
        "timestamp": datetime.now().isoformat(),
        "is_free": availability.get('is_free', False),
        "links": availability.get('links', []),
        "message": availability.get('message', ''),
        "extraction_success": False,
        "extracted_content": {},
        "debug_info": {
            "availability_source": availability.get('source', 'unknown'),
            "total_links_found": len(availability.get('links', [])),
            "extraction_attempted": False,
            "extraction_details": {}
        }
    }
    
    if not availability.get('is_free', False):
        print(f"âŒ PMID {pmid} æ— å…è´¹å…¨æ–‡: {availability.get('message', 'æœªçŸ¥åŸå› ')}")
        result['debug_info']['no_free_reason'] = availability.get('message', 'æœªçŸ¥åŸå› ')
        result['debug_info']['availability_source'] = availability.get('source', 'unknown')
        return result
    
    print(f"âœ… PMID {pmid} æä¾›å…è´¹å…¨æ–‡ (æ¥æº: {availability.get('source', 'unknown')})")
    result['debug_info']['extraction_attempted'] = True
    
    # æ­¥éª¤2ï¼šæå–å…¨æ–‡å†…å®¹
    print("\næ­¥éª¤2: æå–å…¨æ–‡å†…å®¹...")
    try:
        full_text = extract_full_text_content(pmid)
        result['full_text_extraction'] = full_text
        
        # æ›´æ–°è°ƒè¯•ä¿¡æ¯
        if 'debug_info' in full_text:
            result['debug_info']['extraction_details'] = full_text['debug_info']
        
        if full_text.get('extraction_success', False):
            print(f"âœ… æˆåŠŸæå–PMID {pmid}çš„å…¨æ–‡å†…å®¹")
            content_info = full_text.get('content', {})
            result['extraction_success'] = True
            result['extracted_content'] = content_info
            
            # è¯¦ç»†è¾“å‡ºæå–çš„å†…å®¹ä¿¡æ¯
            print(f"   ğŸ“„ æ ‡é¢˜: {content_info.get('title', 'N/A')[:100]}...")
            if 'abstract' in content_info:
                print(f"   ğŸ“ æ‘˜è¦: {len(content_info['abstract'])} å­—ç¬¦")
            if 'body_text' in content_info:
                print(f"   ğŸ“– æ­£æ–‡: {len(content_info['body_text'])} å­—ç¬¦")
            if 'keywords' in content_info:
                print(f"   ğŸ”‘ å…³é”®è¯: {len(content_info['keywords'])} å­—ç¬¦")
            if 'authors' in content_info:
                print(f"   ğŸ‘¥ ä½œè€…ä¿¡æ¯: {len(content_info['authors'])} å­—ç¬¦")
            if 'references' in content_info:
                print(f"   ğŸ“š å‚è€ƒæ–‡çŒ®: {len(content_info['references'])} å­—ç¬¦")
            
            # ç»Ÿè®¡æå–çš„å†…å®¹éƒ¨åˆ†æ•°
            content_parts = len([k for k, v in content_info.items() if v])
            print(f"   ğŸ“Š æ€»è®¡æå–äº† {content_parts} ä¸ªå†…å®¹éƒ¨åˆ†")
            
        else:
            print(f"âŒ PMID {pmid} å…¨æ–‡å†…å®¹æå–å¤±è´¥: {full_text.get('message', 'æœªçŸ¥é”™è¯¯')}")
            result['message'] = full_text.get('message', 'æå–å¤±è´¥')
            
            # æ·»åŠ é”™è¯¯è°ƒè¯•ä¿¡æ¯
            if 'error' in full_text:
                result['debug_info']['extraction_error'] = full_text['error']
                print(f"   ğŸ” é”™è¯¯è¯¦æƒ…: {full_text['error']}")
            
            if 'debug_info' in full_text and 'no_content_reason' in full_text['debug_info']:
                result['debug_info']['no_content_reason'] = full_text['debug_info']['no_content_reason']
                print(f"   ğŸ” å¤±è´¥åŸå› : {full_text['debug_info']['no_content_reason']}")
    
    except Exception as e:
        print(f"âŒ å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}")
        result['message'] = f"å…¨æ–‡æå–è¿‡ç¨‹å‡ºé”™: {str(e)}"
        result['debug_info']['extraction_error'] = str(e)
    
    print(f"\nğŸ“Š PMID {pmid} åˆ†æå®Œæˆ")
    print(f"   - å…è´¹å…¨æ–‡: {'æ˜¯' if result['is_free'] else 'å¦'}")
    print(f"   - æå–æˆåŠŸ: {'æ˜¯' if result['extraction_success'] else 'å¦'}")
    print("=" * 60)
    
    return result

# ================= æµ‹è¯•åŠŸèƒ½ =================
def test_ai_extraction():
    """
    æµ‹è¯•æ··åˆä¿¡æ¯æå–åŠŸèƒ½çš„ç¤ºä¾‹æ‘˜è¦
    """
    test_abstracts = [
        # ç¤ºä¾‹æ‘˜è¦1 - RCTç ”ç©¶
        """
        Background: Medium-chain triglycerides (MCT) have been proposed as a dietary supplement for weight management. 
        Objective: To evaluate the effects of MCT supplementation on body composition in overweight adults.
        Methods: We conducted a randomized controlled trial with 120 overweight participants (BMI 25-30 kg/mÂ²) aged 25-55 years. 
        Participants received either 30ml MCT oil daily (n=60) or placebo (n=60) for 12 weeks. 
        Results: MCT group showed significant reductions in body fat mass (-2.3Â±0.8 kg vs -0.8Â±0.5 kg, p<0.001). 
        Mechanism: MCTs increase thermogenesis and promote fat oxidation through enhanced ketone production.
        Conclusions: Daily MCT supplementation effectively reduces body fat in overweight adults.
        """,
        
        # ç¤ºä¾‹æ‘˜è¦2 - Metaåˆ†æ
        """
        Background: The efficacy of medium-chain triglycerides (MCT) for weight loss remains controversial.
        Objective: To systematically review and meta-analyze RCTs examining MCT effects on weight loss in adults.
        Methods: We searched databases for randomized controlled trials comparing MCT vs control interventions. 
        Eight studies involving 512 participants (age 18-65 years, various BMI ranges) were included.
        Dosage: MCT interventions ranged from 15-45ml daily for 4-24 weeks.
        Results: MCT supplementation significantly reduced body weight (-1.8 kg, 95% CI: -2.5 to -1.1 kg).
        Mechanisms: MCTs increase energy expenditure, enhance satiety, and promote ketogenesis.
        Conclusions: Strong evidence supports MCT use for moderate weight loss in adults.
        """,
        
        # ç¤ºä¾‹æ‘˜è¦3 - æœºåˆ¶ç ”ç©¶
        """
        Background: Medium-chain triglycerides (MCT) may influence metabolic pathways differently than long-chain fatty acids.
        Objective: To investigate the metabolic mechanisms of MCT in human adipocytes.
        Methods: 80 healthy volunteers (40 men, 40 women, age 20-40 years) participated in this study. 
        Subjects consumed 20g MCT daily for 8 weeks. 
        Results: MCT increased resting metabolic rate by 8% and reduced appetite scores significantly.
        Mechanism: MCTs are rapidly oxidized, generating more ATP per gram than long-chain fatty acids, 
        and stimulate uncoupling proteins in brown adipose tissue.
        Conclusions: MCT enhances metabolic rate through enhanced thermogenesis and fat oxidation.
        """
    ]
    
    print("=" * 60)
    print("æµ‹è¯•æ··åˆä¿¡æ¯æå–åŠŸèƒ½")
    print("=" * 60)
    
    for i, abstract in enumerate(test_abstracts, 1):
        print(f"\næµ‹è¯•æ‘˜è¦ {i}:")
        print("-" * 40)
        
        # ä½¿ç”¨æ··åˆæ–¹æ³•æå–ä¿¡æ¯ï¼ˆä¸»è¦ï¼šæ­£åˆ™è¡¨è¾¾å¼ï¼Œå¤‡ç”¨ï¼šAIï¼‰
        print("å¼€å§‹æå–ç ”ç©¶ä¿¡æ¯...")
        
        # é¦–å…ˆå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–
        regex_extracted = extract_info_with_regex(abstract)
        print(f"æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœï¼š{regex_extracted}")
        
        # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœçš„è´¨é‡
        regex_quality = sum(1 for v in regex_extracted.values() if v != "æœªæ˜ç¡®è¯´æ˜")
        print(f"æ­£åˆ™è¡¨è¾¾å¼æå–è´¨é‡ï¼š{regex_quality}/4 å­—æ®µæœ‰æœ‰æ•ˆä¿¡æ¯")
        
        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼ç»“æœè´¨é‡è¾ƒä½ï¼Œå°è¯•AIæå–
        if regex_quality < 2:  # å¦‚æœå°‘äº2ä¸ªå­—æ®µæœ‰æœ‰æ•ˆä¿¡æ¯
            print("æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœè´¨é‡è¾ƒä½ï¼Œå°è¯•AIæå–...")
            ai_extracted = extract_info_with_ai(abstract)
            print(f"AIæå–ç»“æœï¼š{ai_extracted}")
            
            # åˆå¹¶ç»“æœï¼šä¼˜å…ˆä½¿ç”¨AIç»“æœï¼Œç¼ºå¤±æ—¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç»“æœ
            final_extracted = {}
            for key in ["ç ”ç©¶å¯¹è±¡", "æ ·æœ¬é‡", "æ¨èè¡¥å……å‰‚é‡/ç”¨æ³•", "ä½œç”¨æœºç†"]:
                ai_value = ai_extracted.get(key, "")
                regex_value = regex_extracted.get(key, "")
                
                if ai_value and ai_value not in ["N/A", "éœ€äººå·¥ç¡®è®¤", ""]:
                    final_extracted[key] = ai_value
                elif regex_value and regex_value != "æœªæ˜ç¡®è¯´æ˜":
                    final_extracted[key] = regex_value
                else:
                    final_extracted[key] = "éœ€äººå·¥ç¡®è®¤"
        else:
            # æ­£åˆ™è¡¨è¾¾å¼ç»“æœè´¨é‡è¾ƒå¥½ï¼Œç›´æ¥ä½¿ç”¨
            final_extracted = regex_extracted
            print("æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æœè´¨é‡è‰¯å¥½ï¼Œç›´æ¥ä½¿ç”¨")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("æœ€ç»ˆæå–ç»“æœ:")
        for key, value in final_extracted.items():
            print(f"  {key}: {value}")
        
        print(f"\næ‘˜è¦é•¿åº¦: {len(abstract)} å­—ç¬¦")
        time.sleep(2)  # æµ‹è¯•é—´éš”
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)


# ================= APIå¯†é’¥æ± æµ‹è¯•å‡½æ•° =================
def test_api_key_pool():
    """
    æµ‹è¯•APIå¯†é’¥æ± ç®¡ç†å™¨çš„å„é¡¹åŠŸèƒ½
    åŒ…æ‹¬å¯†é’¥è½®æ¢ã€å¤±è´¥æ£€æµ‹ã€ç¦ç”¨é€»è¾‘å’Œç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "=" * 70)
    print("APIå¯†é’¥æ± ç®¡ç†ç³»ç»Ÿæµ‹è¯•")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„å¯†é’¥æ± é…ç½®
    test_keys = [
        "sk-test123456789abcdef",  # å¯†é’¥1
        "sk-test987654321fedcba",  # å¯†é’¥2
        "sk-test111111111111111"   # å¯†é’¥3
    ]
    
    test_config = {
        "max_failure_count": 2,        # è®¾ç½®è¾ƒä½é˜ˆå€¼ç”¨äºæµ‹è¯•
        "disable_duration": 10,        # 10ç§’ç¦ç”¨æ—¶é—´
        "success_reset_threshold": 1,
        "enable_key_rotation": True,
        "log_key_usage": True
    }
    
    # åˆ›å»ºæµ‹è¯•å¯†é’¥æ± ç®¡ç†å™¨
    test_pool = APIKeyPoolManager(test_keys, test_config)
    print(f"âœ… åˆ›å»ºæµ‹è¯•å¯†é’¥æ± ï¼ŒåŒ…å« {len(test_keys)} ä¸ªå¯†é’¥")
    
    # æµ‹è¯•1: åŸºæœ¬å¯†é’¥è·å–
    print("\n--- æµ‹è¯•1: åŸºæœ¬å¯†é’¥è·å– ---")
    key1 = test_pool.get_available_key()
    print(f"è·å–ç¬¬ä¸€ä¸ªå¯ç”¨å¯†é’¥: {key1}")
    assert key1 == test_keys[0], "åº”è¯¥è¿”å›ç¬¬ä¸€ä¸ªå¯†é’¥"
    
    # æµ‹è¯•2: å¯†é’¥è½®æ¢
    print("\n--- æµ‹è¯•2: å¯†é’¥è½®æ¢ ---")
    test_pool.rotate_key()
    key2 = test_pool.get_available_key()
    print(f"è½®æ¢åè·å–å¯†é’¥: {key2}")
    assert key2 == test_keys[1], "åº”è¯¥è¿”å›ç¬¬äºŒä¸ªå¯†é’¥"
    
    # æµ‹è¯•3: å¤±è´¥è®¡æ•°å’Œç¦ç”¨
    print("\n--- æµ‹è¯•3: å¤±è´¥è®¡æ•°å’Œè‡ªåŠ¨ç¦ç”¨ ---")
    initial_stats = test_pool.get_key_statistics()
    print(f"åˆå§‹çŠ¶æ€: {initial_stats}")
    
    # æŠ¥å‘Šå¤±è´¥ç›´åˆ°è§¦å‘ç¦ç”¨
    for i in range(test_config["max_failure_count"]):
        test_pool.report_failure(key1, "test_error")
        stats = test_pool.get_key_statistics()
        print(f"å¤±è´¥ {i+1} æ¬¡å: key_1 å¤±è´¥æ¬¡æ•°={stats['key_1']['failure_count']}")
    
    # æ£€æŸ¥å¯†é’¥æ˜¯å¦è¢«ç¦ç”¨
    key_after_failures = test_pool.get_available_key()
    print(f"ç¦ç”¨åè·å–çš„å¯†é’¥: {key_after_failures}")
    assert key_after_failures == test_keys[1], "åº”è¯¥è·³è¿‡ç¦ç”¨çš„å¯†é’¥1"
    
    # æµ‹è¯•4: æˆåŠŸé‡ç½®å¤±è´¥è®¡æ•°
    print("\n--- æµ‹è¯•4: æˆåŠŸé‡ç½®å¤±è´¥è®¡æ•° ---")
    test_pool.report_success(key2)
    stats = test_pool.get_key_statistics()
    print(f"æˆåŠŸåç»Ÿè®¡: key_2 æˆåŠŸ={stats['key_2']['success_count']}, å¤±è´¥={stats['key_2']['failure_count']}")
    
    # æµ‹è¯•5: ç¦ç”¨æ¢å¤
    print("\n--- æµ‹è¯•5: ç¦ç”¨æ¢å¤æœºåˆ¶ ---")
    key1_stats_before = test_pool.get_key_statistics()['key_1']
    print(f"å¯†é’¥1ç¦ç”¨çŠ¶æ€: {key1_stats_before['is_disabled']}")
    
    if key1_stats_before['is_disabled']:
        print(f"ç­‰å¾…ç¦ç”¨æœŸç»“æŸ (å½“å‰é…ç½®: {test_config['disable_duration']}ç§’)")
        print("å®é™…æµ‹è¯•ä¸­ï¼Œæ‚¨å¯ä»¥è®¾ç½®æ›´çŸ­çš„ç¦ç”¨æ—¶é—´è¿›è¡Œå¿«é€Ÿæµ‹è¯•")
        
        # åœ¨å®é™…æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿæ—¶é—´è·³è¿‡
        # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨é‡ç½®ç¦ç”¨çŠ¶æ€æ¥æ¼”ç¤º
        test_pool.key_states['key_1']['is_disabled'] = False
        test_pool.key_states['key_1']['disabled_until'] = None
        print("æ‰‹åŠ¨é‡ç½®ç¦ç”¨çŠ¶æ€ç”¨äºæ¼”ç¤º")
    
    # æµ‹è¯•6: ç»Ÿè®¡ä¿¡æ¯
    print("\n--- æµ‹è¯•6: ç»Ÿè®¡ä¿¡æ¯è·å– ---")
    final_stats = test_pool.get_key_statistics()
    print("æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
    for key_id, stats in final_stats.items():
        print(f"  {key_id}:")
        print(f"    çŠ¶æ€: {'ç¦ç”¨' if stats['is_disabled'] else 'æ­£å¸¸'}")
        print(f"    æ€»è¯·æ±‚: {stats['total_requests']}")
        print(f"    æ€»æˆåŠŸ: {stats['total_successes']}")
        print(f"    æˆåŠŸç‡: {stats['success_rate']:.2%}")
    
    # æµ‹è¯•7: æ‰€æœ‰å¯†é’¥éƒ½ä¸å¯ç”¨çš„æƒ…å†µ
    print("\n--- æµ‹è¯•7: å…¨éƒ¨å¯†é’¥ç¦ç”¨æƒ…å†µ ---")
    # ç¦ç”¨æ‰€æœ‰å¯†é’¥
    for i in range(len(test_keys)):
        key_id = f"key_{i+1}"
        test_pool.key_states[key_id]['is_disabled'] = True
        test_pool.key_states[key_id]['disabled_until'] = time.time() + 60
    
    no_key = test_pool.get_available_key()
    print(f"æ‰€æœ‰å¯†é’¥ç¦ç”¨æ—¶è·å–ç»“æœ: {no_key}")
    assert no_key is None, "åº”è¯¥è¿”å›Noneè¡¨ç¤ºæ²¡æœ‰å¯ç”¨å¯†é’¥"
    
    print("\n" + "=" * 70)
    print("APIå¯†é’¥æ± æµ‹è¯•å®Œæˆ")
    print("=" * 70)
    
    return test_pool


def test_key_pool_scenarios():
    """
    æµ‹è¯•å¯†é’¥æ± åœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­çš„è¡¨ç°
    """
    print("\n" + "=" * 70)
    print("å¯†é’¥æ± å®é™…ä½¿ç”¨åœºæ™¯æµ‹è¯•")
    print("=" * 70)
    
    # ä½¿ç”¨å®é™…çš„å¯†é’¥æ± é…ç½®
    print(f"ä½¿ç”¨å®é™…å¯†é’¥æ± ï¼ŒåŒ…å« {len(API_KEYS_POOL)} ä¸ªå¯†é’¥")
    
    # æ˜¾ç¤ºå¯†é’¥æ± ç»Ÿè®¡ä¿¡æ¯
    stats = api_key_pool.get_key_statistics()
    print("å½“å‰å¯†é’¥æ± çŠ¶æ€:")
    for key_id, key_stats in stats.items():
        status = "ğŸ”´ ç¦ç”¨" if key_stats['is_disabled'] else "ğŸŸ¢ æ­£å¸¸"
        last_used = "æœªä½¿ç”¨" if not key_stats['last_used'] else time.strftime("%H:%M:%S", time.localtime(key_stats['last_used']))
        
        print(f"  {key_id}: {status}")
        print(f"    æ€»è¯·æ±‚: {key_stats['total_requests']}, æˆåŠŸ: {key_stats['total_successes']}")
        print(f"    æˆåŠŸç‡: {key_stats['success_rate']:.1%}")
        print(f"    æœ€åä½¿ç”¨: {last_used}")
    
    # æµ‹è¯•å¯†é’¥è·å–
    print("\n--- æµ‹è¯•å¯†é’¥è·å– ---")
    available_key = api_key_pool.get_available_key()
    if available_key:
        key_id = api_key_pool._get_key_id(available_key)
        print(f"âœ… è·å–åˆ°å¯ç”¨å¯†é’¥: {key_id}")
        
        # æ¨¡æ‹ŸæˆåŠŸè¯·æ±‚
        api_key_pool.report_success(available_key)
        print(f"âœ… æŠ¥å‘Šå¯†é’¥ {key_id} è¯·æ±‚æˆåŠŸ")
        
        # è·å–æ›´æ–°åçš„ç»Ÿè®¡
        updated_stats = api_key_pool.get_key_statistics()[key_id]
        print(f"æ›´æ–°åæˆåŠŸç‡: {updated_stats['success_rate']:.1%}")
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å¯†é’¥")
    
    print("\n" + "=" * 70)
    print("å®é™…åœºæ™¯æµ‹è¯•å®Œæˆ")
    print("=" * 70)


def test_country_processing():
    """æµ‹è¯•é‡æ„åçš„å›½å®¶å¤„ç†åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("é‡æ„åçš„å›½å®¶å¤„ç†åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    # æ¨¡æ‹Ÿarticle_data
    mock_articles = [
        {
            "AuthorList": [{
                "AffiliationInfo": [{
                    "Affiliation": "Department of Cardiology, Johns Hopkins University, Baltimore, MD 21287, United States"
                }]
            }]
        },
        {
            "AuthorList": [{
                "AffiliationInfo": [{
                    "Affiliation": "School of Medicine, Peking University, Beijing, China"
                }]
            }]
        },
        {
            "AuthorList": [{
                "AffiliationInfo": [{
                    "Affiliation": "Institute of Medical Sciences, University of Tokyo, Tokyo, Japan"
                }]
            }]
        },
        {
            "AuthorList": [{
                "AffiliationInfo": [{
                    "Affiliation": "Random Hospital, Unknown City, Some Unknown Place"
                }]
            }]
        },
        {
            "AuthorList": [{}]  # æ²¡æœ‰æœºæ„ä¿¡æ¯
        }
    ]
    
    print("æµ‹è¯•ç¼“å­˜æœºåˆ¶...")
    
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    result1 = extract_country_from_affiliation(mock_articles[0])
    print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨ç»“æœ: {result1}")
    
    result2 = extract_country_from_affiliation(mock_articles[0])  # ç›¸åŒçš„æœºæ„ä¿¡æ¯
    print(f"ç¬¬äºŒæ¬¡è°ƒç”¨ç»“æœ (ç¼“å­˜): {result2}")
    
    assert result1 == result2, "ç¼“å­˜åº”è¯¥è¿”å›ç›¸åŒç»“æœ"
    print("âœ“ ç¼“å­˜æœºåˆ¶æµ‹è¯•é€šè¿‡")
    
    print("\næµ‹è¯•å„ç§å›½å®¶è¯†åˆ«åœºæ™¯...")
    
    expected_results = ["United States", "China", "Japan", "éœ€äººå·¥ç¡®è®¤", "éœ€äººå·¥ç¡®è®¤"]
    
    for i, (article, expected) in enumerate(zip(mock_articles, expected_results)):
        result = extract_country_from_affiliation(article)
        print(f"æµ‹è¯•æ¡ˆä¾‹ {i+1}: {result}")
        print(f"  é¢„æœŸç»“æœ: {expected}")
        print(f"  çŠ¶æ€: {'âœ“ é€šè¿‡' if result == expected else 'âœ— ä¸åŒ¹é…'}")
    
    print("\næµ‹è¯•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")
    print(f"å½“å‰ç¼“å­˜å¤§å°: {len(COUNTRY_CACHE)}")
    print("âœ“ å›½å®¶å¤„ç†åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    
    # æ¸…ç†ç¼“å­˜
    COUNTRY_CACHE.clear()
    print("ç¼“å­˜å·²æ¸…ç†")


# ä¿®æ”¹ä¸»ç¨‹åºä»¥æ”¯æŒå¯†é’¥æ± æµ‹è¯•
if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            test_ai_extraction()
        elif sys.argv[1] == "test_key_pool":
            test_api_key_pool()
            test_key_pool_scenarios()
        elif sys.argv[1] == "test_country":
            test_country_processing()
        else:
            print("å¯ç”¨å‘½ä»¤:")
            print("  python pubmed.py           - è¿è¡Œæ­£å¸¸ç¨‹åº")
            print("  python pubmed.py test      - è¿è¡ŒAIæå–æµ‹è¯•")
            print("  python pubmed.py test_key_pool - è¿è¡Œå¯†é’¥æ± æµ‹è¯•")
            print("  python pubmed.py test_country - è¿è¡Œå›½å®¶å¤„ç†æµ‹è¯•")
    else:
        # è·å–æœç´¢è¯
        search_term = get_user_search_term()
        
        print(f"\nå¼€å§‹æœç´¢: {search_term[:100]}...")
        
        # 1. æœç´¢
        ids = search_pubmed(search_term, MAX_RESULTS)
        
        if ids:
            print(f"æ‰¾åˆ° {len(ids)} ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œå¼€å§‹è·å–è¯¦ç»†ä¿¡æ¯...")
            
            # 2. è·å–è¯¦æƒ…
            articles = fetch_details(ids)
            
            # 3. è§£ææ•°æ®
            results = []
            for i, article in enumerate(articles):
                print(f"æ­£åœ¨å¤„ç†æ–‡çŒ® {i+1}/{len(articles)}...")
                results.append(parse_record(article))
            
            # 4. ç”Ÿæˆè¡¨æ ¼
            df = pd.DataFrame(results)
            
            # è°ƒæ•´åˆ—é¡ºåºä»¥ç¬¦åˆæ–‡ä»¶è¦æ±‚
            columns_order = [
                'å‘è¡¨å¹´ä»½', 'æ•°æ®æ”¶é›†å¹´ä»½', 'å›½å®¶', 'ç ”ç©¶ç±»å‹', 'ç ”ç©¶å¯¹è±¡', 'æ ·æœ¬é‡', 'æ¨èå‰‚é‡', 
                'è¡¥å……å‰‚é‡/ç”¨æ³•', 'ä½œç”¨æœºç†', 'æ‘˜è¦ä¸»è¦å†…å®¹', 'è¯æ®ç­‰çº§', 'ç»“è®ºæ‘˜è¦', 'æ ‡é¢˜', 'PMID',
                # å…¨æ–‡ç›¸å…³å­—æ®µ
                'å…è´¹å…¨æ–‡çŠ¶æ€', 'å…è´¹å…¨æ–‡é“¾æ¥æ•°', 'å…¨æ–‡æå–çŠ¶æ€', 'å…¨æ–‡å†…å®¹æ‘˜è¦'
            ]
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
            for col in columns_order:
                if col not in df.columns:
                    if col in ['å‘è¡¨å¹´ä»½', 'æ•°æ®æ”¶é›†å¹´ä»½', 'å›½å®¶', 'ç ”ç©¶ç±»å‹', 'ç ”ç©¶å¯¹è±¡', 'æ ·æœ¬é‡', 'æ¨èå‰‚é‡', 'è¡¥å……å‰‚é‡/ç”¨æ³•', 'ä½œç”¨æœºç†', 'æ‘˜è¦ä¸»è¦å†…å®¹', 'è¯æ®ç­‰çº§', 'ç»“è®ºæ‘˜è¦', 'æ ‡é¢˜']:
                        df[col] = "éœ€äººå·¥ç¡®è®¤"
                    elif col in ['å…è´¹å…¨æ–‡çŠ¶æ€', 'å…è´¹å…¨æ–‡é“¾æ¥æ•°', 'å…¨æ–‡æå–çŠ¶æ€']:
                        df[col] = False if col in ['å…è´¹å…¨æ–‡çŠ¶æ€', 'å…¨æ–‡æå–çŠ¶æ€'] else 0
                    elif col == 'å…¨æ–‡å†…å®¹æ‘˜è¦':
                        df[col] = "æœªå¯ç”¨å…¨æ–‡æå–"
                    elif col == 'PMID':
                        df[col] = ""
                    
            df = df[columns_order]
            
            # å¯¼å‡º
            filename = f"Literature_Search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            df.to_excel(filename, index=False)
            print(f"\nâœ… æˆåŠŸå¯¼å‡ºè¡¨æ ¼ï¼š{filename}")
            print(f"ğŸ“Š åŒ…å« {len(df)} ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯")
        else:
            print("\nâŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥æœç´¢è¯æ˜¯å¦æ­£ç¡®")